#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>

#+HTML_HEAD: <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
#+HTML_HEAD: <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.min.js"></script>
#+HTML_HEAD: <script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>

#+STARTUP: latexpreview
#+STARTUP: entitiespretty

#+TITLE: Riassunto Probabilità e Statistica per l'Informatica
#+AUTHOR:    Gabriele Calissi
#+EMAIL:     gabrielecalissi@gmail.com

* GitHub
- [[https://github.com/gabrielecalissi/psi][Guardalo in GitHub]];
- [[https://github.com/gabrielecalissi/psi/archive/master.zip][Scarica .zip]];
- [[https://github.com/gabrielecalissi/psi/archive/master.tar.gz][Scarica .tar.gz]].
* Lezioni
** Statistica Descrittiva
Raccolta di metodi e strumenti matematici atti ad organizzare una o più serie di dati in modo tale
da evidenziare in forma sintetica eventuali:
- Simmetrie;
- Periodicità;
- Leggi di altro genere.
Ovvero in grado di descriverne in maniera immediatamente comprensibile le informazioni implicitamente
contenute nei dati stessi.

Solitamente la serie di dati di cui si dispone è costituita da un numero limitato di /osservazioni/ che devono
essere rappresentative di un'ampia /popolazione/.

Con il termine /popolazione/ si intende l'insieme degli elementi cui si riferisce l'/indagine statistica/.

Affrontare uno studio di statistica descrittiva richiede di tener presente che le tecniche di organizzazione
dei dati varino in funzione dei modi di presentarsi delle /caratteristiche (caratteri)/ degli elementi
su cui è svolta l'indagine.

/Caratteri qualitativi/: quando le caratteristiche sono qualità o dati non numerici. Per es.: GUSTO =
{dolce, amaro} e COLORE = {rosso, verde, giallo, ...}.

/Caratteri quantitativi/: quando le caratteristiche sono grandezze misurabili.
- /Quantitativi discreti/: possono assumere un numero limitato di valori, per es. DADO = {1, 2, ..., 6}.
- /Quantitativi continui/: assumono valori reali come la TEMPERATURA.

Supponiamo di considerare $n$ elementi della popolazione e di rilevare, per ognuno di essi, il dato
relativo al carattere quantitativo da esaminare.

/Insieme dei dati/: $E = \{x_1, \dots, x_n\}$.

/Numerosità/: numero di elementi considerati $n$.

Quando il /carattere è discreto/ è comodo raggruppare i dati considerando l'insieme di tutti i valori assumibili
(modalità del carattere) ed associare ad ognuno di tali valori il numero di volte che esso compare in $E$.

$N$ = numero totale di modalità.

$S = \{s_1, \dots, s_N\}$ = insieme delle modalità.

/Frequenza assoluta/ della modalità $s_j, j = 1, \dots, N$: $f_j, j = 1, \dots, N$. Data dal numero di elementi di
$E = \{x_1, \dots, x_n\}$ che hanno valore $s_j$.

/Distribuzione di frequenza assoluta dei dati osservati/: funzione che associa ad ogni modalità la corrispondente
frequenza assoluta. $f: S \to \mathbb{N}$, $S = \{s_1, \dots s_N\}$.

La /frequenza cumulata assoluta/ per la modalità $s_j$ è la somma delle frequenze assolute di tutte le modalità
$s_k \in S \mid s_k \leq s_j$. $F_j = \sum_{s_k \leq s_j} f_k$.

/Frequenza relativa/: $p_j = \frac{f_j}{n}$.

/Frequenza cumulata relativa/: $P_j = \sum_{k:s_k \leq s_j} p_k$.

Si dicono distribuzione di frequenza cumulata assoluta, relativa e cumulata relativa dei dati osservali, le
funzioni $F, p$ e $P$ che associano ad ogni modalità $s_j$ le relative frequenze $F_j, p_j, P_j$. Con $n$ numero
delle osservazioni.

Esempio:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 12:15:55
[[file:Lezioni/screenshot_2018-03-01_12-15-55.png]]


#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 12:16:09
[[file:Lezioni/screenshot_2018-03-01_12-16-09.png]]

La forma tabellare consente di fare facilmente alcune semplici osservazioni:
- Il numero di stanze che si presenta con maggior frequenza è 4 (24).
- 3 appartamenti su 10 hanno 4 stanze.
- 9 appartamenti su 10 hanno meno di 7 stanze.

Quando il /carattere/ da studiare è /continuo/ (o discreto con un numero elevato di modalità) è
conveniente /ricondursi a raggruppamenti/ come quelli appena trattati.
Si suddivide l'insieme $S$ delle modalità in /classi/.

/Classe/: un qualsiasi sottoinsieme di $S$.

/Partizione/: ogni famiglia di classi tra loro disgiunte la cui unione è tutto $S$.

La scelta delle classi con cui si suddivide l'insieme $S$ è del tutto /arbitraria/ anche se
è /necessario/ che esse formino una /partizione/ di $S$.

Le partizioni devono essere:
- Significative per il caso in esame.
- Sufficientemente numerose.

Ad ogni classe sono associate diverse grandezze che la caratterizzano come:
- /Confine inferiore/superiore/: valori estremi delle classe.
- /Ampiezza/: differenza tra confine superiore ed inferiore.
- /Valore centrale/: semi-somma tra i due confini.

Nel caso in cui il carattere esaminato sia continuo occorre specificare quando le classi sono
chiuse, a destra o a sinistra, ovvero specificare se gli elementi dell'indagine il cui dato coincide
con il confine della classe sono da raggruppare all'interno della classe stessa oppure no.

Per esempio:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 12:21:02
[[file:Lezioni/screenshot_2018-03-01_12-21-02.png]]

$S = [0.44, 5.42] \subset R$

Volendo suddividere $S$ in 5 classi potremmo scegliere:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 12:21:51
[[file:Lezioni/screenshot_2018-03-01_12-21-51.png]]

I valori centrali delle classi sono $\bar{x_k}$.

Le classi scelte non hanno tutte la stessa ampiezza, utile se si desidera che ogni classe sia abbastanza
consistente. Gli appartamenti con valore maggiore di 4 migliaia di euro sono pochi, si è ritenuto
opportuno adottare un intervallo di ampiezza maggiore dei restanti quattro intervalli.

Per esempio:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 12:23:23
[[file:Lezioni/screenshot_2018-03-01_12-23-23.png]]

/Indici di tendenza centrale/

In generale un ricercatore che disponga di una serie di osservazioni statistiche e si proponga di
descriverne le proprietà, fissa la propria attenzione sulla determinazione di /un solo valore/ che
rappresenti in qualche modo l'intera serie.

Per tale ragione sono stati introdotti gli /indici di tendenza centrale/ o /indici di posizione/ che
sono quantità in grado di sintetizzare con un solo valore numerico i valori assunti dai dati.

Il valore che è da solo certamente più utile di ogni altro nello studio di una serie di dati è la
/media/ definita come media aritmetica tra tutti i valori dei dati osservati.

Supponiamo di indicare con $x_1, \dots, x_n$ l'insieme delle osservazioni disponibili (valori) allora
definiremo la /media/ come $\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i = \frac{x_1 + \dots + x_n}{n}$

Nel caso in cui i /dati siano di tipo quantitativo discreto/ allora avremo
$\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} s_j f_j = \frac{s_1 f_1+ \dots + s_N f_N}{n}$

$\bar{x} = \sum_{j=1}^n s_j p_j = s_1 p_1 + \dots + s_n p_N$.

/Momento \(k\)-esimo/ rispetto ad $y$:
$M_{k, y} = \frac{1}{n} \sum_{i=1}^n (x_i - y)^k$.

Allora la media è anche il momento primo rispetto all'origine:
$x = \frac{1}{n} \sum_{i=1}^n x_i$.

$x = \frac{1}{n} \sum_{i=1}^n (x_i - 0)^1$.

Un secondo indice di tendenza è rappresentata dalla /mediana/ definita come quel numero reale che precede tanti
elementi delle serie di dati quanti ne segue.

Se ordiniamo la serie di dati $x_1, \dots, x_n$ in modo crescente ottenendo la serie x(1), ..., x(n),
la mediana $\hat{x}$ è data:
- Dall'elemento di posto $\frac{(n+1)}{2}$ se $n$ è dispari.
- Dalla media aritmetica tra l'elemento di posto $\frac{n}{2}$ e quello di posto $\frac{n}{2} + 1$ se $n$
  è pari.
  
Un ultimo indice di tendenza è rappresentato dalla /moda/ $\tilde{x}$ definita come quel valore o classe cui
corrisponde la massima frequenza assoluta.

La moda viene spesso utilizzata nel caso di dati qualitativi ovvero quando risulti impossibile definire
media e mediana.

Si osservi che non è garantita l'unicità della moda. Infatti parleremo di:
- Distribuzione /uni-modale/ nel caso in cui vi sia un'unica moda.
- Distribuzione /multi-modale/ nel caso in cui vi siano più mode.

Per esempio:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 12:39:12
[[file:Lezioni/screenshot_2018-03-01_12-39-12.png]]


#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 12:39:23
[[file:Lezioni/screenshot_2018-03-01_12-39-23.png]]

Il calcolo della mediana richiede di ordinare la serie di dati.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 12:39:45
[[file:Lezioni/screenshot_2018-03-01_12-39-45.png]]

Per quanto riguarda il calcolo della moda è banale ricavarla dalla tabella delle frequenze.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 12:40:09
[[file:Lezioni/screenshot_2018-03-01_12-40-09.png]]

Si ricerca il valore con massima frequenza assoluta ottenendo che la moda è $\tilde{x} = 4$.

/Indice di variabilità/

Siano date le seguenti serie di dati: $E_1 = \{0.5, 0.8, 2.0, 2.7, 4.0\}$ ed $E_2 = \{1.4, 1.7, 2.0, 2.1, 2.8\}$
che rappresentano il reddito mensile (migliaia di euro) di due gruppi distinti di individui (appartenenti a
due diverse regioni). Le due serie di dati appaiono molto diverse tra loro. Gli elementi di $E_1$ sono molto
disomogenei tra loro mentre non lo sono quelli di $E_2$. Entrambi i casi hanno però ugual /media/ e /mediana/.

Gli indici di tendenza centrale, quindi, non sono utili per fornire informazioni circa l'omogeneità o
disomogeneità dei dati.

Per ovviare a tale limite vengono introdotti gli indici che misurano il grado di /omogeneità/ o /dispersione/
dei dai.

Il più importante tra questi è senz'altro la /varianza/ definita come: $x^2 = \frac{1}{n} (x_i - \bar{x})^2$.
Viene definita confrontando ogni singola osservazione $x_i$ con la media $\bar{x}$ e sommando i quadrati
delle differenze così ottenute.

Ricordando la definizione $M_{k, y} = \frac{1}{n} \sum_{i=1}^n (x_i - y)^k$ la /varianza/ è allora il /momento secondo
rispetto alla media/: $x^2 = \frac{1}{n} (x_i - \bar{x})^2$.

Viene introdotto il quadrato perché in caso contrario $\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x}) = 0$. La
varianza è tanto più grande quanto più i singoli dati si scostano dalla media, vale a dire tanto più
i dati risultano disomogenei. Nel caso in esame avremo: $s_{E_1}^2 = 1.69$ e $s_{E_2}^2 = 0.22$. E pertanto
la misura varianza consente di rappresentare il /grado di disomogeneità/ della serie di dati.

Nel caso di /caratteri quantitativi discreti/, di cui sia nota la distribuzione di frequenza, la /varianza/
può essere calcolata tramite le seguenti formule:

$s^2 = \frac{1}{n} \sum_{j=1}^N (s_j - \bar{x})^2 f_j = \sum_{j=1}^N (s_j - \bar{x})^2 p_j$. Alternativamente la /varianza/ può essere
calcolata tramite la seguente formula: $s^2 = \frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x}^{2}$.

Analogamente è possibile mostrare che per /caratteri quantitativi discreti/ vale la seguente relazione:
$s^2 = \sum_{j=1}^{N} s_j^2 p_j - \bar{x}^2$.

Poiché la dimensione della varianza è il quadrato di quella dei dati,
in molti casi si preferisce una diversa misura detta /scarto quadratico medio/:

$s = \sqrt{s^2} = \sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x}^2} = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2}$.

Il molti casi si è interessati a studiare fenomeni che coinvolgono due o più caratteri della popolazione
tali da non potersi considerare separatamente. Si pensi ad esempio al caso in cui vengano considerati
separatamente. 

Si pensi ad esempio al caso in cui vengano considerati i seguenti caratteri:
- Valore al metro quadro.
- Superficie.
per un insieme di appartamenti di una data zona di una città.

In quel caso è necessario considerare congiuntamente le due caratteristiche, vale a dire procedere in modo
differente a quanto fatto fin qui. Limitiamoci a considerare il caso di due caratteri contemporanei.

In questo caso l'insieme dei dati statistici $E$ sarà costituito da coppie di valori $E = \{(x_1, y_1), \dots, (x_n, y_n)\}$.

Ipotizziamo inoltre che entrambi i caratteri siano di tipo quantitativo e discreto.

I caratteri quantitativi continui vengono trattati in modo analogo previo una procedura di raggruppamento in classi.

Sia $S = \{(s_j, u_k), j = 1, \dots, N; k = 1, \dots, M\}$ l'insieme delle coppie di valori assumibili dalla coppia di caratteri
analizzati. Viene detta /frequenza assoluta/ di $(s_j, u_k)$ la quantità $f_{jk}$ definita come $f_{jk}$ = numero di elementi
$E$ aventi valore $(s_j, u_k)$.

Definiremo /distribuzione di frequenza assoluta doppia/ la funzione $f$ che associa ad ogni coppia di valori $(s_j, u_k)$
la corrispondente frequenza $f_{jk}$.

Analogamente al caso unidimensionale vengono solitamente definiti ed utilizzati altri tipi di frequenze:
- Frequenza cumulata assoluta: $F_{jk} = \sum_{r:s_r \leq s_j; l:u_l \leq u_k} f_{rl}$.
- Frequenza relativa: $p_{jk} = \frac{f_{jk}}{n}$.
- Frequenza cumulata relativa: $P_{jk} = \sum_{r:s_r \leq s_j; l:u_l \leq u_k} p_{rl}$.

Con /distribuzione di frequenza doppia/ si intende infine una qualsiasi delle funzioni $f, F, p, P$ che
associ ad ogni coppia (s_j, u_k) la corrispondente frequenza. In aggiunta alla distribuzioni appena citate,
analoghe a quelle del caso unidimensionale, esistono altre distribuzioni di frequenza spesso prese in
considerazione.

/Distribuzioni marginali/: distribuzioni dei singoli caratteri presi indipendentemente dagli altri.

Nel caso ci si riferisca al primo carattere, per ogni valore assumibile ad esso, sia $s_j$, è detta
/frequenza assoluta marginale/ la quantità $f_{xj}$ data dal numero di elementi $E$ il cui primo carattere
ha valore $s_j$, vale a dire $f_{xj}$ = numero di elementi $E$ aventi valore $(s_{j}, *)$.

Analogamente avremo:
- Frequenza assoluta marginale: $F_{xj}$ = somma delle frequenze assolute marginali di tutti i valori $s_k$ tali che
  $s_k \leq s_j$.
- Frequenza relativa marginale: $p_{xj}$ = rapporto tra frequenza assoluta marginale $f_{xj}$ e numerosità $n$ delle
  osservazioni.
- Frequenza cumulata relativa marginale: $P_{xj}$ = somma delle frequenze relative marginali di tutti i valori $s_k$
  tali che $s_k \leq s_j$.

Per esempio:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 13:14:48
[[file:Lezioni/screenshot_2018-03-01_13-14-48.png]]


#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 13:15:01
[[file:Lezioni/screenshot_2018-03-01_13-15-01.png]]

Es.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 13:15:19
[[file:Lezioni/screenshot_2018-03-01_13-15-19.png]]

Consideriamo due serie $\{x_i\}\ \{y_i\},\ i = 1, \dots, n$ e poniamo a confronto le variazioni delle coppie di dati rispetto ai
corrispondenti valori medi considerando le /coppie di scarti/ $x_i - \bar{x}$ e $y_i - \bar{y}$. Risulta abbastanza
naturale pensare che esista una relazione di dipendenza tra i due caratteri se a valori positivi (negativi) dello scarto
$x_i - \bar{x}$ corrispondono sistematicamente o quasi sempre valori positivi o negativi dello scarto $y_i - \bar{y}$.

/Covarianza/: si definisce covarianza $c_{xy}$ (dei dati o campionaria) delle due serie di dati $\{x_i\}\ \{y_i\}$:
$c_{xy} = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})$, $s^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2$.

La covarianza assume un valore positivo (negativo) che diviene grande in valore assoluto nel caso in cui i termini prodotto
$(x_i - \bar{x}) (y_i - \bar{y})$ abbiano segni concordi (positivi o negativi che siano). In questo caso si parla di
serie statistiche fortemente correlate o per meglio dire di dati delle serie fortemente correlati.

Nel caso opposto vale a dire nel caso in cui i dati delle serie siano incorrelati (non vi è dipendenza degli uni
dagli altri) avremo che i prodotti $(x_i - \bar{x})(y_i - \bar{y})$ avranno segni diversi (discordi in segno) e la
covarianza, per come definita, risulterà piccola in valore assoluto (prossima al valore 0). La covarianza può
essere calcolata anche tramite la seguente formula:

$c_{xy} = \frac{1}{n} \sum_{i=1}^n x_i y_i - \bar{x} \bar{y}$, $s^2 = \frac{1}{n} \sum_{i=1}^n x_i^2 - \bar{x}^2$.

Nel caso in cui i dati si riferiscano a caratteri quantitativi discreti, di cui è nota la distribuzione di
frequenza doppia, è possibile utilizzare le seguenti formule per il calcolo della covarianza.

$c_{xy} = \sum_{j=1}^N \sum_{k=1}^M (s_j - \bar{x})(u_k - \bar{y}) p_{jk} = \sum_{j=1}^N \sum_{k=1}^M s_j u_k p_{jk} - \bar{x} \bar{y}$.

Due serie di dati $\{x_i\}\ \{y_i\}$ si dicono:
- Statisticamente incorrelate se $\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) = 0$ e quindi $c_{xy} = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) = 0$;
- Statisticamente indipendenti se vale la seguente condizione:
  $\forall j = 1, \dots, N$ e $k = 1, \dots, M \quad p_{jk} = p_j p_k$, con $p_{jk} = \frac{f_{jk}}{n}$, $p_j = \frac{f_j}{n}$, $p_k = \frac{f_k}{n}$.

Due serie di dati statisticamente indipendenti sono incorrelate mentre non è necessariamente vero il contrario.

Statisticamente indipendenti $\implies$ statisticamente incorrelate.

Statisticamente incorrelate $\not\implies$ statisticamente indipendenti.

Infatti: $\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) = \sum_{i=1}^n (x_i - \bar{x}) \sum_{i=1}^n (y_i - \bar{y}) = 0$.

Nel caso bidimensionale (variabili $x$ e $y$) la covarianza si può rappresentare attraverso una matrice
$2 \times 2$:
\begin{equation*}
c = 
\begin{bmatrix}
c_{xx} &c_{xy}\\
c_{xy} &c_{yy}
\end{bmatrix}
=
\begin{bmatrix}
var(x) &cov(x, y)\\
cov(x, y) & var(y)
\end{bmatrix}
\end{equation*}
È indipendente dalla grandezza delle varianze.

Per una misura indipendente dalla variabilità delle grandezze si usa la matrice di correlazione:
\begin{equation*}
\text{Corr} =
\begin{bmatrix}
\frac{c_{xx}}{\sigma_x^2} &\frac{c_{xy}}{\sigma_x \sigma_y}\\
\frac{c_{xy}}{\sigma_x \sigma_y} &\frac{c_{yy}}{\sigma_y^2}
\end{bmatrix}
=
\begin{bmatrix}
1 &corr(x, y)\\
corr(x, y) &1
\end{bmatrix}
\end{equation*}
Nel caso di $m$ variabili,
\begin{equation*}
\text{Corr} =
\begin{bmatrix}
1 &\frac{c_{x_1 x_2}}{\sigma_{x_1} \sigma_{x_2}} &\dots &\frac{c_{x_1 x_m}}{\sigma_{x_1} \sigma_{x_m}}\\
\frac{c_{x_1 x_2}}{\sigma_{x_1} \sigma_{x_2}} &1 &\dots &\vdots\\
\vdots &\ddots &\ddots &\vdots\\
\frac{c_{x_1 x_m}}{\sigma_{x_1} \sigma_{x_m}} &\dots &\dots &1
\end{bmatrix}
\end{equation*}

/Regressione lineare per serie di dati/

Consideriamo un campione costituito da un insieme $E$ di coppie di dati, relativi a due caratteri $x$ ed $y$:
$E = \{L(x_1, y_1), \dots, (x_n, y_n)\}$. In molti casi ci si pone la questione se tra tali caratteri $x$ ed $y$ esista
un legame di tipo funzionale o una relazione di tipo funzionale che ne descriva in modo soddisfacentemente
corretto il legame realmente esistente. Un'analisi tesa a rispondere a una tale questione viene detta /analisi
di regressione/.

Un tale studio viene affrontato pensando ad uno dei due caratteri come ad una /variabile indipendente/, sia
per esempio il carattere $x$, e cercando di stabilire quale funzione $f$, all'interno di una ben determinata
classe, consenta di scrivere la seguente relazione $y = f(x)$ in modo che essa descriva al meglio il legame
tra la variabile indipendente $x$ e il carattere $y$ che a questo punto viene interpretato coma /variabile
dipendente/. Occorre specificare cosa si intende per funzione che meglio descrive il legame tra i due caratteri.
Solitamente si determina la funzione $f$ che minimizza le distanze tra i valori osservati del carattere $y$ e quelli
che si otterrebbero per il carattere $y$ se la relazione che lega il carattere $y$ ad $x$ fosse proprio quella descritta
da $f$.

In altri termini quello che si cerca è la funzione $f$ che minimizza la seguente quantità $g(f) = \sum_{i=1}^n [f(x_i) - y_i]^2$ dove
il quadrato si utilizza affinché le distanze vengano tutte considerate con segno positivo. Nel caso particolare in cui $f$
sia vincolata ad essere una funzione lineare (retta) allora parleremo di /regressione lineare/.

Nel caso della /regressione lineare/ il problema si riduce alla determinazione dei coefficienti $m$ e $q$ della retta
$y = mx + q$ per cui risulti minima la quantità $g(m, q) = \sum_{i=1}^n [mx_i + q - y_i]^2$.

I valori $mx_i + q$ sono proprio i valori $f(x_i)$ che rappresentano l'approssimazione alle $y_i$ tramite
la funzione $f$.

Risolvendo il sistema algebrico ottenuto e ricordando le definizioni di varianza e covarianza si ottiene in
definitiva $m = \frac{c_{xy}}{s_x^2}$ e $q = \bar{y} - \frac{c_{xy}}{s_x^2} \bar{x}$.

Questo metodo consente di determinare la retta che meglio descrive la relazione tra i due caratteri senza
peraltro fornire alcuna indicazione circa il grado di approssimazione che è in grado di offrire. Per tale
motivo è stata introdotta una nuova grandezza detta /coefficiente di correlazione lineare/: $r_{xy} = \frac{c_{xy}}{s_x s_y}$.

L'importanza di tale coefficiente deriva dal fatto che esso assume valori sempre appartenenti all'intervallo
$[-1, 1]$. Inoltre:
- È nullo nel caso in cui le serie di dati sono /statisticamente incorrelate/.
- È pari ad 1 (in valore assoluto) quando le /coppie di dati/ si trovano /esattamente sulla retta/
  $y = mx + q$.
Pertanto esso rappresenta /il grado di allineamento delle coppie di dati/.

Per esempio, supponiamo di voler controllare la resistenza di un campione di 15 travi di cemento, tutte
ottenute dalla stessa gettata, misurando sia i carichi di prima lesione che quelli di rottura finale e
supponiamo che i dati disponibili siano i seguenti:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 14:22:07
[[file:Lezioni/screenshot_2018-03-01_14-22-07.png]]

È possibile computare il coefficiente di correlazione che sarà pari a $r_{xy} = 0.0195$.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 14:22:49
[[file:Lezioni/screenshot_2018-03-01_14-22-49.png]]

/Regressione (modello lineare) per serie di dati/

Abbiamo accennato in precedenza al fatto che non si è sempre vincolati alla scelta di una retta tra le
funzioni che possono descrivere la relazione tra le due serie di dati.

Quanto esposto in precedenza può essere applicato anche nel caso in cui si considerino relazioni funzionali
di diversa natura, la cui scelta può essere suggerita da una qualche impressione derivante da ispezioni
visive dei dati o da altre forme di conoscenza circa il fenomeno analizzato.

Per esempio, si faccia ancora riferimento ai dati dell'esempio precedente e si valuti l'opportunità di
sfruttare il seguente modello:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-01 14:25:32
[[file:Lezioni/screenshot_2018-03-01_14-25-32.png]]

Si noti inoltre che molte relazioni funzionali non lineari possono essere ricondotte a tali (lineari)
con opportune trasformazioni delle variabili. Per esempio, una relazione del tipo $y = a \cdot e^{bx}$ può
essere riscritta come $\tilde{y} = \beta \cdot \tilde{x} + \alpha$, dove $\tilde{y} = \log{(y)}$, $\tilde{x} = x$ e
$\alpha = \log{(a)}$, $\beta = b$.

La determinazione dei coefficienti $a$ e $b$ che meglio permettono di approssimare una serie di punti
$\{x_i, y_i\}$ può essere effettuata riconducendosi ad una regressione lineare ovvero determinando i
coefficienti $\alpha$ e $\beta$ che meglio approssimano, linearmente, la serie dei punti $\{\tilde{x_i} \tilde{y_i}\}$, dove
$\tilde{y_i} = \log{(y_i)}$ e $\tilde{x_i} = x_i$. Una volta determinati tali coefficienti, il calcolo di $a$ e $b$
risulta immediato.

Alcune funzioni riconducibili a lineari:
- $y = a \log{(x)} + b$;
- $y = a x^b$;
- $y = \frac{1}{a + b \cdot e^{-x}}$, $\beta = b$.

** Calcolo delle probabilità
Teoria (assiomatica o frequentista o soggettiva) che riguarda il calcolo della probabilità del verificarsi di certi
eventi composti di eventi elementari. È lo strumento base per la /statistica/, che trae conclusioni su una popolazione,
utilizzando i dati osservati su una collezione di individui (campione) appartenenti alla popolazione (inferenza su
popolazione).

Al fine di presentare l'/impostazione assiomatica/ supponiamo di voler studiare una situazione (esperimento) avente un 
insieme $\Omega$ di diversi possibili esiti, ben distinti tra loro. Ogni sottoinsieme $A$ di $\Omega$ viene detto /evento/.
Ad ogni evento $A$ è associabile una quantità numerica detta /probabilità/ e denotata tramite $P(A)$ il cui significato
varia a seconda dell'impostazione.

/Impostazione assiomatica/

L'insieme $\Omega$ che può contenere un numero finito o infinito di elementi viene detto /spazio campione/.

Ogni evento può essere /elementare/ se è costituito da un elemento singolo di $\Omega$, oppure /composto/ in caso
contrario.

Due eventi $A$ e $B$ vengono detti /incompatibili/ se sono /sottoinsiemi disgiunti/.

Si definisce /insieme delle parti/ di $\Omega$ l'insieme di tutti i suoi sottoinsiemi e lo si denota tramite $\mathcal{P}(\Omega)$

La probabilità deve essere definita per tutti gli elementi di $\mathcal{P}(\Omega)$ con particolari proprietà (algebra di Boole
o \(\sigma\)-algebre).

Per esempio:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-06 11:50:52
[[file:Lezioni/screenshot_2018-03-06_11-50-52.png]]

Possibili eventi sono i suoi sottoinsiemi, $\{R\}$ e $\{B, G\}$

Un altro esempio:

Consideriamo un esperimento che può fornire come risultato un qualsiasi valore reale in un intervallo
$[a, b] \subseteq \mathbb{R}$.

In questo caso avremo:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-06 11:54:12
[[file:Lezioni/screenshot_2018-03-06_11-54-12.png]]

/Definizione di probabilità secondo Kolmogorov/

Viene detta /misura di probabilità/ ogni applicazione $P : \mathcal{P}(\Omega) \to \mathbb{R}_0^+$ che associa
un valore reale ad ogni sottoinsieme di $\Omega$ e per cui valgono le seguenti proprietà:
- Per ogni $A \subseteq \Omega$ esiste ed è unico un numero $P(A) \geq 0$.

  Interpretando $P(A)$ come /frequenza relativa/ dell'evento $A$, essa è compresa tra 0 e 1.

- $P(\Omega) = 1$;
- Data la famiglia $\{A_i, i \in I \subseteq \mathbb{N}\}$ di eventi incompatibili vale
  $P\left(\bigcup_{i \in I} A_i \right) = \sum_{i \in I} P(A_i)$;

Ogni /misura di probabilità/ è una funzione che assegna valori numerici a sottoinsiemi di $\Omega$ e non ai suoi
elementi (eventi elementari), come contrariamente si è portati a pensare.

Per esempio consideriamo lo spazio campione: $\Omega = [a, b] \subseteq \mathbb{R}$. Una misura di probabilità
da assegnare a $\mathcal{P}(\Omega) \to \mathbb{R}_0^+$ definita ponendo $P((c, d)) = \frac{d-c}{b-a}$ quando
l'evento che consideriamo è un intervallo $(c, d) \subseteq [a, b] \quad a \leq c < d \leq b$. Possiamo, per
esempio, porre $P(\{s\}) = 0$ per ogni $s \in [a, b]$, in modo da poter calcolare anche la probabilità
di eventi con estremi inclusi.

Dalle 3 proprietà che definiscono le misure di probabilità, discendono in modo immediato alcune /proprietà
aggiuntive/. Sia $P$ una misura di probabilità definita sull'insieme delle parti $\mathcal{P}(\Omega)$ di uno
spazio campione $\Omega$.
Allora:
- Per ogni $A \subseteq \Omega$ vale $P(\bar{A}) = 1 - P(A)$;
- Per ogni $A \subseteq \Omega$ vale $P(A) \leq 1$;
- Per ogni $A, B \subseteq \Omega$ se $A \subseteq B$ allora $P(A) \leq P(B)$;
- Per ogni $A, B \subseteq \Omega$ anche non incompatibili, vale $P(A \cup B) = P(A) + P(B) - P(A \cap B)$;
- Per ogni $A \subseteq \Omega$ vale $P(\bar{A}) = 1 - P(A)$, infatti $1 = P(A \cup \bar{A}) = P(A) + P(\bar{A}) - P(A \cap \bar{A}) =
  P(A) + P(\bar{A}) - 0$;
- Per ogni $A, B \subseteq \Omega$ anche non incompatibili, vale $P(A \cup B) = P(A) + P(B) - P(A \cap B)$;
- Per ogni $A, B \subseteq \Omega$ anche non incompatibili, vale $P(A \cup B) = P(A \cap \bar{B}) + P(A \cap B) + P(\bar{A} \cap B)$;
- Per ogni $A, B \subseteq \Omega$ anche non incompatibili, vale $P(A) = P(A \cap \bar{B}) + P(A \cap B)$ e $P(B) = P(A \cap B) + P(\bar{A} \cap B)$.

In base alle precedenti proprietà è possibile osservare che la proprietà:
- Data la famiglia $\{A_i, i \in I \subseteq N\}$ di eventi incompatibili vale $P\left(\bigcup_{i \in I} A_i \right) = \sum_{i \in I} P(A_i)$
È un caso particolare della proprietà:
- Per ogni $A, B \subseteq \Omega$ anche non incompatibili, vale $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ nel caso in cui gli
  eventi $A$ e $B$ siano incompatibili ($A \cap B = \emptyset$).

Per esempio, supponiamo di avere effettuato un investimento immobiliare e di avere a disposizione 3 appartamenti di
diverso valore da vendere. Supponiamo di essere interessati a fare delle stime sui ricavi del nostro investimento
tenendo conto di quanti di tali appartamenti saranno stati venduti entro la fine dell'anno. Un modo per descrivere
tutte le possibili situazioni relative alle vendite alla fine dell'anno consiste nell'associare a ciascuna di esse
una terna $(\alpha_1, \alpha_2, \alpha_3)$, dove $\alpha_i = 1$ se l'appartamento \(i\)-mo sarà stato venduto alla fine dell'anno e
$\alpha_i = 0$ in caso contrario.

In questo modo l'insieme delle possibili situazioni alla fine dell'anno può essere descritta come
$\Omega = \{(0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)\}$.

Supponiamo ora che gli 8 casi realizzabili (eventi elementari) siano egualmente possibili ovvero che la
misura di probabilità adatta a descrivere il fenomeno sia la $P$ definita come segue:
$P(A) = \frac{\text{numero di elementi di } A}{\text{numero di elementi di } \Omega}$ per ogni elemento $A$ di $\Omega$.

Consideriamo ora l'evento $A$: "almeno un appartamento viene venduto".

Siamo pertanto interessati al seguente evento: $A = \{(1, 0, 0), (0, 1, 0), (0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)\}$.
Poiché esso contiene 7 elementi su un totale di 8 casi possibili, allora: $P(A) = \frac{\text{numero di elementi di } A}{\text{numero di elementi di } \Omega}
= \frac{7}{8}$.

Consideriamo ora l'evento $B$: "almeno due appartamenti vengono venduti". È abbastanza chiaro che ogni situazione di vendita contenuta
in $B$ lo è a maggior ragione in $A$ (almeno 1 venduto contiene almeno 2 venduti) per cui $B \subseteq A$. In effetti avremo
$B = \{(1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)\}$ da cui si ricava $P(B) = \frac{4}{8} = \frac{1}{2}$, $P(A) = \frac{7}{8}$.
Pertanto vale quanto asserito in precedenza, ovvero che $B \subseteq A \implies P(B) \leq P(A)$. Infine, consideriamo l'evento
$C$ "il terzo appartamento viene venduto", ovvero $C = \{(0, 0, 1), (1, 0, 1), (0, 1, 1), (1, 1, 1)\}$. Supponiamo
di essere interessati a determinare la probabilità di $B \cup C$: $P(B \cup C) = P(B) + P(C) - P(B \cap C) = \frac{1}{2} + \frac{1}{2}
= \frac{5}{8}$ essendo $B \cap C = \{(1, 0, 1), (0, 1, 1), (1, 1, 1)\}$. In effetti $P(B \cup C)$ avrebbe potuto essere determinata direttamente
osservando che $B \cup C = \{(0, 0, 1), (1, 1, 0), (1, 0, 1), (0, 1, 1), (1, 1, 1)\}$.

Esempio: da un'urna con 6 palline bianche e 5 palline nere se ne estraggono 2. Qual è la probabilità che una delle palline
estratte sia bianca e l'altra nera?.

Lo spazio campione ha $11 \cdot 10 = 110 \text{elementi}$. Ci sono $6 \cdot 5 = 30$ modi in cui la prima estratta è bianca e la seconda è nera,
e $5 \cdot 6 = 30$ modi in cui la prima estratta è nera e la seconda è bianca. Quindi, pensando che tutti i punti dello spazio
campione siano egualmente probabili, la probabilità cercata è: $\frac{30 + 30}{110} = \frac{6}{11}$.

Secondo esempio: in quanti modi si possono ordinare le lettere $a, b, c$ (senza ripetizione)?

Ce ne sono 6 ($3 \cdot 2$): abc, acb, bac, bca, cab, cba. Ciascuno di questi ordinamenti è una /permutazione/.

In generale $n \cdot (n-1) \cdot \cdots \cdot 2 \cdot 1 = n!$

Terzo esempio: qual è la probabilità che tra $n$ persone ce ne siano almeno 2 che compiono gli anni lo stesso
giorno dell'anno? → $356 \cdot \dots 365 = 365^n$ possibilità.

Ragioniamo sull'evento negato, calcoliamo la probabilità che le $n$ persone abbiano tutte compleanni in giorni
diversi:
- La prima persona ha 365 possibili giorni per il proprio compleanno;
- La seconda può averlo solo nei rimanenti 364;
- La terza solo nei rimanenti 363.
- Ecc.

La probabilità che i compleanni non coincidano in nessun caso (per nessuna delle $n$ persone) è:
$\frac{365 \cdot 364 \cdot \cdots \cdot (365 - n+1)}{365^n}$. Pertanto, la probabilità che almeno 2 persone su $n$ compiano
gli anni gli anni lo stesso giorno è $1 - \frac{365 \cdot 364 \cdot \dots \cdot (365 - n+1)}{365^n}$

Nel caso di n = 23:

La probabilità che i compleanni non coincidano in nessuno caso (per nessuna delle $n$ persone) è:
$\frac{365 \cdot 364 \cdot \cdots \cdot (365 - 23 + 1)}{365^n} = 0.4927$.

Pertanto, la probabilità che almeno 2 persone su $n$ compiano gli anni lo stesso giorno è:
$1 - \frac{365 \cdot 364 \cdot \cdots \cdot (365 - 23 + 1)}{365^n} = 0.5073$.

Si voglia determinare il numero di differenti gruppi di $r$ oggetti che è possibile costruire (formare)
usando $n$ oggetti diversi (non importa l'ordine degli oggetti).

Per esempio: quanti gruppi diversi di 3 oggetti è possibile costruire dai 5 oggetti $A, B, C, D, E$?

- Il primo può esser scelto in 5 modi diversi;
- Il secondo in 4;
- Il terzo in 3.

Però, lo stesso gruppo di 3, per esempio $ABC$, può presentarsi come $ABC, ACB, BAC, BCA, CAB, CBA$, ma il gruppo è
sempre lo stesso. Perciò il numero di gruppi diversi è: $\frac{5 \cdot 4 \cdot 3}{3 \cdot 2 \cdot 1} = 10$.

In generale: $\frac{n (n-1) \cdots (n - r +1)}{r!} = \frac{n!}{(n-r)! r!} = \binom{n}{r}$
detto /coefficiente binomiale/, numero delle /combinazioni/ di $n$ oggetti presi a gruppi di $r$.

Esempio: si selezioni a caso un gruppo di 5 persone da un insieme di 6 uomini e 9 donne. Qual è la probabilità che nel gruppo
selezionato ci siano esattamente 3 uomini e 2 donne?

Ognuna delle $\left(\begin{smallmatrix}15\\5\end{smallmatrix}\right)$ combinazioni è ugualmente probabile. Ci sono
$\left(\begin{smallmatrix}6\\3\end{smallmatrix}\right)$ scelte possibili per la scelta dei 3 uomini e
$\left(\begin{smallmatrix}9\\2\end{smallmatrix}\right)$ per la scelta delle 2 donne.

Perciò la probabilità $\frac{\left(\begin{smallmatrix}6\\3\end{smallmatrix}\right) \cdot \left(\begin{smallmatrix}9\\2\end{smallmatrix}\right)}{\left(\begin{smallmatrix}15\\5\end{smallmatrix}\right)}
= \frac{240}{1001}$

Siano dati uno spazio campione $\Omega$ ed una misura di probabilità $P$ definita sul suo insieme delle parti $\mathcal{P}(\Omega)$

Secondo l'impostazione assiomatica di probabilità, considerati due eventi, $A$ e $B$, con $P(B) > 0$, è detta /probabilità
dell'evento/ $A$ condizionata dall'evento $B$ la quantità $P(A|B) = \frac{P(A \cap B)}{P(B)}$.

Per esempio: si consideri un lotto di produzione di transistor. 5 di questi sono difettosi immediatamente, 10 parzialmente
difettosi (non funzionano dopo un uso di 2 ore), e 25 sono accettabili. Si scelga a caso un transistor. Se non risulta
immediatamente difettoso, qual è la probabilità che sia accettabile (cioè funzioni dopo un uso di 2 ore)?

Poiché non risulta immediatamente difettoso, non è uno dei 5: quindi
$$ P(\text{accett.}|\text{non  immed. difet.}) = \frac{P(\text{accett.} \cap \text{non immed. difet.})}{P(\text{non immed. difet.})}
= \frac{P(\text{accett.})}{P(\text{non immed. difet.})} = \frac{\frac{25}{40}}{\frac{35}{40}} = \frac{5}{7}$$

Si riprenda in esame l'esempio degli appartamenti. Si supponga di essere interessati a determinare la probabilità dell'evento $B$
"almeno 2 appartamenti verranno venduti" sapendo con certezza che si è verificato l'evento $C$ "il terzo appartamento viene venduto".

In questo caso ci troviamo di fronte ad un condizionamento, infatti il verificarsi certo dell'evento $C$ influisce sulla
possibilità che si verifichi anche $B$. La nuova probabilità da associare a $B$ diventa allora: $P(B \cap C) = \frac{3}{8}$.

$$P(B|C) = \frac{P(B \cap C)}{P(C)} = \frac{P(B \cap C)}{P(C)} = \frac{\frac{3}{8}}{\frac{4}{8}} = \frac{3}{4}$$
con $P(B) = \frac{1}{2}$ e $P(C) = \frac{4}{8}$

Due eventi $A, B \in \mathcal{P}(\Omega)$ sono detti /stocasticamente indipendenti/ se vale la seguente condizione: $P(A) = P(A|B)$
ovvero se vale $P(B) = P(B|A)$, o ancora se $P(A \cap B) = P(A)P(B)$.

Si continui con l'ultimo esempio: si consideri l'evento $C$ "il terzo appartamento viene venduto" e l'evento $D$
"il secondo appartamento viene venduto". In base alla definizione condizionata risulta
$$ P(C|D) = \frac{P(C \cap D)}{P(D)} = \frac{\frac{2}{8}}{\frac{4}{8}} = \frac{1}{2} = P(C)$$
con $P(C \cap D) = \frac{2}{8}$ e $P(D) = \frac{4}{8}$, essendo $D = \{(0, 1, 0), (1, 1, 0), (0, 1, 1), (1, 1, 1)\}$,
$C \cap D = \{(0, 1, 1), (1, 1, 1)\}$.

Pertanto è possibile concludere come gli eventi $C$ e $D$ risultino /stocasticamente indipendenti/. Infatti
varranno anche le altre relazioni collegate.

Osserviamo che l'indipendenza è influenzata non solo dagli eventi considerati ma anche dalla particolare misura di
probabilità $P$ adottata. Infatti, scelte differenti per $P$, legate a considerazioni non matematiche, avrebbero
potuto portare ad esempio ad avere gli eventi $C$ e $D$ non stocasticamente indipendenti.

Si riprenda in esame l'urna con 4 palline fisicamente identiche ma di diverso colore, una rossa, due blu e una verde,
(pertanto faremo riferimento alla probabilità $P'$ e non a $P$).

Supponiamo di estrarre in maniera casuale una pallina, e di fare poi una seconda estrazione tenendo fuori dall'urna
la pallina appena estratta (senza reimmissione). Si voglia calcolare la probabilità che le 2 palline estratte in questo
modo siano quelle blu.

Consideriamo gli eventi $A_1$: "la pallina è blu" e $A_2$: "la seconda pallina è blu" e calcoliamo la probabilità di
$A_1 \cap A_2$ facendo uso della seguente formula: $P'(A_1 \cap A_2) = P'(A_2|A_1) P'(A_1)$. A tal scopo si osservi che
$P'(A_1) = \frac{1}{2}$ e $P'(A_2|A_1) = \frac{1}{3}$. In definitiva avremo $P'(A_1 \cap A_2) = P'(A_2|A_1) P'(A_1) = \frac{1}{2}
\frac{1}{3} = \frac{1}{6}$.

Nello stesso caso, ma /con reimmissione/ della pallina pescata: $P'(A_1 \cap A_2) = P'(A_1) P'(A_1) = \frac{1}{2}
\frac{1}{2} = \frac{1}{4}$.

La formula $P (A \cap B) = P(A|B) P(B)$ può essere generalizzata al caso dell'intersezione di più eventi, in tal caso
prende il nome di "formula del prodotto" e diviene $P(A_1 \cap \dots \cap A_n) = P(A_1) P(A_2|A_1) \dots P(A_n|A_1 \cap \dots \cap A_{n-1})$ ed è
valida comunque sia scelto $n \in \mathbb{N}_+$ e la famiglia $\{A_i, i = 1, \dots, n\}$ di sottoinsiemi di $\Omega$.

Torniamo all'urna e supponiamo di effettuare 3 estrazioni, senza reimmissione. Si vuole calcolare la probabilità che
la prima volta venga estratta una pallina blu, la seconda una rossa e la terza una verde e denotiamo questi eventi
rispettivamente con $A_1, A_2$ e $A_3$: $P(A_1 \cap A_2 \cap A_3) = P(A_1) P(A_2|A_1)P(A_3|A_1 \cap A_2)$. $P(A_1) = \frac{1}{2}$,
$P(A_2|A_1) = \frac{1}{3}$ e $P(A_3|A_1 \cap A_2) = \frac{1}{2}$. Quindi $P(A_1 \cap A_2 \cap A_3) = \frac{1}{2} \frac{1}{3}
\frac{1}{2} = \frac{1}{12}$.

È utile ricordare altre formule particolarmente importanti nel calcolo delle probabilità. Consideriamo una partizione
$\{A_i, i = 1, \dots, n; A_i \subseteq \Omega\}$ dello spazio campione $\Omega$ ovvero una famiglia di eventi /mutualmente incompatibili/
e tali che la loro unione sia $\Omega$ stesso. La prima formula è nota come "formula delle probabilità totali":
$$P(B) = \sum_{i=1}^n P(B|A_i) P(A_i)$$

Caso particolare: $A$ e $B$ siano eventi. $A = (A \cap B) \cup (A \cap \bar{B})$. $(A \cap B)$ e $(A \cap \bar{B})$ sono esclusivi,
quindi:
$$ P(A) = P(A \cap B) + P(A \cap \bar{B}) = P(A|B) P(B) + P(A|\bar{B}) P(\bar{B}) = P(A|B) P(B) + P(A|\bar{B})[1 - P(B)]$$
Permette di calcolare la probabilità di un evento condizionando al fatto che un secondo evento si sia o meno verificato.

Per esempio, un'assicurazione decide di dividere i guidatori in 2 categorie: facili agli incidenti ($A$) e non facili
($\bar{A}$). Le statistiche dicono che un guidatore facile agli incidenti ha probabilità 0.4 di fare un incidente
nell'anno mentre uno non facile agli incidenti ha probabilità 0.2 di fare un incidente nell'anno.

Supponiamo che il 30% dei guidatori sia facile agli incidenti, qual è la probabilità che un guidatore abbia un incidente
nell'anno?

Sia $A_1$ l'evento "il guidatore ha un incidente": $P(A_1) = P(A_1|A) P(A) + P(A_1|\bar{A}) P(\bar{A}) = 0.4 \cdot 0.3 + 0.2 \cdot 0.7
= .26$

Se un cliente ha effettivamente un incidente nell'anno, qual è la probabilità che sia facile agli incidenti?

Si sa che inizialmente la stima della probabilità che fosse facile agli incidenti era 0.3. Sapendo che ha avuto un incidente,
rivalutiamo questa probabilità con la nuova informazione:

Dalle formule viste:
$$P(A|A_1) = \frac{P(A, A_1)}{P(A_1)} = \frac{P(A) P(A_1|A)}{P(A_1)} = \frac{(.3)(.4)}{(.26)} = .4615$$

Esempio: un laboratorio di analisi del sangue è in grado di scoprire con probabilità 0.99 se una certa malattia è presente.
C'è però una probabilità di 0.01 di falso positivo (cioè anche se la persona è sana il test la valuta come malata).

Se la percentuale di popolazione malata è 0.5% qual è la probabilità che una persona sia effettivamente malata se viene
valutata come malata dal test?

Sia $D$ l'evento: una persona è malata; $E$ l'evento: il test è positivo (persona valutata come malata). Si deve
calcolare:
$$P(D|E) = \frac{P(D, E)}{P(E)} = \frac{P(E|D) P(D)}{P(E|\bar{D}) P(\bar{D}) + P(E|D) P(D)} =
\frac{(.99)(.005)}{(.01)(.995) + (.99)(.005)} = .3322$$

È un caso particolare della seconda formula molto importante: la /formula di Bayes/.

La "formula di Bayes" nella forma generale per eventi $B$ tali che $P(B) > 0$ è data da
$$ P(A_i|B) = \frac{P(B|A_i) P(A_i)}{\sum_{j=1}^n P(B|A_j) P(A_j)}$$
ed è valida per ogni $i = 1, \dots, n$.

Si osservi che anche in questo caso è necessario richiedere che la famiglia $\{A_i, i = 1, \dots, n; A_i \subseteq \Omega\}$
sia una partizione dello spazio campione $\Omega$.

Esempio: supponiamo di aver effettuato un'indagine sugli individui in età lavorativa abitanti in un quartiere di una
data città italiana, e di aver riscontrato che il 40% di tal individui ha la licenza elementare o media, il 50% ha
un titolo di scuola superiore mentre il restante 10% è laureato.

Una seconda indagine ha permesso di rilevare i tassi di disoccupazione tra i tre gruppi di individui che risultano
essere rispettivamente 15%, 5% e 10%.

Supponiamo ora di assegnare un numero ad onguno di tali individui e di estrarre uno dei numeri così assegnati,
selezionando quindi in modo del tutto casuale un singolo individuo tra tutti. Si vuole determinare la
probabilità dell'evento $B$ "l'individuo che verrà estratto è disoccupato".

Il metodo migliore per calcolare tale probabilità consiste nel far ricorso alla formula delle probabilità totali,
denotando: $A_1$ "l'indiviudo ha la licenza elementare o media", $A_2$ "l'individuo ha un titolo di scuola superiore",
$A_3$ "l'individuo ha la laurea". Sfruttando l'informazione disponibile assegneremo le seguenti probabilità:
$P(A_1) = 0.4, P(A_2) = 0.5, P(A_3) = 0.1$.

Inoltre, sempre in base ai risultati delle indagini avremo $P(B|A_1) = 0.15, P(B|A_2) = 0.05, P(B|A_3) = 0.1$.
Osservando che la terna $A_1, A_2$ ed $A_3$ costituisce una partizione dello spazio campione si può applicare la
formula delle probabilità totali ottenendo: $P(B) = \sum_{i=1}^3 P(B|A_i) P(A_i) = 0.4 \cdot 0.15 + 0.5 \cdot 0.05 + 0.1 \cdot 0.1 = 0.095$.

Supponiamo ora di essere certi che l'individuo estratto sia un disoccupato, e di essere interessati a determinare
la probabilità, condizionata a tale informazione, che esso sia laureato.

Allora è possibile far uso della Formula di Bayes, in base alla quale si ricava:
$$ P(A_3|B) = \frac{P(B|A_3) P(A_3)}{\sum_{j=1}^3 P(B|A_j) P(A_j)} = \frac{0.1 \cdot 0.1}{0.095} = 0.105$$

Delle opportune trasformazioni, chiamate "variabili aleatorie", consentono di ricondursi sempre ad $\mathbb{R}$ come spazio
campione e di considerare quali suoi sottoinsiemi tutti gli intervalli del tipo (a, b) o [a, b] con $-\infty \leq a < b \leq +\infty$,
tutte le possibili unioni ed intersezioni (finite o infinite), e tutti i loro complementi. Formalmente:
dato  uno spazio campione $\Omega$, è detta "variabile aleatoria" (o casuale) un'applicazione $X: \Omega \to \mathbb{R}$ che associa un
numero reale ad ogni elemento di $\Omega$.

In base a questa definizione è possibile assegnare delle probabilità ad eventi del tipo $X \in B \subseteq \mathbb{R}$ essendo
$P(\{X \in B\}) = P(\{\omega \in \Omega | X(\omega) \in B\})$ dove $P$ rappresenta una misura di probabilità definita su $\mathcal(P)(\Omega)$.
Per comodità denoteremo da qui in avanti con $P(X \in B)$ tali probabilità, ma ci si ricordi sempre che $X \in B$ va pensato
come evento di $\Omega$.

Facciamo riferimento all'esempio degli appartamenti e consideriamo lo spazio $\Omega$ e la probabilità $P$ in esso definiti.
Una variabile aleatoria che ha senso considerare in questo caso potrebbe essere la $X :=$ "numero di appartamenti venduti
a fine anno". Formalmente essa andrebbe definita come funziona da $\Omega$ in $\mathbb{R}$ che assegna:
- $X((0, 0, 0)) = 0$
- $X((0, 0, 1)) = X((0, 1, 0)) = X((1, 0, 0)) = 1$
- $X((0, 1, 1)) = X((1, 1, 0)) = X((1, 0, 1)) = 2$
- $X((1, 1, 1)) = 3$

In base alla definizione della variabile casuale $X$ che abbiamo fornito, ha senso definire la probabilità che
esattamente un appartamento venga venduto $X$ = 1. Infatti, vale quanto segue:
$$ P(X=1) = P(\omega \in \Omega : X(\omega) = 1) = P(\{(0, 0, 1), (0, 1, 0), (1, 0, 0)\}) = \frac{3}{8}$$

La definizione di variabile aleatoria può risultare poco chiara dal punto di vista intuitivo anche se per gli scopi
del corso sarà necessario pensare alle variabili aleatorie come ad esiti esprimibili numericamente di esperimenti
ancora da effettuare, dove per esperimento intenderemo un qualsiasi fenomeno o situazione con sviluppi imprevedibili
a priori.

Come notazione adotteremo quella solitamente utilizzata in campo statistico, indicando con lettere maiuscole le
variabili aleatorie e con lettere minuscole le rispettive possibili realizzazioni.

Essendo imprevedibile a priori il valore assunto da una variabile aleatoria, tutto ciò che si può fare relativamente ad
essa è esprimere delle valutazioni di tipo probabilistico sui valori che essa assumerà. Per tale ragione ad ogni variabile
aleatoria $X$ è associata una funzione che esprime in modo chiaro tali valutazioni. Essa è la /funzione di ripartizione/:
$$F_X : \mathbb{R} \to [0, 1] \subseteq \mathbb{R}$$
definita come $F_X(t) = P(X \leq t)$ per ogni valore di $t \in \mathbb{R}$.
Riconsideriamo la variabile aleatoria definita nell'esempio precedente, e determiniamone la corrispondente funzione di
ripartizione. Per questo osserviamo prima di tutto che la $X$ può assumere solo i valori 0, 1, 2 e 3.
Quindi sicuramente sarà $F_X(t) = P(X \leq t) = 0$ per $t < 0$. Avremo poi
\begin{align*}
&F_X(t) = P(X = 0) = P(\{0, 0, 0\}) = \frac{1}{8} \text{ per } t <0\\
&F_X(t) = P(X = 0 \text{ oppure } 1) = P(\{(0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1)\}) = \frac{1}{2} \text{ per } 1 \leq t \leq 2\\
&F_X(t) = P(X = 0 \text{ oppure } 1 \text{ oppure } 2) = 1 - P(X = 3) = 1 - P(\{(1, 1, 1)\}) = 1 - \frac{1}{8} = \frac{7}{8} \text{ per } 2 \leq t < 3\\
&F_X(t) = P(X = 0 \text{  oppure  } 1 \text{ oppure } 2 \text{ oppure } 3) = P(\Omega) = 1 \text{ per } t \geq 3
\end{align*}

La funzione di ripartizione risulta essere descritta dal grafico nella figura sottostante:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 10:05:12
[[file:Lezioni/screenshot_2018-03-14_10-05-12.png]]

Una volta nota la funzione di ripartizione di una variabile aleatoria, è possibile determinare la probabilità che
essa assuma valori in intervalli dell'asse reale di nostro interesse osservando che vale:
$$ P(X \in (a, b]) = F_X (b) - F_X (a) \text{ per ogni $a, b, \in \mathbb{R}$ con } a < b$$

La dimostrazione di questa uguaglianza segue dal fatto che, per ogni $a < b$, gli eventi $X \in (-\infty, a]$ ed $X \in (a, b]$
sono incompatibili e che dalla seguente proprietà:
- Data la famiglia $\{A_i, i \in I \subseteq N\}$ di eventi incompatibili vale $P\left(\bigcup_{i \in I} A_i \right) = \sum_{i \in I} P(A_i)$
  risulta $P(\{X \in (-\infty, a]\} \cup \{X \in (a, b]\}) = P(X \in (-\infty, a]) + P(X \in (a, b])$.
Vale allora anche la seguente relazione:
$$F_X (b) = P(X \leq b) = P(\{X \in (-\infty, a]\} \cup \{X \in (a, b]\}) = P(X \in (-\infty, a]) + P(X \in (a, b]) = F_X (a) + P(X \in (a, b])$$
da cui si ricava appunto la seguente: $P(X \in (a, b]) = F_X(b) - F_X(a)$ per ogni $a, b \in \mathbb{R}$ con $a < b$.

Per esempio, si voglia determinare la probabilità che la variabile aleatoria introdotta negli esempi precedenti assuma
valori in [1, 2]. Si osservi innanzitutto che in base alla seguente relazione:
- Per ogni $A, B \subseteq \Omega$ anche non incompatibili, vale $P(A \cup B) = P(A) + P(B) - P(A \cap B)$ vale nel caso specifico
  la seguente relazione $P(X \in [1, 2]) = P(X = 1) + P(X \in (1, 2])$ essendo $X=1$ ed $X \in (1, 2]$ eventi incompatibili.

Da cui segue che $P(X \in [1, 2]) = P(X = 1) + P(X \in (1, 2]) = \frac{3}{8} + [F_X(2) - F_X(1)] = \frac{3}{8} + \left[
\frac{7}{8} - \frac{1}{2} \right] = \frac{3}{4}$:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 11:40:34
[[file:Lezioni/screenshot_2018-03-14_11-40-34.png]]

Notiamo però che in genere la funzione di ripartizione di una variabile aleatoria non è nota; obiettivo della
statistica è quello di determinarla o di determinare grandezze ad essa associate, mentre nella probabilità e
nelle sue applicazioni si assume che essa sia nota.

È possibile dimostrare che sono delle funzioni di ripartizione tutte e sole le funzioni $F: \mathbb{R} \to [0, 1]$ che
godono simultaneamente delle seguenti proprietà:
- $F$ è monotona non decrescente;
- $\lim_{t \to +\infty} F(t) = 1$;
- $\lim_{t \to -\infty} F(t) = 0$;
- $\lim_{t \to t_0^+} F(t) = F(t_0)$ per ogni $t_0 \in \mathbb{R}$.

Per esempio, immaginiamo di essere interessati ad effettuare delle valutazioni sul tasso di inflazione $X$ che vi sarà
alla fine dell'anno. Poiché al momento attuale non è noto il valore che assumerà $X$, possiamo pensare ad esso come
ad una variabile aleatoria. Un economista contattato, in proposito afferma, in base alle sue considerazioni, che
$X$ è una variabile avente /funzione di ripartizione/ definita come segue:
\begin{equation*}
F_X (t) =
\begin{cases}
0 \quad &\text{per $t < 0$}\\
1-e^{-t} \quad &\text{per $t \geq 0$}
\end{cases}
\end{equation*}
Il grafico dell'andamento di tale funzione è riportato nella figura sottostante:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 12:08:28
[[file:Lezioni/screenshot_2018-03-14_12-08-28.png]]

È facile verificare che la funzione $F_X(t)$ così soddisfa le condizioni richieste, e pertanto risulta effettivamente una
funzione di ripartizione.

Supponiamo di essere ora interessati a determinare la probabilità che tale tasso sia compreso nell'intervallo $(1, 2]$, a
tal scopo sarà sufficiente applicare la seguente formula: $P(X \in (a, b]) = F_X(b) - F_X(a)$ per ogni $a, b \in \mathbb{R}$ con $a < b$,
che nel caso in questione diviene $P(X \in (1, 2])$ = F_X(2) - F_X(1) e dalla cui applicazione si ricava
$$P(X \in (1, 2]) = F_X(2) - F_X(1) = (1 - e^{-2}) - (1-e^{-1}) = 0.865 - 0.632 = 0.233$$

Si osservi che la relazione $P(X \in (a, b]) = F_X(b) - F_X(a)$ per ogni $a, b \in \mathbb{R}$ con $a < b$ può essere utilizzata
anche se desideriamo calcolare la probabilità che il tasso sia strettamente maggiore di 2, infatti averemo quanto segue:
$$P(X \in (2, +\infty]) = \lim_{t \to +\infty} F_X(t) - F_X(2) = 1 - (1 - e^{-2}) = 0.135$$
Si può inoltre osservare che le funzioni di ripartizione considerate negli ultimi 2 esempi, sebbene entrambe soddisfino
tutte le proprietà, presentano una significativa differenza:
- La prima non è una funzione continua su $\mathbb{R}$;
- La seconda è una funzione continua su $\mathbb{R}$.
  
In effetti le variabili aleatorie si distinguono in due categorie in base alla proprietà di continuità delle corrispondenti
funzioni di ripartizione:
- Variabile aleatoria /discreta/: l'insieme dei valori $S$ che essa può assumere (supporto) è finito o costituito da
  un'infinità di valori discreti. Ad ogni variabile aleatoria discreta è associabile, oltre alla funzione di ripartizione,
  una seconda funzione che fornisce delle valutazioni sulle probabilità che essa assuma specifici valori.

  Sia $S$ il supporto della variabile aleatoria discreta $X$. Viene detta /distribuzione discreta di probabilità/ la funzione
  $p_x : \mathbb{R} \to [0, 1]$ definita come segue:
  \begin{equation*}
  p_x(t) =
  \begin{cases}
  P(X=t) \quad &\text{per ogni $t \in S$}\\
  0 \quad &\text{altrimenti}
  \end{cases}
  \end{equation*}
  Come per le funzioni di ripartizione, esistono alcune proprietà che identificano le distribuzioni discrete di probabilità.
  Infatti, una funzione $p_x$ definita su un insieme finito $S$ è una distribuzione di probabilità se e solo se sono
  soddisfatte simultaneamente le seguenti proprietà:
  - $p_x (t) \geq 0, t \in \mathbb{R}$;
  - $\sum_{s \in S} p_x(s) = 1$.
  Si osservi che la seconda delle due proprietà è conseguenza diretta delle due seguenti proprietà precedentemente introdotte:
  - $P(\Omega) = 1$;
  - Data la famiglia $\{A_i, i \in I \subseteq N\}$ di eventi incompatibili vale $P\left(\bigcup_{i \in I} A_i \right) = \sum_{p \in I} P(A_i)$.
  Tra le funzioni di ripartizione delle variabili discrete e le distribuzioni discrete di probabilità esiste una corrispondeza
  biunivoca. Infatti, valgono le seguenti relazioni:
  - $F_X(t) = \sum_{s \in S : s \leq t} p_x(s)$ per ogni $t \in \mathbb{R}$;
  - $p_X(s) = F_X(s) - \lim_{t \to s^-} F_X(t)$ per ogni $s \in S$.
  Dalla prima di tali relazioni se ne deduce che le funzioni di ripartizione delle variabili aleatorie discrete presentano dei
  "salti" in corrispondenza dei valori $s$, mentre sono costanti per gli altri valori: per tale ragione vengono dette
  /funzioni a gradino/.

  Per esempio si consideri la variabile aleatoria definita negli esempi relativi agli appartamenti. Essa è discreta in
  quanto può assumere i seguenti valori $S = \{0, 1, 2, 3\}$. La sua distribuzione discreta di probabilità può essere
  definita facendo uso o della seguente relazione:
  $$ p_X(s) = F_X(s) - \lim_{t \to s^-} F_X(t) \quad \text{per ogni } s \in S$$
  considerando la funzione di ripartizione calcolata precedentemente, oppure direttamente andando a calcolare la
  probabilità che $X$ assuma i singoli valori in $S$ facendo uso della seguente relazione:
  \begin{equation*}
  p_x(t) =
  \begin{cases}
  P(X=t) \quad &\text{per ogni $t \in S$}\\
  0 \quad &\text{altrimenti}
  \end{cases}
  \end{equation*}
#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 12:41:38
[[file:Lezioni/screenshot_2018-03-14_12-41-38.png]]

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 12:43:10
[[file:Lezioni/screenshot_2018-03-14_12-43-10.png]]
- Variabile aleatoria /continua/: la corrispondente funzione $F_X$ è continua. In particolare, è detta
  /assolutamente continua/ se esiste una funzione $f_X : \mathbb{R} \to \mathbb{R}_+$ tale che
  $F_X(t) = \int_{-\infty}^t f_X(u)du$ per ogni $t \in \mathbb{R}$. Una tale funzione, quando esiste, viene detta /densità di
  probabilità/ di $X$. È detto poi /supporto/ della variabile $X$ l'insieme $S = \{t \in \mathbb{R} : f_X(t) \neq 0\}$.
  Si osservi che se la densità di probabilità di una variabile casuale esiste allora la funzione di ripartizione
  è una sua primitiva.

  Per semplicità supporremo nel seguito che le variabili aleatorie assolutamente continue abbiano funzione di
  ripartizione derivabile e che la funzione di densità di probabilità sia la derivata della funzione di ripartizione.

  Come per le distribuzioni discrete di probabilità anche le funzioni di densità di probabilità per essere tali
  devono soddisfare le seguenti due proprietà:
  - $f_X(t) \geq 0$ per ogni $t \in \mathbb{R}$;
  - $\int_{-\infty}^{+\infty} f_X(t)dt = 1$
  
  La probabilità che una variabile aleatoria continua (o assolutamente continua) assuma un ben determinato valore è
  sempre nulla. Infatti, se $X$ è una variabile aleatoria continua allora per ogni $t_0 \in \mathbb{R}$ vale:
  $$P(X = t_0) = P(X \leq t_0) - \lim_{t \to t_0^-} P(X \leq t) = F_X(t_0) - \lim_{t \to t_0^-} F_X(t) = F_X(t_0) - F_X(t_0) = 0$$
  Pertanto quando si pensa a variabili aleatorie continue, non ha mai senso domandarsi, quale sia la probabilità
  che assumano valori esatti. Al contrario ha senso domandarsi quale sia la probabilità che tali variabili
  assumano valori in specifici intervalli dell'asse reale.
  
  Per calcolare la probabilità che una variabile casuale continua $X$ assuma un valore in un intervallo $(a, b] \subseteq \mathbb{R}$
  è possibile far ricorso alla seguente formula: $P(X \in (a, b]) = F_X(b) - F_X(a)$ per ogni $a, b \in \mathbb{R}$ con $a < b$, oppure
  alla seguente formula: $P(X \in (a, b]) = \int_a^b f_X(u)du$ per ogni $a, b \in \mathbb{R}$ con $a < b$. La quale è ricavabile dalla
  precedente combinata con la seguente $F_X(t) = \int_{-\infty}^t f_X(u)du$ per ogni $t \in \mathbb{R}$.

  È utile segnalare che $P(X \in (a, b]) = F_X(b) - F_X(a) = \int_{-\infty}^b f_x(u)du - \int_{-\infty}^a f_x(u)du = \int_a^b f_x(u)du$.
  #+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 14:40:36
  [[file:Lezioni/screenshot_2018-03-14_14-40-36.png]]

  Essendo $P(X=a) = 0$ risulta sempre $P(X \in [a, b]) = P(X \in (a, b])$ per ogni $a, b \in \mathbb{R}$ con $a < b$.

Si consideri la variabile aleatoria $X$ introdotta nell'esempio precedente. Essa è continua in quanto la sua funzione
di densità di probabilità, ottenuta derivando la ripartizione, è data da:
\begin{equation*}
f_X(t) =
\begin{cases}
0 \quad &\text{per $t < 0$}\\
e^{-t} \quad &\text{per $t \geq 0$}
\end{cases}
\end{equation*}
In realtà è possibile osservare che la funzione di ripartizione non è derivabile in 0 e pertanto la scelta di porre la sua
derivata pari a $f_X(0) = e^{-0} = 1$ in quel punto è arbitraria, anche se non influenza in alcun modo le valutazioni sulla
variabile considerata $X$.

Calcoliamo la probabilità che $X$ assuma valori in $[1, 2]$ facendo ricorso ala formula
$P(X \in (a, b]) = \int_a^b f_X(u)du$ per ogni $a, b$, con $a < b$, ottenendo
$$P(X \in [1, 2]) = P(X = 1) + \int_1^2 f_X(u)du = 0 + \int_1^2 e^{-u} du = 0 - e^{-2} + e^{-1} = 0.233$$

La funzione di ripartizione è la funzione integrale della funzione di densità di probabilità. Quindi, data la funzione
di partizione, si ottiene la funzione di densità di probabilità tramite derivazione $\frac{d}{dt} F_X(t) = f_X(t)$.

In molti casi è lecito considerare situazioni (esperimenti) il cui esito è rappresentato, anziché da un valore numerico,
da una coppia o da una \(n\)-pla di valori; si pensi ad esempio alla coppia costi-ricavi in un investimento immobiliare.
Si parla allora di /variabili aleatorie multidimensionali/.

In modo analogo a quanto visto per le variabili unidimensionali, le variabili di tal tipo sono definite come applicazioni
da uno spazio campione $\Omega$ allo spazio $\mathbb{R}^n$ dove $n$ è la dimensione della variabile. Come per le variabili aleatorie
unidimensionali, conviene pensare a queste variabili aleatorie come a risultati di esperimenti esprimibili tramite
\(n\)-ple di valori numerici. Anche in questo caso è consuetudine considerare funzioni che esprimano le valutazioni
probabilistiche sui valori assumibili delle variabili.

Per il momento definiremo tali funzioni limitandoci a considerare /variabili aleatorie bidimensionali assolutamente
continue/ anche se quanto descritto in seguito può essere facilmente esteso al caso di più dimensioni e non
necessariamente continuo.

Sia quindi $(X;Y): \Omega \to \mathbb{R}^2$ una variabile aleatoria bidimensionale dove $\Omega$ è uno spazio campione al quale è associata
una probabilità $P$ definita sui sottoinsiemi di $\Omega$.

È detta /funzione di ripartizione congiunta/ la funzione bidimensionale $F_{X, Y} (t, s) : \mathbb{R}^2 \to [0, 1] \subseteq \mathbb{R}$
definita come $F_{X, Y}(t, s) = P(\{X \leq t\} \cap \{Y \leq s\})$ per ogni $(t, s) \in \mathbb{R}^2$.

Se, come qui assunto, la variabile $(X;Y)$ è assolutamente continua, allora esiste la /funzione di densità congiunta/
$F_{X, Y} : \mathbb{R}^2 \to \mathbb{R}_+$ tale che
$F_{X, Y}(t, s) = \int_{-\infty}^t \int_{-\infty}^s f_{X, Y} (u, v)du dv$ per ogni $(t, s) \in \mathbb{R}^2$.

Conoscendo la funzione di ripartizione congiunta o quella di densità congiunta è possibile determinare la probabilità
che la coppia $(X;Y)$ assuma valori in un qualsiasi sottoinsieme rettangolare $(a_1, b_1] \times (a_2, b_2] \in \mathbb{R}^2$.

Infatti, valgono le seguenti formule:
- $P((X, Y) \in (a_1, b_1] \times (a_2, b_2]) = F_{X, Y}(b_1, b_2) - F_{X, Y}(a_1, b_2) - F_{X, Y}(b_1, a_2) + F_{X, Y}(a_1, a_2)$
- $P((X, Y) \in (a_1, b_1] \times (a_2, b_2]) = \int_{a_1}^{b_1} \int_{a_2}^{b_2} f_{X, Y} (u, v) du dv$
per ogni $(a_1, b_1] \times (a_2, b_2] \in \mathbb{R}^2$.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 15:11:47
[[file:Lezioni/screenshot_2018-03-14_15-11-47.png]]

In molti casi, benché ci si trovi di fronte a situazioni i cui esiti sono di tipo multidimensionali, capita di essere
interessati ai valori che possono essere assunti solamente da una delle variabili (si è interessati a valutare probabilità
associate a solo uno dei valori numerici che descrivono l'esito dell'esperimento). Per questo motivo sono state introdotto
le /funzioni marginali/. Anche per la loro descrizione ci limitiamo al caso bidimensionale. Data una variabile aleatoria
bidimensionale $(X;Y)$ assolutamente continua, avente funzione di ripartizione congiunta $F_{X, Y}$ e funzione di densità
congiunta $f_{X, Y}$ sono dette /funzione di ripartizione marginale di $X$/ e /funzione di densità marginale di $X$/:
$$F_X(t) = P(X \leq t) = P(\{X \leq t\} \cap \{Y \leq +\infty\}) = F_{X, Y}(t, +\infty)$$
$$f_X(t) = \int_{-\infty}^{+\infty} f_{X, Y}(t, s)ds$$

Concludiamo questa parte introducendo la nozione di indipendenza tra variabili aleatorie. Data una variabile aleatoria
bidimensionale ($X;Y$) diciamo che due variabili $X$ ed $Y$ considerate singolarmente sono /stocasticamente
indipendenti/ se e solo se per ogni $(t, s) \in \mathbb{R}^2$ vale $F_{X, Y}(t, s) = F_X(t) \cdot F_Y(s)$. È possibile verificare
che tale condizione è equivalente alla seguente: $P(\{X \in A\} \cap \{Y \in B\}) = P(X \in A) \cdot P(Y \in B)$ per ogni $A, B \subseteq \mathbb{R}$
e che questa è a sua volta equivalente alla seguente $f_{X, Y}(t, s) = f_X(t) \cdot f_Y(s)$ per ogni coppia $(t, s) \in \mathbb{R}^2$
quando la coppia di variabili $(X, Y)$ sia assolutamente continua.

Nell'esempio che segue esaminiamo una variabile aleatoria bidimensionale discreta.

Supponiamo di dover lanciare un dato equilibrato e di essere interessati agli eventi $A$ "esce un numero pari" e $B$
"esce un numero minore o uguale a 4". Al fine di esplicitare il fatto che questi eventi si verifichino lanciando il dado,
consideriamo la coppia di variabili $(X, Y)$ dove:
\begin{equation*}
X :=
\begin{cases}
1 \quad &\text{se l'evento $A$ si verifica}\\
0 \quad &\text{se l'evento $A$ non si verifica}
\end{cases}
\end{equation*}
\begin{equation*}
Y :=
\begin{cases}
1 \quad &\text{se l'evento $B$ si verifica}\\
0 \quad &\text{se l'evento $B$ non si verifica}
\end{cases}
\end{equation*}
Poiché le due variabili dipendono dallo stesso esperimento è logico studiarle congiuntamente. Come prima cosa potremmo
determinarne la funzione di ripartizione congiunta.

In realtà però quando si abbia a che fare con variabili multidimensionali discrete si preferesce considerare la
distribuzione di probabilità congiunta definita come $P_{X, Y}(t, s) = P(\{X = t\} \cap \{Y = s\})$ dove $(t, s)$ è una
coppia di valori assumibili da $(X, Y)$.
Osserviamo ora che al variare del risultato del lancio del dado la coppia $(X, Y)$ assume i seguenti valori
\begin{equation*}
(X, Y) =
\begin{cases}
(0, 0) \quad &\text{se esito 5}\\
(0, 1) \quad &\text{se esito 1 oppure 3}\\
(1, 0) \quad &\text{se esito 6}\\
(1, 1) \quad &\text{se esito 2 oppure 4}
\end{cases}
\end{equation*}
Supponendo che il dado sia equilibrato sarà lecito attribuire le seguenti probabilità:
- $p_{X, Y}(0, 0) = P(\text{"esito 5"}) = \frac{1}{6}$
- $p_{X, Y}(0, 1) = P(\text{"esito 1 o 3"}) = \frac{2}{6}$
- $p_{X, Y}(1, 0) = P(\text{"esito 6"}) = \frac{1}{6}$
- $p_{X, Y}(1, 1) = P(\text{"esito 2 o 4"}) = \frac{2}{6}$
Calcoliamo ora le probabilità marginali della coppia $(X, Y)$ ottenendo
- $p_X(0) = P(\{X = 0\} \cap \{Y \text{ qualsiasi}\}) = p_{X, Y}(0, 0) + p_{X, Y}(0, 1) = \frac{1}{2}$
- $p_X(1) = P(\{X = 1\} \cap \{Y \text{ qualsiasi}\}) = p_{X, Y}(1, 0) + P_{X, Y}(1, 1) = \frac{1}{2}$
- $p_Y(0) = P(\{X \text{ qualsiasi}\} \cap \{Y = 0\}) = p_{X, Y}(0, 0) + p_{X, Y}(1, 0) = \frac{1}{3}$
- $p_Y(0) = P(\{X \text{ qualsiasi}\} \cap \{Y = 1\}) = p_{X, Y}(0, 1) + p_{X, Y}(1, 1) = \frac{2}{3}$
Le due variabili aleatorie $X$ ed $Y$ risultano essere stocasticamente indipendenti, infatti risulta:
- $p_{X, Y} (0, 0) = \frac{1}{6} = \frac{1}{2} \cdot \frac{1}{3} = p_X(0) \cdot p_Y(0)$
- $p_{X, Y}(0, 1) = \frac{2}{6} = \frac{1}{2} \cdot \frac{2}{3} = p_X(0) \cdot p_Y(1)$
- $p_{X, Y} (1, 0) = \frac{1}{6} = \frac{1}{2} \cdot \frac{1}{3} = p_X(1) \cdot p_Y(0)$
- $p_{X, Y}(1, 1) = \frac{2}{6} = \frac{1}{2} \cdot \frac{2}{3} = p_X(1) \cdot p_Y(1)$
  
Gli indici di tendenza centrare e la variabilità sono grandezze numeriche associate alle variabili aleatorie
in grado di sintetizzare, con un solo valore, le principali caratteristiche delle loro distribuzioni.
Risultano strettamente legati agli indici introdotti nella prima parte in relazione alla statistica descrittiva.

Il più importante degli /indici di tendenza centrale/ è il /valore atteso/, corrispondente alla media matematica
dei dati statistici. Data una variabile aleatoria unidimensionale $X$ con supporto $S \subseteq \mathbb{R}$ è detto
/valore atteso/ di $X$ la quantità
\begin{equation*}
E[X] =
\begin{cases}
\sum_{s \in S} s \cdot p_X(s) \quad &\text{se $X$ è discreta}\\
\int_{-\infty}^{+\infty} u \cdot f_X(u)du \quad &\text{se $X$ è assolutamente continua}
\end{cases}
\end{equation*}
Si osservi l'analogia di questa formula con quella della media pesata di una serie di dati statistici.
In effetti il valore atteso, così come la media di una serie di dati, va pensato come una "media pesata" dei
valori assumibili dalla variabile, e fornisce un'indicazione di massima del posizionamento delle variabili
lungo l'asse dei numeri reali.

Il valore atteso gode delle seguenti tre proprietà:
1. Per ogni $a \in \mathbb{R}$, se $X=a$ con probabilità uguale ad 1 allora $E[x] = a$;
2. $E[a \cdot X + b] = a \cdot E[x] + b$ per ogni variabile $X$ e per ogni $a, b, \in \mathbb{R}$;
3. Data una funzione $y = g(X)$ della variabile aleatoria $X$, il suo valore atteso è $E[g(X)] = \int_{-\infty}^{+\infty} g(u) f_X(u) du$
Occorre osservare che il valore atteso di una variabile potrebbe anche non esistere, poiché esso è definito come
integrale improprio.
Questa situazione può verificarsi nel caso in cui l'integrale o la sommatoria non convergano.

Il valore atteso è in realtà un caso particolare di momento centrale, per ogni $r = 1, 2, \dots$ è detto /momento centrale
di $X$ di ordine $r$/ la quantità
\begin{equation*}
E[X^r] =
\begin{cases}
\sum_{s \in S} s^r \cdot p_X(s) \quad &\text{se $X$ è discreta}\\
\int_{-\infty}^{+\infty} u^r \cdot f_X(u)du \quad &\text{se $X$ è assolutamente continua}
\end{cases}
\end{equation*}
Un secondo indice di tendenza centrale che occorre descrivere è la moda.

Data una variabile aleatoria $X$ è detta /moda/ una quantitò $\tilde{X} \in \mathbb{R}$ corrispondente al valore per
cui è massima la distribuzione discreta di probabilità (se $X$ è discreta) oppure la funzione di densità (se $X$
è assolutamente continua).

Non è detto che un tale valore sia unico, se lo è diremo che la distribuzione di $X$ è /unimodale/, in caso contrario
si parlerà di distribuzione /multimodale/. Può capitare in alcuni casi che la distribuzione discreta di probabilità o la
funzione di densità presentino diversi punti di massimo locale. Sebbene ciò non sarebbe formalmente corretto, tutti
questi punti di massimo locale vengono solitamente considerati come punti modali e pertanto anche in questo caso si
parla di distribuzione multimodale.

Un terzo indice di tendenza centrale è la /mediana/. Data una variabile aleatoria $X$ diciamo mediana una quantità
$\hat{X} \in \mathbb{R}$ che soddisfa la diseguaglianza $\lim_{t \to \hat{X}^-} F_X(t) \leq \frac{1}{2} \leq F_X (\hat{X})$.

Nel caso in cui la funzione di ripartizione della variabile sia continua ed invertibile allora $\hat{X} = F_X^{-1}(0.5)$.

Nel caso di variabili discrete invece la mediana è il valore dell'ascissa in cui la funzione di ripartizione passa
da un valore minore di 0.5 ad uno superiore.

Più semplicemente è possibile pensare alla mediana come a quel valore
per cui sia la probabilità che $X$ assuma valori più piccoli che la probabilità che $X$ assuma valori più grandi
sono pari a 0.5.

La mediana può non essere unica e ciò si verifica quando esistano più valori $t$ per i quali risulti $F_X(t) = \frac{1}{2}$.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 16:28:48
[[file:Lezioni/screenshot_2018-03-14_16-28-48.png]]

Unitamente alla mediana è possibile considerare altri indici definiti in maniera simile e che dividono la retta
dei reali in due intervalli di probabilità assegnata e che sono detti "quantili". Dato un valore
$p \in [0, 1] \subseteq \mathbb{R}$ è detto /quantile \(p\)-esimo della variabile aleatoria $X$/ il valore
$x_p \in \mathbb{R} : \lim_{t \to x_p^-}F_X(t) \leq p \leq F_X(x_p)$. Nel caso in cui la funzione di ripartizione sia continua ed
invertibile allora $x_p = F_X^{-1}(p)$.

Pertanto è possibile pensare ad $x_p$ come a quel valore per cui risulta $P(X \leq x_p) = p$ e $P(X > x_p) = 1 - p$.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 17:22:18
[[file:Lezioni/screenshot_2018-03-14_17-22-18.png]]

Supponiamo di avere a disposizione 2 appartamenti da vendere entro la fine dell'anno e supponiamo che la
vendita di ciascuno di essi sia indipendente dalla vendità dell'altro. Supponiamo inoltre di sapere, in base alla nostra
esperienza, che le probabilità di vendere i due appartementi nel tempo prefissato siano 3/4 e 2/3.

Consideriamo la variabile aleatoria $X$ "numero di appartamenti venduti a fine anno". Essa è una variabile discreta per
il cui computo della distribuzione di probabilità conviene considerare i seguenti eventi: $A$ "il primo alloggio viene
venduto" e $B$ "il secondo alloggio viene venduto".

Osservando che in base a quanto affermato in precedenza avremo $P(A) = \frac{3}{4}$, $P(\bar{A}) = \frac{1}{4}$,
$P(B) = \frac{2}{3}$ e $P(\bar{B}) = \frac{1}{3}$ si ricava immediatamente:
- $p_X(0) = P(\bar{A} \cap \bar{B}) = P(\bar{a}) \cdot P(\bar{B}) = \frac{1}{12}$;
- $p_X(1) = P(\{\bar{A} \cap B\} \cup \{A \cap \bar{B}\}) = P(\{\bar{A} \cap B\}) + P(\{A \cap \bar{B}\}) = P(\bar{A}) \cdot P(B) + P(A) \cdot P(\bar{B})
  = \frac{5}{12}$;
- $p_X(2) = P(A \cap B) = P(A) \cdot P(B) = \frac{6}{12}$.
Possiamo determinare facilmente sia il valore atteso $E[X] = \sum_{s \in S} s \cdot p_X(s) = 0 \cdot p_X(0) + 1 \cdot p_X(1) + 2 \cdot p_X(2) =
0 \cdot \frac{1}{12} + 1 \cdot \frac{5}{12} + 2 \cdot \frac{6}{12} = 1.416$
che la moda per la variabile $X$: $\tilde{X} = 2$. Il calcolo della mediana richiede la determinazione preventiva della
funzione di ripartizione.

Tale funzione può essere calcolata per mezzo della distribuzione di probabilità e sarà:
\begin{equation*}
F_X(t) =
\begin{cases}
0 \quad &\text{per $t < 0$}\\
\frac{1}{12} \quad &\text{per $0 \leq t < 1$}\\
\frac{1}{2} \quad &\text{per $1 \leq t < 2$}\\
1 \quad &\text{per $t \geq 2$}
\end{cases}
\end{equation*}
Il grafico della funzione di ripartizione è riportato in seguito:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 17:38:00
[[file:Lezioni/screenshot_2018-03-14_17-38-00.png]]

Dal grafico è possibile osservare che tutti i valori $t \in [1, 2)$ sono mediane, infatti per ognuno di tali valori si ha che
$P(X \leq t) = \frac{1}{2}$ e $P(X \geq t) = \frac{1}{2}$.

Consideriamo la variabile assolutamente continua $X$ già vista avente funzione di densità di probabilità data da
\begin{equation*}
f_X (t) =
\begin{cases}
0 \quad &\text{per $t < 0$}\\
e^{-t} \quad &\text{per $t \geq 0$}
\end{cases}
\end{equation*}
e funzione di ripartizione data da
\begin{equation*}
F_X (t) =
\begin{cases}
0 \quad &\text{per $t < 0$}\\
1-e^{-t} \quad &\text{per $t \geq 0$}
\end{cases}
\end{equation*}
Per questa variabile si ricavano immediatamente il valore atteso $E[X] = \int_{-\infty}^{+\infty} t \cdot f_X(t)dt = \int_0^{+\infty} t \cdot e^{-t}dt = \lim_{b \to +\infty}|-e^{-t}(t+1)|_0^b = 1$,
la mediana $\bar{X} = F_X^{-1}(0.5) = - \log 0.5 = 0.693$ ed infine la moda $\tilde{X} = 0$ essendo questo
il punto di massimo della funzione di densità di probabilità.

Per misurare il grado di dispersione dei valori assumibili da una variabile aleatoria vengono usati gli /indici di
variabilità/. Tra questi quello che risulta in assoluto il più importante è senza dubbio la /varianza/. Formalmente,
data una variabile aleatoria $X$ è detta /varianza/ di $X$ la quantità
\begin{equation*}
V[X] =
\begin{cases}
\sum_{s \in S}(s - E[X])^2 p_X(s) \quad &\text{se $X$ è discreta}\\
\int_{-\infty}^{+\infty}(u - E[X])^2 f_X(u)du \quad &\text{se $X$ è assolutamente continua}
\end{cases}
\end{equation*}
Così come il valore atteso anche la varianza talvolta può non esistere, quando la sommatoria o l'integrale divergono.

Così come il valore atteso di una variabile aleatoria $X$ viene spesso indicato con $\mu_x$ la /varianza/ viene sovente indicata
$\sigma_X^2$. Essa viene indicata con il quadrato in quanto la sua radice è un altro indice molto importante chiamato
/deviazione standard/ $\sigma_X = \sqrt{\sigma_X^2}$. La deviazione standard ha il vantaggio rispetto alla varianza di avere la stessa
unità di misura del valore atteso.

Anche la varianza gode di alcune proprietà che è meglio ricordare:
- Per ogni $a \in \mathbb{R}$, se $X = a$ con probabilità uguale ad 1 allora $V[X] = 0$;
- $V[a \cdot X + b] = a^2 \cdot V[X]$ per ogni variabile $X$ e per ogni $a, b \in \mathbb{R}$;
- $V[X] = E[X^2] - (E[X])^2$ per ogni variabile aleatoria $X$.

Consideriamo la variabile aleatoria descritta nell'esempio dei 2 appartamenti. Possiamo determinare la varianza o tramite la
definizione stessa o tramite la seguente proprietà: $V[X] = E[X^2] - (E[X])^2$ per ogni variabile aleatoria $X$.

In generale risulta più comodo utilizzare la proprietà sopra riportate così come faremo di seguito pertanto determiniamo
il momento centrale di ordine 2 della variabile $X$.
$$E[X^2] = 0^2 \cdot p_X (0) + 1^2 \cdot p_X (1) + 2^2 \cdot p_X (2) = 0^2 \cdot \frac{1}{12} + 1^2 \cdot \frac{5}{12} + 2^2 \cdot \frac{6}{12} = \frac{29}{12}$$
La varianza risulta quindi essere $V[X] = E[X^2] - (E[X])^2 = \frac{29}{12} - (1.416)^2 = 0.42$ mentre la deviazione standard
è $\sigma_X = \sqrt{0.42} = 0.65$.

Consideriamo la variabile aleatoria $X$ descritta precedentemente. Calcoliamone la varianza:
$$V[X] = E[X^2] - (E[X])^2 = \int_0^{+\infty} t^2 \cdot e^{-t} dt - 1^2 = \lim_{b \to +\infty}|-e^{-t}(t^2 + 2t + 2)|_0^b - 1= 2 - 1 = 1$$
inoltre la deviazione standard è $\sigma_X = \sqrt{1} = 1$ ed è pertanto maggiore di quella dell'esempio precedente.

Gli indici visti fino ad ora sono relativi a variabili unidimensionali. Anche per le variabili multidimensionali ed in
particolare per quelle bidimensionali esistono indici di tendenza centrale e variabilità.

Sia $(X, Y)$ una variabile aleatoria bidimensionale discreta o continua. Sono detti /valori attesi marginali/ e
/varianze marginali/ le quantità $E[X], E[Y], V[X], V[Y]$ ottenute considerando le distribuzioni marginali di $X$ ed $Y$
ed integrando (o sommando) in accordo alle seguenti
\begin{equation*}
E[X] =
\begin{cases}
\sum_{s \in S} s \cdot p_X(s) \quad &\text{se $X$ è discreta}\\
\int_{-\infty}^{+\infty} u \cdot f_X(u)du \quad &\text{se $X$ è assolutamente continua}
\end{cases}
\end{equation*}
\begin{equation*}
V[X] =
\begin{cases}
\sum_{s \in S}(s - E[X])^2 p_X(s) \quad &\text{se $X$ è discreta}\\
\int_{-\infty}^{+\infty}(u - E[X])^2 f_X(u)du \quad &\text{se $X$ è assolutamente continua}
\end{cases}
\end{equation*}
Si noti che la varianza è il valore atteso di $g(X) = (X - E[X])^2$.

È utile ricordare che valgono le seguenti relazioni:
- $E[a \cdot X + b \cdot Y] = a \cdot E[X] + b \cdot E[Y]$ per ogni coppia $X, Y$ e per ogni $a, b \in \mathbb{R}$;
- $E[X \cdot Y] = E[X] \cdot E[Y]$ per ogni coppia $X, Y$ stocasticamente indipendenti;
- $V[X + Y] = V[X] + V[Y]$ per ogni coppia $X, Y$ stocasticamente indipendenti.
Oltre ai valori attesi ed alle varianze marginali, un altro indice è estremamente importante, si tratta della
/covarianza/ definita come
$$\text{Cov}[X, Y] = E[(X - E[X]) \cdot (Y - E[Y])] = \iint_{\mathbb{R}^2} (t - E[X]) \cdot (s - E[Y]) \cdot f_{X, Y} (t, s) dt ds$$
o equivalentemente come
$$\text{Cov}[X, Y] = E[X \cdot Y] - E[X] \cdot E[Y] = \iint_{\mathbb{R}^2} t \cdot s \cdot f_{X, Y}(t, s)dt ds - \int_{\mathbb{R}} t \cdot f_X (t) dt \cdot \int_{\mathbb{R}} s \cdot f_Y(s)ds$$
La covarianza è un indice della correlazione che sussiste tra due variabili ovvero del loro grado di dipendenza reciproca.
Tanto più essa è grande tanto più forte è il legame di dipendenza tra le variabili. Si noti ad esempio che se le variabili
$X$ ed $Y$ sono stocasticamente indipendenti, allora in base alla seguente proprietà:

$E[X \cdot Y] = E[X] \cdot E[Y]$. Per ogni coppia $X, Y$ stocasticamente indipendente
si ottiene che $\text{Cov}[X, Y] = E[X \cdot Y] - E[X] \cdot E[Y] = E[X] \cdot E[Y] - E[X] - E[Y] = 0$.
Due variabili aleatorie aventi covarianza nulla vengono dette /incorrelate/.

Occorre però sottolineare che la nozione di incorrelazione è più debole di quella di indipendenza. Infatti, è possibile
mostrare che seppur esistono coppie di variabili incorrelate esse non sono indipendenti.

Un ultimo indice da ricordare, strettamente legato alla covarianza ed utilizzato per esprimere più chiaramente il grado
di dipendenza tra due variabili, è il /coefficiente di correlazione lineare di Pearson/:
$$ \rho_{XY} = \frac{\text{Cov}[X, Y]}{\sqrt{V[X]\cdot V[Y]}} = \frac{\text{Cov}[X, Y]}{\sigma_X \cdot \sigma_Y}$$
Tale indice rispetto alla covarianza ha il vantaggio di godere delle seguenti proprietà:
- $\rho_{XY} = 0$ se $X$ ed $Y$ sono incorrelate;
- $|\rho_{XY}| = 1$ se vale la relazione $Y = a \cdot X + b$ per ogni $a, b \in \mathbb{R}$. Più precisamente, se $\rho_{XY}$ vale +1 allora $a > 0$,
  e se vale -1 allora $a < 0$.
  
** Distribuzioni Notevoli 

Abbiamo visto che esiste una corrispondenza biunivoca tra la *funzione di ripartizione* e la
- Densità di probabilità (variabili assolutamente continue);
- Distribuzione di probabilità (variabili discrete).
  
Per tale ragione si usa parlare di "Distribuzione" di una variabile intendendo indifferentemente la sua ripartizione o
la sua densità (o distribuzione di probabilità).

La notazione $X \sim F$ va letta come "la variabile $X$ è distribuita secondo $F$".

*** Distribuzione Bernoulliana
Una variabile aleatoria $X$ è detta /distribuita secondo una Bernoulliana di parametro $p$/, $p \in [0, 1]$, $X \sim B(p)$,
se essa può assumere solo i valori 1 e 0 rispettivamente con probabilità $p$:
\begin{equation*}
p_X(t) =
\begin{cases}
1 - p &\text{se $t = 0$}\\
p     &\text{se $t = 1$}\\
0     &\text{altrimenti}
\end{cases}
\end{equation*}
\begin{equation*}
F_X(t)=
\begin{cases}
0 &\text{se $t < 0$}\\
1 - p &\text{se $0 \leq t < 1$}\\
1 &\text{se $t \geq 1$}
\end{cases}
\end{equation*}
L'importanza di questa semplice distribuzione è ovvia, sono variabili di Bernoulli tutte quelle che individuano il verificarsi
di uno specifico evento e che valgono 1 se questo si verifica e 0 altrimenti.

Immediate sono le determinazioni della /media/ e della /varianza/ di una /Bernoulliana/ che risultano essere:
$$ E[X] = 0 \cdot (1 - p) + 1 \cdot p = p$$
$$ V[X] = [0^2 \cdot (1 - p) + 1^2 \cdot p] - p^2 = (1 - p)p$$

*** Distribuzione Binomiale
Siano $X_1, \dots, X_n, n$ variabili Bernoulliane di identico parametro $p$ e stocasticamente indipendenti tra loro. Sia poi $X$
una variabile aleatoria definita come somma delle $X_i$ ovvero sia $X = X_1 + \dots + X_n$. Una tale variabile è detta
/distribuita secondo una Binomiale con parametri $n$ e $p$/: $X \sim \text{Bin}(n, p)$. Una tale variabile può assumere qualsiasi
valore intero $k$ compreso tra $0$ ed $n$ in accordo alla seguente probabilità:
$$P(X=k) = \binom{n}{k} \cdot p^k \cdot (1 - p)^{n-k}$$

La motivazione della precedente formula è la seguente: $p^k \cdot (1 - p)^{n-k}$ fornisce la probabilità che $k$ delle $n$ variabili $X_i$
assuma il valore 1 e che le restanti $(n-k)$ assumano valore 0.

$\binom{n}{k} = \frac{n!}{(n-k)! k!}$ esprime il numero di combinazioni possibili per cui $k$ variabili valgono 1 e $(n-k)$
valgono 0.

In definitiva la /distribuzione di probabilità/ e la /funzione di ripartizione/ risultano essere

\begin{equation*}
p_X(t)=
\begin{cases}
\binom{n}{t}p^t(1-p)^{n-t} &\text{se $t \in \{0, 1, \dots, n\}$}\\
0 &\text{altrimenti}
\end{cases}
\end{equation*}
$$F_X(t) = \sum_{0 \leq k \leq n:k \leq t} \binom{n}{k} p^k(1-p)^{n-k} \quad \text{per ogni $t \in \mathbb{R}$}$$
L'indipendenza tra le variabili $X_i$ consente di determinare facilmente il /valore atteso/ e la /varianza/ della
variabile $X$:
$$E[X] = E[X_1 + \dots + X_n] = E[X_1] + \dots + E[X_n] = n \cdot p$$
$$V[X] = V[X_1] + \dots + V[X_n] = n \cdot (1-p) \cdot p$$
La principale applicazione della distribuzione binomiale consiste nella definizione di variabile che "contano" le realizzazioni
di eventi quando questi siano da considerarsi indipendenti e con identica probabilità di verificarsi.

Questo è per esempio il caso della variabile $X$ definita nei 2 seguenti esempi visti in precedenza:

Facciamo riferimento all'esempio degli appartamenti e consideriamo lo spazio $\Omega$ e la probabilità $P$ in esso definiti.
Una variabile aleatoria che ha senso considerare in questo caso potrebbe essere la $X :=$ "numero di appartamenti venduti
a fine anno". Formalmente essa andrebbe definita come funziona da $\Omega$ in $\mathbb{R}$ che assegna:
- $X((0, 0, 0)) = 0$
- $X((0, 0, 1)) = X((0, 1, 0)) = X((1, 0, 0)) = 1$
- $X((0, 1, 1)) = X((1, 1, 0)) = X((1, 0, 1)) = 2$
- $X((1, 1, 1)) = 3$

In base alla definizione della variabile casuale $X$ che abbiamo fornito, ha senso definire la probabilità che
esattamente un appartamento venga venduto $X$ = 1. Infatti, vale quanto segue:
$$ P(X=1) = P(\omega \in \Omega : X(\omega) = 1) = P(\{(0, 0, 1), (0, 1, 0), (1, 0, 0)\}) = \frac{3}{8}$$

Riconsideriamo la variabile aleatoria definita nell'esempio precedente, e determiniamone la corrispondente funzione di
ripartizione. Per questo osserviamo prima di tutto che la $X$ può assumere solo i valori 0, 1, 2 e 3.
Quindi sicuramente sarà $F_X(t) = P(X \leq t) = 0$ per $t < 0$. Avremo poi
\begin{align*}
&F_X(t) = P(X = 0) = P(\{0, 0, 0\}) = \frac{1}{8} \text{ per } t <0\\
&F_X(t) = P(X = 0 \text{ oppure } 1) = P(\{(0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1)\}) = \frac{1}{2} \text{ per } 1 \leq t \leq 2\\
&F_X(t) = P(X = 0 \text{ oppure } 1 \text{ oppure } 2) = 1 - P(X = 3) = 1 - P(\{(1, 1, 1)\}) = 1 - \frac{1}{8} = \frac{7}{8} \text{ per } 2 \leq t < 3\\
&F_X(t) = P(X = 0 \text{  oppure  } 1 \text{ oppure } 2 \text{ oppure } 3) = P(\Omega) = 1 \text{ per } t \geq 3
\end{align*}

La funzione di ripartizione risulta essere descritta dal grafico nella figura sottostante:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-14 10:05:12
[[file:Lezioni/screenshot_2018-03-14_10-05-12.png]]

In questi 2 esempi $X$ risulta essere una variabile aleatoria distribuita secondo una Binomiale con parametri 3 e 1/2.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 11:26:21
[[file:Lezioni/screenshot_2018-03-24_11-26-21.png]]

*** Distribuzione di Poisson
La distribuzione di /Poisson/ può essere vista come un caso particolare della distribuzione Binomiale che si ottiene quando il
numero di variabili $X_i$ che compaiono in $X = X_1 + \dots X_n$ tende ad infinito mentre il valore del parametro $p$ tende a
zero in modo tale che il prodotto $n \cdot p$ resti costante.

In questo caso, assumendo $\lambda = n \cdot p$ diremo che la variabile $X = X_1 + \dots + X_n$ è distribuita secondo una Poisson con parametro
$\lambda, \lambda \in \mathbb{R}_+$: $X \sim \text{Poi}(\lambda)$.

Osserviamo che la variabile così definita può assumere un qualsiasi valore intero $k$. Le probabilità associate ai valori
assumibili da $X$ si ricavano dalla seguente relazione: $P(X=k) = \binom{n}{k} \cdot p^k \cdot (1-p)^{n-k}$ con un passiaggio al limite per
$n \to +\infty$ (sotto il vincolo $\lambda = n \cdot p$ costante) e risultano:
$$ P(X=k) = \frac{\lambda^k}{k!} e^{-\lambda} \quad \forall k \in \mathbb{N}$$
In definitiva la /distribuzione di probabilità/ e la /funzione di ripartizione/ risultano essere:
\begin{equation*}
p_X(t) =
\begin{cases}
\frac{\lambda^t}{t!}\cdot e^{-\lambda} &\text{se $t \in \{0, 1, \dots\}$}\\
0 &\text{altrimenti}
\end{cases}
\end{equation*}
$$F_X(t) = \sum_{k \in \mathbb{N} : k \leq t} \frac{\lambda^k}{k!} \cdot e^{-\lambda} \quad \text{per ogni $t \in \mathbb{R}$}$$
Il /valore atteso/ e la /varianza/ di una tale variabile Poissoniana si ricavano facilmente da quelli delle Bernoulliane,
ricordando che $\lambda = n \cdot p$.

Il /valore atteso/ risulta essere
$$E[X] = n \cdot p = \lambda$$
mentre per calcolare la /varianza/ è necessario osservare che $V[X] = n \cdot (1-p) \cdot p = n \cdot p - n \cdot p^2 = \lambda - \frac{\lambda^2}{n}$ e
che tale quantità diviene uguale a $\lambda$ quando $n$ tende ad infinito per cui
$$ V[X] = \lambda$$
La distribuzione di Poisson viene utilizzata quando si considerino grandi popolazioni di individui in cui ogni individuo ha
una probabilità $p$ molto piccola di essere soggetto ad uno specifico evento in esame. Per tale ragione la /distribuzione
di Poisson viene anche detta degli eventi rari/.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 11:47:08
[[file:Lezioni/screenshot_2018-03-24_11-47-08.png]]

*** Distribuzione Geometrica
Una variabile aleatoria $X$ è detta /distribuita secondo una Geometrica/ di parametro $p$, $p \in [0, 1]$: $X \sim \text{Geo}(p)$ se può
assumere qualsiasi valore intero non negativo $k$ con probabilità $P(X=k) = p \cdot (1-p)^k$ ovvero se ha la /distribuzione di probabilità/
e la /funzione di ripartizione/ date da
\begin{equation*}
p_X(t)=
\begin{cases}
p \cdot (1-p)^t &\text{se $t \in \mathbb{N}$}\\
0 &\text{altrimenti}
\end{cases}
\end{equation*}
$$F_X(t) = \sum_{k \in \mathbb{N} : k \leq t} p \cdot (1-p)^k \quad \text{per ogni $t \in \mathbb{R}$}$$
Si consideri un esperimento, ripetuto a istanti rappresentati da numeri interi, di verifica di funzionamento di una macchina:

Se interpretiamo $p$ come la probabilità che a un certo istante la macchina si guasti, la distribuzione geometrica dà la
/distribuzione di probabilità del PRIMO guasto/, cioè $p_X(t)$ è la probabilità che il guasto si verifichi al tempo \(t+1\)-mo
(nei primi $t$ istanti funziona).

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 11:58:21
[[file:Lezioni/screenshot_2018-03-24_11-58-21.png]]

/Valore atteso/ e /varianza/ di una variabile $X$ con distribuzione Geometrica sono:
$$E[X] = \frac{1 - p}{p} \quad V[X] = \frac{1-p}{p^2}$$
L'importanza di questa distribuzione sta nella /proprietà di assenza di memoria/: $P(X=k+m|X \geq m) = P(X=k)$.

Per comprenderne il significato, supponiamo che $X$ sia il tempo di vita di una macchina soggetta a guasti (possono
avvenire solo in corrispondenza di intervalli di tempo unitari), e supponiamo di aver rilevato che per $m$ unità di
tempo essa non si sia guastata.

La proprietà di assenza di memoria asserisce che la probabilità che la macchina si guasti all'istante \(k+m\)-esimo,
condizionata dall'evento $X \geq m$, è uguale alla probabilità iniziale che essa si guasti all'istante \(k\)-esimo.

In definitiva, la proprietà di assenza di memoria asserisce che il tempo trascorso da quando abbiamo iniziato ad esaminare
il funzionamento della macchina non influisce sulla distribuzione del tempo restante al verificarsi del guasto.

*** Distribuzione Uniforme
La /distribuzione uniforme/ rappresenta la più semplice distribuzione continua e viene adottata nel caso in cui
la variabile considerata possa assumere qualsiasi valore compreso in un dato intervallo con probabilità costante.

Formalmente diciamo che la variabile $X$ è /distribuita secondo una Uniforme di supporto/ $[a, b]$: $X \sim U[a, b]$ se
essa è assolutamente continua con /densità/ e /funzione di ripartizione/:
\begin{equation*}
f_X(t)=
\begin{cases}
\frac{1}{b-a} &\text{se $t \in [a, b]$}\\
0 &\text{altrimenti}
\end{cases}
\end{equation*}
\begin{equation*}
F_X(t) =
\begin{cases}
0 &\text{se $t < a$}\\
\frac{t-a}{b-a} &\text{se $t \in [a, b]$}\\
1 &\text{se $t > b$}
\end{cases}
\end{equation*}
Tramite semplici integrazioni è possibile ricavare il /valore atteso/ e la /varianza/:
$$ E[X] = \frac{a + b}{2} \quad V[X] = \frac{(b-a)^2}{12}$$
Come accennato in precedenza l'interesse in questa distribuzione è giustificato dal fatto che essa descrive bene
situazioni nelle quali le variabili possono assumere valori in intervalli finiti di $\mathbb{R}$ con probabilità uniforme
ovvero tale da essere identica per intervalli di medesima ampiezza (purché contenuti nel supporto della variabile stessa).

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 12:23:30
[[file:Lezioni/screenshot_2018-03-24_12-23-30.png]]

*** Distribuzione triangolare
Quando si considerano variabili aleatorie con supporto $[a, b] \subseteq \mathbb{R}$ può essere limitativo pensare che i
valori assumibili abbiano tutti la stessa probabilità di presentarsi. Per questa ragione sono state introdotte in letteratura
diverse generalizzazioni della distribuzione uniforme. Una di queste è la distribuzione triangolare, che assegna alla
densità di probabilità valori maggiori al centro del supporto e minori in prossimità degli estremi.

Formalmente diciamo che la variabile $X$ è /distribuita secondo una Triangolare di supporto/ $[a, b]$: $X \sim T[a, b]$
se essa è assolutamente continua con /densità/ e /funzione di ripartizione/
\begin{equation*}
f_X(t)=
\begin{cases}
\frac{4 \cdot (t-a)}{(b-a)^2} &\text{se $t \in \left[a, \frac{a + b}{2}\right)$}\\
\frac{4 \cdot (b-t)}{(b-a)^2} &\text{se $t \in \left[\frac{a + b}{2}, b\right]$}
\end{cases}
\end{equation*}
\begin{equation*}
F_X(t)=
\begin{cases}
0 &\text{se $t < a$}\\
2 \cdot \frac{(t-a)^2}{(b-a)^2} &\text{se $t \in \left[a, \frac{a+b}{2}\right)$}\\
1 - 2 \cdot \frac{(b-t)^2}{(b-a)^2} &\text{se $t \in \left[\frac{a+b}{2}, b\right]$}
\end{cases}
\end{equation*}
Tramite integrazione è possibile ricavare il /valore atteso/ e la /varianza/
$$E[X] = \frac{a+b}{2} \quad V[X] = \frac{(b-a)^2}{24}$$

*** Distribuzione esponenziale
La distribuzione esponenziale è particolarmente importante nello studio di quelle variabili che descrivono i tempi
occorrenti al verificarsi di un evento (tempi di attesa per la vendita o per la costruzione di un immobile).

Formalmente, una variabile aleatoria $X$ è /distribuita secondo una Esponenziale di parametro $\lambda$/, con $\lambda \in \mathbb{R}_+$:
$X \sim \text{Exp}(\lambda)$ se essa è assolutamente continua con /densità/ e /funzione di ripartizione/
\begin{equation*}
f_X(t) =
\begin{cases}
\lambda \cdot e^{-\lambda t} &\text{se $t \geq 0$}\\
0 &\text{se $t < 0$}
\end{cases}
\end{equation*}
\begin{equation*}
F_X(t)=
\begin{cases}
1 - e^{-\lambda t} &\text{se $t \geq 0$}\\
0 &\text{se $t < 0$}
\end{cases}
\end{equation*}
Tramite integrazione si ricavano il /valore atteso/ e la /varianza/ che risultano essere
$$E[X] = \frac{1}{\lambda} \quad V[X] = \frac{1}{\lambda^2}$$
Proprietà di assenza di memoria: $P(X > s + t | X > s) = P(X > t)$.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 14:44:08
[[file:Lezioni/screenshot_2018-03-24_14-44-08.png]]

*** Distribuzione normale
Una variabile aleatoria $X$ è detta distribuita secondo una /Normale/ con parametri $\mu$ e $\sigma$, con $\mu \in \mathbb{R}$
e $\sigma \in \mathbb{R}_+$: $X \sim N(\mu, \sigma)$ se essa è /assolutamente continua/ con densità
$$f_X(t) = \frac{1}{\sqrt{2 \cdot \pi \cdot \sigma^2}} \cdot e^{-\frac{(t - \mu)^2}{2 \cdot \sigma^2}}$$
per ogni $t \in \mathbb{R}$.

Nonostante la funzione di densità della distribuzione Normale sia molto complessa, questa distribuzione è fondamentale
nella statistica in virtù del /teorema centrale limite/ e delle numerose proprietà da essa possedute che vedremo nel
seguito. Per ora limitiamoci ad osservare che tramite integrazioni non particolarmente semplici si ricavano
/valore atteso/ e /varianza/ date da
$$E[X] = \mu \quad V[X] = \sigma^2$$
Pertanto i parametri della distribuzione sono rispettivamente il /valore atteso/ e la /deviazione standard/ della distribuzione.

Inoltre, la variabile aleatoria ha supporto su tutto l'asse reale.

Graficamente la densità di una Normale risulta come quella presentata sotto:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 14:51:43
[[file:Lezioni/screenshot_2018-03-24_14-51-43.png]]

Si osservi che la moda coincide con la media e che in corrispondenza dei valori $\mu - \sigma$ e $\mu + \sigma$ vi sono dei punti di flesso.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 14:52:56
[[file:Lezioni/screenshot_2018-03-24_14-52-56.png]]

Non è facile, dati $a$ e $b \in \mathbb{R}$ con $a < b$, determinare la probabilità che la variabile $X \sim N(\mu, \sigma)$
assuma valori in $[a, b]$ ovvero calcolare
$$P(X \in [a, b]) = \int_a^b \frac{1}{\sqrt{2 \cdot \pi \cdot \sigma^2}} \cdot e^{-\frac{(t - \mu)^2}{2 \cdot \sigma^2}}dt$$
in quanto la risoluzione dell'integrale non è immediata.

Per questa ragione si ricorre ad opportune tavole che si riferiscono alla /distribuzione normale standard/ ovvero
con parametri $\mu=0$ e $\sigma=1$ e che forniscono i valori di $\int_0^z f_X(t)dt$ per un elevato numero di valodi
di $z \in \mathbb{R}_+$.

Quando si sia interessati a determinare delle probabilità associate ad una generica $X ~ N(\mu, \sigma)$ è possibile
ricondursi al caso appena presentato osservando che la variabile $Z = \frac{X - \mu}{\sigma}$ è distribuita secondo
una /Normale Standard/ ovvero vale la seguente proprietà:
$$\text{se } X \sim N(\mu, \sigma) \text{ allora } Z = \frac{X - \mu}{\sigma} \sim N(0, 1)$$
In base a tale proprietà ogni variabile $X \sim N(\mu, \sigma)$ può essere ricondotta ad una /Normale Standardizzata/, ovvero
ancora per ogni $[a, b] \subseteq \mathbb{R}$ si avrà:
\begin{equation*}
P(X \in [a, b]) = P(a \leq X \leq b) = P\left(\frac{a - \mu}{\sigma} \leq \frac{X - \mu}{\sigma} \leq \frac{b - \mu}{\sigma}\right)
= P\left(Z \in \left[\frac{a - \mu}{\sigma}, \frac{b-\mu}{\sigma}\right]\right)
\end{equation*}

Per esempio, sia $X$ una variabile aleatoria con distribuzione normale di parametri $\mu = 10$ e $\sigma = 1$ e si voglia
determinare la probabilità dell'evento "$X \in [9.2, 11.35]$".
In base a quanto appena esposto avremo:
$$P(X \in [9.2, 11.35]) = P \left(Z \in \left[\frac{9.2 - 10}{1}, \frac{11.35 - 10}{1}\right]\right) = P(Z \in [-0.8, 1.35])$$
L'ultimo valore di probabilità è relativo ad una normale standard e può essere calcolato tramite le tavole,
inoltre data la simmetria della normale si può scrivere:
$$P(Z \in [-0.8, 0]) = P(Z \in [0, 0.8])$$

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 15:14:09
[[file:Lezioni/screenshot_2018-03-24_15-14-09.png]]

Inoltre, dalle tavole si ricava $P(Z \in [0, 1.35]) = 0.4115$ e $P(Z \in [-0.8, 0]) = P(Z \in [0, 0.8]) = 0.2881$.

In definitiva avremo $P(X \in [9.2, 11.35]) = P(Z \in [-0.8, 1.35]) = P(Z \in [0, 0.8]) + P(Z \in [0, 1.35]) = 0.44115 + 0.2881 = 0.6996$

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 15:16:37
[[file:Lezioni/screenshot_2018-03-24_15-16-37.png]]

Se invece avessimo voluto determinare la probabilità $P(X \in [10.53, 12.15])$. Avremmo dovuto innanzitutto osservare
che $P(X \in [10.53, 12. 15]) = P(Z \in [0.53, 2.15])$. Tramite le tavole determinare
$P(Z \in [0.00, 0.53]) = 0.2019$ e $P(Z in [0.00, 2.15]) = 0.4842$ ed osservare che
$P(Z \in [0.53, 2.15]) = P(Z \in [0.00, 2.15]) - P(Z \in [0.00, 0.53]) = 0.4842 - 0.2019 = 0.2823$

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 15:19:48
[[file:Lezioni/screenshot_2018-03-24_15-19-48.png]]

Infine, se avessimo voluto determinare la probaiblità $P(X \in [10.3, +\infty])$, avremmo dovuto innanzitutto osservare che
$P(X \in [10.3, +\infty]) = P(Z \in [0.3, +\infty]) = 1 - P(Z \in [-\infty, 0.3])$. Poiché in base alle tavole
$P(Z \in [0, 0.3]) = 0.1179$ segue che
$P(X \in [10.3, +\infty]) = 1 - P(Z \in [-\infty, 0.3]) = 1 - 0.5 - 0.1179 = 0.3821$

/Regole di calcolo per normali standardizzate da tabelle per integrali/ $\int_0^b f(u)du$:
- Integrali della forma $\int_{-\infty}^b f(u)du$:
  - $b$ finito > 0: $\int_{-\infty}^b f(u)du = \frac{1}{2} + \int_0^b f(u)du$;
  - $b$ finito < 0: $\int_{-\infty}^b f(u)du = \frac{1}{2} - \int_0^b f(u)du$;
- $\int_a^{+\infty} f(u)du = 1 - \int_{-\infty}^a f(u)du$;
- $\int_a^b f(u)du = \int_{-\infty}^b f(u)du - \int_{-\infty}^a f(u)du$;

In base a quanto presentato nell'esempio è possibile verificare che per ogni variabile $X$ con distribuzione Normale
e per qualsiasi valore dei suoi parametri valgono le seguenti eguaglianze:
- $P(X \in [\mu - \sigma, \mu + \sigma]) = 0.683$;
- $P(X \in [\mu - 2 \cdot \sigma, \mu + 2 \cdot \sigma]) = 0.954$;
- $P(X \in [\mu - 3 \cdot \sigma, \mu + 3 \cdot \sigma]) = 0.997$.
È interessante verificare in base alla terza di tali uguaglianze che ogni variabile casuale $X \sim N(\mu, \sigma)$ assumerà valori compresi
tra $\mu - 3\sigma$ e $\mu + 3\sigma$ con probabilità molto prossima ad uno.

Una delle principali /proprietà/ della distribuzione normale è quella /di chiusura rispetto all'operazione di somma di variabili
aleatorie stocasticamente indipendenti/:
- Se $X_1 \sim N(\mu_1, \sigma_1)$ e $X_2 \sim N(\mu_2, \sigma_2)$ e se $X_1$ e $X_2$ sono indipendenti, allora la variabile $Y = X_1 + X_2$ è tale che
  $Y \sim N(\mu_1 + \mu_2, \sqrt{\sigma_1^2 + \sigma_2^2})$
In altri termini la variabile somma di due variabili aleatorie stocasticamente indipendenti con distribuzioni normali è
ancora una variabile aleatoria distribuita secondo una normale i cui parametri sono ricavabili facilmente da quelli delle
distribuzioni degli addendi.

*** Distribuzione Chi-Quadro
Siano $X_1, \dots, X_n$ $n$ variabili con distribuzione normale di parametri 0 ed 1 (normali standardizzate) ed
indipendenti tra loro. Sia poi $X$ una variabile aleatoria definita come somma dei quadrati delle $X_i$
ovvero sia $X = X_1^2 + \dots + X_n^2$. Una tale variabile è distribuita secondo una /Chi-Quadro con $n$ gradi di libertà/:
$X \sim \chi_n^2$.

Notiamo che, essendo definita come somma di quadrati, una variabile con distribuzione Chi-Quadro può assumere solo valori
non negativi.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 16:06:24
[[file:Lezioni/screenshot_2018-03-24_16-06-24.png]]

*** Distribuzione t di STUDENT
Siano $Z \sim N(0, 1)$ e $Y \sim \chi_n^2$ due variabili indipendenti. Sia poi $X$ una variabile aleatoria definita come
$$X = \frac{Z}{\sqrt{\frac{Y}{n}}}$$
Una tale variabile è distribuita secondo una /t di student/ con $n$ gradi di libertà: $X \sim t_n$. Questa distribuzione è
di grande interesse come la distribuzione Normale e la distribuzione Chi-Quadro.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 16:10:15
[[file:Lezioni/screenshot_2018-03-24_16-10-15.png]]

*** Distribuzione F di FISHER
Siano $U \sim \chi_m^2$ e $V \sim \chi_n^2$ due variabili indipendenti. Sia poi $X$ una variabile aleatoria definita come
$$X = \frac{\frac{U}{m}}{\frac{V}{n}}$$
Una tale variabile è /distribuita secondo una $F$ con $m$ ed $n$ gradi di libertà/: $X \sim F(m, n)$.

Questa distribuzione è di grande interesse coma la distribuzione Normale, la distribuzione Chi Quadro e la distribuzione t
di Student.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-24 16:12:57
[[file:Lezioni/screenshot_2018-03-24_16-12-57.png]]
** Teoremi di Convergenza
In questo capitolo presentiamo 2 teoremi fondamentali nella statistica inferenziale:
- Legge dei Grandi Numeri;
- Teorema Limite Centrale.
Questi 2 teoremi richiedono la nozione di convergenza di variabili aleatorie.

/Convergenza in distribuzione/

Consideriamo una successione $\{X_n, n \in \mathbb{N}\}$ di variabili aleatorie, e sia $F_n$ la funzione di ripartizione
della generica variabile $X_n$ della successione. Diremo che /la successione converge in distribuzione alla variabile
X/ avente funzione di ripartizione $F$ se vale $\lim_{n \to \infty}F_n(t) = F(t)$ per ogni $t \in \mathbb{R}$ che sia punto
di continuità per la $F$. Verranno utilizzate le notazioni $X_n \overset{d}{\to} X \quad F_n \overset{d}{\to} F$.

Per esempio, consideriamo una successione di variabili aleatorie $\{X_n, n \in \mathbb{N}\}$ in cui la generica variabile
$X_n$ ha funzione di ripartizione
\begin{equation*}
F_n(t) =
\begin{cases}
0 &\text{ se $t \leq 0$}\\
t^{\left(\frac{n}{n+1}\right)} &\text{ se $0 < t < 1$}\\
1 &\text{ se $t \geq 1$}
\end{cases}
\end{equation*}

Osserviamo ora che per ogni $t \in (0, 1) \subseteq \mathbb{R}$ vale $\lim_{n \to \infty}t^{\left(\frac{n}{n+1}\right)} = t$ da
cui se ne deduce che vale $X_n \overset{d}{\to} X$ dove $X$ ha funzione di ripartizione data da
\begin{equation*}
F_n(t) =
\begin{cases}
0 &\text{ se $t \leq 0$}\\
t &\text{ se $0 < t < 1$}\\
1 &\text{ se $t \geq 1$}
\end{cases}
\end{equation*}

Ovvero la successione di variabili aleatorie considerata converge ad una variabile aleatoria avente distribuzione
uniforme di supporto $[0, 1]$.

Il grafico della successione per $n = 10$ mostra un comportamento molto simile a quello proprio di una variabile
aleatoria con distribuzione uniforme in $[0, 1]$.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-26 15:50:00
[[file:Lezioni/screenshot_2018-04-26_15-50-00.png]]

Nella definizione di convergenza in distribuzione vengono esclusi, nel passaggio al limite, i punti in cui la funzione
di ripartizione limite $F$ è discontinua, e ciò affinché si abbia un concetto di convergenza il più possibile vicino
all'intuizione.

Consideriamo ad esempio una successione di numeri reali $\{a_n, n \in \mathbb{N}\}$ tale che sia $a_n \overset{d}{\to} a$ per
$n \to \infty$ dove $a \in \mathbb{R}$ e pensiamo alle variabili aleatorie $X_n$ aventi funzione di ripartizione
\begin{equation*}
F_n(t) =
\begin{cases}
0 &\text{ se $t < a_n$}\\
1 &\text{ se $t \geq a_n$}
\end{cases}
\end{equation*}

In pratica la generica variabile $X_n$ assume valore $a_n$ con probabilità uguale ad uno. Da un punto di vista intuitivo
siamo portati a pensare che valga $X_n \overset{d}{\to} X$ dove la variabile $X$ ha funzione di ripartizione
\begin{equation*}
F (t) =
\begin{cases}
0 &\text{ se $t < a$}\\
1 &\text{ se $t \geq a$}
\end{cases}
\end{equation*}
Ovvero $X=a$ con probabilità uguale ad uno.

Osserviamo però che se la successione ${a_n, n \in \mathbb{N}}$ è tale che $a < a_n$ con $n$ pari e $a_n < a$ con $n$
dispari, allora risulta $F_n(a) = 0$ per $n$ pari e $F_n(a) = 1$ per n dispari. Pertanto non esiste il limite
$\lim_{n \to \infty} F_n(a)$.

Pertanto non saremmo autorizzati a dire che vale $X_n \overset{d}{\to} X$. Non essendo definibile in $a$ la funzione
di ripartizione limite $F$.

Problemi di questo tipo non si verificano se si escludono, nella definizione di convergenza, i valori in cui la $F$
limite non è continua. Si osservi infatti che per tutti gli altri valori di $t$ risulta, correttamente:
\begin{gather*}
&\lim_{n \to \infty} F_n(t) = 0 = F(t) \quad \text{ se } t < a\\
&\lim_{n \to \infty} F_n(t) = 1 = F(t) \quad \text{ se } a < t
\end{gather*}

Osservazione: segnaliamo che la convergenza in distribuzione non è l'unico tipo di convergenza tra variabili aleatorie
definito in letteratura. Due tipi di convergenza estremamente importanti sono ad esempio la
- Convergenza quasi certa;
- Converganza in probabilità;
che comunque possiamo non considerare ai fini della nostra trattazione.
*** Legge dei grandi numeri
Consideriamo una successione $\{X_i, i \in \mathbb{N}_+\}$ di variabili aleatorie, Indipendenti ed Identicamente
Distribuite (I.I.D). Consideriamo poi la variabile aleatoria definita come
$$ \bar{X}_n = \frac{X_1 + \dots + X_n}{n}$$
detta /media aritmetica \(n\)-sima della successione/.

Se le variabili $X_i$ hanno /valore atteso e varianza esistenti e finiti/ $E[X_i] = \mu$ e $V[X_i] = \sigma^2$ allora vale
$$\bar{X}_n \overset{d}{\to} M$$
dove $M$ è una variabile aleatoria che assume valore $\mu$ con probabilità 1.

La proprietà appena introdotta costituisce una /forma debole/ del risultato noto sotto il nome di /Legge dei Grandi Numeri/.

La /Legge dei Grandi Numeri/ asserisce che:

Se consideriamo una successione di variabili aleatorie I.I.D $\{X_i, i \in \mathbb{N}_+\}$ per cui esistono valore atteso e
varianza (finiti) allora possiamo affermare che la successione $\{\bar{X}_n, n \in \mathbb{N}\}$ delle corrispondenti medie
aritmetiche tende, al crescere di $n$, ad una variabile che assume certamente il valore $E[X_i] = \mu$.

In altre parole, se consideriamo una successione $\{x_i, i \in \mathbb{N}_+\}$ di realizzazioni delle variabili
$\{X_i, i \in \mathbb{N}_+\}$ e se consideriamo la successione $\{\bar{x}_n, n \in \mathbb{N}_+\}$ delle corrispondenti
realizzazioni delle medie aritmetiche, abbiamo che questa seconda successione tende, per $n$ tendente ad infinito,
al valore $E[X_i] = \mu$.

Per esempio, supponiamo di avere acquistato 50 appartamenti dello stesso valore commerciale e di averli successivamente
ristrutturati con l'intenzione di rivenderli ottenendone un profitto. Supponiamo di sapere che, tenuto conto della
situazione di mercato e delle eventuali contrattazioni con gli acquirenti, il ricavato della vendita di ciascuno di
questi appartamenti (milioni di lire) sia rappresentabile con una variabile $X_i, i = 1, \dots, 50$, avente distribuzione
triangolare di supporto $[5, 15]$. Supponiamo poi che sia lecito assumere che i singoli ricavi $X_i$, siano indipendenti
tra loro.

La Legge dei Grandi Numeri consente di affermare che la /media aritmetica dei ricavi/ che otterremo per ogni appartamento,
data dalla variabile $\bar{X}_{50}$ non si discosterà troppo dal valore atteso, della singola $X_i$, ovvero da $\frac{5 + 15}{2} = 10$.
Pertanto il ricavato totale dell'investimento non si discosterà troppo da $10 \cdot 50 = 500$.

Osservazione: in realtà le ipotesi della Legge dei Grandi Numeri possono essere indebolite rispetto a quelle riportare in
precedenza. Infatti, esistono versioni alternative di questa proprietà in cui non è richiesta l'ipotesi che le variabili
$X_i$ siano identicamente distribuite.
*** Teorema Limite Centrale
La Legge dei Grandi Numeri assicura la convergenza $\bar{X}_n \overset{d}{\to} M$, ma non specifica nulla circa la rapidità con
cui questa avviene. Non sappiamo dire per quale valore di $n$ sarà lecito pensare che una realizzazione $\bar{x}_n$ assuma
valore $\mu$ o un valore molto prossimo ad esso.

Nell'esempio visto si asserisce infatti che il ricavo totale dell'investimento sarà prossimo a 500 milioni ma non quanto
prossimo.

È intuitivo pensare che la convergenza sia tanto più rapida quanto più la varianza sarà piccola. Quanto appena asserito
viene formalizzato tramite il /Teorema Limite Centrale/, il quale specifica quale sia la distribuzione della variabile
aleatoria $\bar{X}_n$ per $n$ /sufficientemente/ grande e quali siano il valore atteso e la varianza della stessa.

Sia $\{X_i, i \in \mathbb{N}_+\}$ una successione di variabili aleatorie che soddisfa le ipotesi della Legge dei Grandi
Numeri, ovvero siano le $X_i$ I.I.D ed aventi valore atteso $E[X_i] = \mu$ e varianza $V[X_i] = \sigma^2$ entrambe esistenti e finiti.

Consideriamo la variabile aleatoria $S_n$ definita come segue: $S_n = X_1 + \dots + X_n$, vale
$$S_n \overset{d}{\to} X \sim N(n \cdot \mu, \sqrt{n} \cdot \sigma) = N(n \cdot \mu, n \cdot \sigma^2)$$
ovvero $S_n$ converge in distribuzione ad una variabile distribuita come una Normale di media $n \cdot \mu$ e deviazione
standard $\sqrt{n} \cdot \sigma$.

Tale proprietà è una forma debole di un noto risultato che prende il nome di /Teorema Limite Centrale/. Osservato
che vale $\bar{X}_n = \frac{S_n}{n}$, dalle seguenti relazioni:
$$S_n \overset{d}{\to} X \sim N(n \cdot \mu, \sqrt{n} \cdot \sigma) = N(n \cdot \mu, n \cdot \sigma^2)$$
$$E[a \cdot X + b] = a \cdot E[X] + b \quad \text{per ogni variabile $X$ e per ogni $a, b \in \mathbb{R}$}$$
si ricava
$$E[\bar{X}_n] = E\left[\frac{S_n}{n}\right] = \frac{1}{n} E[S_n] = \frac{1}{n} E[X_1 + \dots + X_n] = \frac{1}{n}(n \mu) = \mu$$
e dalle seguenti
$$S_n \overset{d}{\to} X \sim N(n \cdot \mu, \sqrt{n} \cdot \sigma) = N(n \cdot \mu, n \cdot \sigma^2)$$
$$V[a \cdot X + b] = a^2 \cdot V[X] \quad \text{per ogni variabile $X$ e per ogni $a, b \in \mathbb{R}$}$$
si ricava
$$V[\bar{X}_n] = V\left[\frac{S_n}{n}\right] = \frac{1}{n^2} V[S_n] = \frac{1}{n^2} V[X_1 + \dots + X_n] = \frac{1}{n^2}(n \sigma^2) = \frac{\sigma^2}{n}$$

Si conclude che
$$ \bar{X}_n \overset{d}{\to} X \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right) = N \left(\mu, \frac{\sigma^2}{n}\right)$$

Praticamente il /Teorema Limite Centrale/ asserisce che per $n$ /sufficientemente/ grande, possiamo approssimare le
variabili $S_n$ e $\bar{X}_n$ con delle variabili aventi distribuzione normale, i cui parametri dipendono da quelli
delle variabili $X_i$. Relativamente all'espressione /per $n$ sufficientemente grande/ possiamo dire che in genere
si utilizza questa approssimazione tutte le volte che $n \geq 30$.

Per esempio, riprendiamo in considerazione l'esempio dell'investimento immobiliare. Abbiamo visto che dobbiamo attenderci
un ricavo totale prossimo ai 500 milioni di lire. Supponiamo ora di avere pronto un preventivo delle spese da sostenere
per la ristrutturazione degli appartamenti e supponiamo che questo sia di 470 milioni di lire. Qual è la probabilità
che il nostro ricavo sia inferiore alle spese che dobbiamo sostenere per le ristrutturazioni?

Per rispondere alla domande precedente possiamo fare ricorso al Teorema Limite Centrale. Infatti, questo teorema consente
di pensare alla variabile $S_n$ (totale ricavi) $S_n = X_1 + \dots + X_n$ come ad una variabile normalmente distribuita, i cui
parametri si ricavano da quelli delle variabili $X_i$, che sono: $E[X_i] = \frac{5 + 15}{2} = 10$ e
$V[X_i] = \frac{(15 - 5)^2}{24} = 4.16$.

I parametri $\mu$ e $\sigma$ relativi alla seguente formula:
$$S_n \overset{d}{\to} X \sim N(n \cdot \mu, \sqrt{n} \cdot \sigma) = N(n \cdot \mu, n \cdot \sigma^2)$$
sono quindi rispettivamente 10 e 2.04, ed è lecito pensare che $S_n \sim N(50 \cdot 10, \sqrt{50} \cdot 2.04) = N(500, 14.42)$.

Per rispondere alla nostra domanda basta ora osservare che
$$P(\text{spesa supera ricavato}) = P(S_n \leq 470) = P \left(\frac{S_n - 500}{14.42} \leq \frac{470 - 500}{14.42}\right) =
P(Z \leq -2.08) = P(Z \geq 2.08) = 1 - P(Z \leq 2.08)$$

Dove $Z$ è una Normale Standardizzata.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-26 19:22:21
[[file:Lezioni/screenshot_2018-04-26_19-22-21.png]]

$$P(S_n \leq 470) = 1 - P(Z \leq 2.08) = 1 - 0.98124 = 0.01876$$
** Stime di parametri
La /statistica inferenziale/ consente di dedurre particolari caratteristiche di una popolazione limitandosi ad analizzare
un numero finito e preferibilmente piccolo di suoi individui. Quando le /caratteristiche/ che si vogliono individuare sono
/esprimibili numericamente/, allora esse sono dette /parametri/.

Per /stima di parametri/ si intende quindi il problema della /deduzione/ di /caratteristiche di tipo numerico/ di una
/popolazione/ facendo ricorso per questo all'analisi di un suo sottoinsieme finito opportunamente scelto detto /campione/.

Diverse tecniche possono essere utilizzate per effettuare delle stime di parametri. Noi ci limiteremo a considerare quelle
classiche basate sulla conoscenza delle /distribuzioni campionarie/, vale a dire distribuzioni di particolari indici
statistici associati alle caratteristiche del campione. Tecniche alternative vengono comunque succintamente descritte
nel paragrafo conclusivo di questo capitolo.
*** Campionamento e campioni
Diverse ragioni possono portare a voler determinare le caratteristiche di una popolazione facendo ricorso esclusivamente
ad un numero limitato di suoi individui. Può trattarsi di un problema economico o di tempo, può altresì capitare che non
tutti gli elementi della popolazione siano disponibili o ancora che le misure da effettuare "distruggano" le unità della
popolazione che vengono analizzate. In questi casi occorre allora effettuare un /campionamento/, vale a dire una scelta
degli individui che verranno analizzati per effettuare le inferenze sull'intera popolazione.

Questo è un problema che non va trascurato, infatti dal metodo utilizzato nel condurre un campionamento dipende anche
la validità della tecnica utilizzata nella fase di inferenza.

Tutte le tecniche che verranno presentate in questo capitolo sono valide solo nel caso in cui il campione sia stato scelto
secondo una procedura detta /campionamento casuale/, che assegna la stessa probabilità di essere estratto ad ogni individuo
della popolazione.

Il metodo solitamente utilizzato per generare un /campione casuale/ consiste nell'assegnare, in maniera progressiva, un
numero ad ogni individuo della popolazione, e quindi estrarre, con un qualsiasi generatore casuale, tanti numeri quanti
devono essere gli elementi del campione.

Questa procedura è dispendiosa in termini di tempo, ma per contro occorre tener presente che altrimenti è facile commettere
l'errore di /ritenere casuale un campione che invece non lo è/.

Nel caso in cui la popolazione sulla quale condurre l'indagine sia costituita dagli abitanti di una città potremmo essere
portati a scegliere il campione facendo uso di un elenco telefonico o fermando a caso le persone per strada.

Così facendo non si ottiene un campione casuale:
- Nel primo caso vengono escluse le persone che non posseggono un apparecchio telefonico;
- Nel secondo caso le persone che raramente escono di casa hanno minore probabilità di essere parte del campione.

Un'altra considerazione che occorre sempre fare durante un'operazione di campionamento riguarda la possibilità di estrarre più
volte uno stesso individuo (campionamento con ripetizione o senza ripetizione). La scelta tra queste 2 alternative diviene
rilevante quando la popolazione considerata è di numerosità limitata e diviene trascurabile nel caso di popolazioni di
vaste dimensioni o infinite, e questo è il caso che verrà trattato d'ora in avanti.

Nel seguito introduciamo alcune definizioni e notazioni relative ai campioni casuali ed alle distribuzioni di variabili ad
essi associate.

A tale scopo denotiamo con $X$ il carattere della popolazione su cui siamo interessati a fare dell'inferenza. Ovviamente
il valore assunto da questo carattere varia a seconda dell'individuo considerato. Pertanto conviene pensare ad $X$ come
ad una variabile aleatoria la cui distribuzione (sconosciuta) corrisponde a quella che si otterrebbe facendo ricorso alle
tecniche della statistica descrittiva (potendo analizzare l'intera popolazione), e pensare invece ai valori assunti dai
singoli individui come a delle realizzazioni di $X$.

Formalmente, ipotizzando di aver effettuato un /campionamento casuale da una popolazione di numerosità infinita/,
un /campione casuale di numerosità $n$/ è una \(n\)-pla $(X_1, \dots, X_n)$ di /variabili aleatorie stocasticamente
indipendenti/ aventi ognuna la /stessa distribuzione del carattere $X$/ della popolazione.

I valori $(x_1, \dots, x_n)$ assunti da questa \(n\)-pla sono una /realizzazione/ di $(X_1, \dots, X_n)$.

Per comodità abbiamo l'espressione /distribuzione della popolazione/ anziché il termine più corretto /distribuzione
del carattere in esame della popolazione $X$/.

Un /parametro/ è un /valore numerico che descrive una caratteristica della popolazione/, e come tale è una
grandezza associata alla sua distribuzione.

Una /stima/ è invece una /misura che descrive una caratteristica del campione/, o meglio un'espressione funzionale
delle realizzazioni $(x_1, \dots, x_n)$ di $(X_1, \dots, X_n)$.

Per esempio, si consideri la seguente tabella:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-26 22:01:26
[[file:Lezioni/screenshot_2018-04-26_22-01-26.png]]

dove i valori rappresentano i costi al metro quadro di 80 appartamenti scelti a caso tra quelli di un determinato quartiere
di una città italiana.

Possiamo pensare a tali valori come ad una realizzazione $(x_1, x_{80})$ di un campione $(X_1, \dots, X_{80})$ di numerosità $n = 80$
e che si riferisce alla popolazione $X := \text{"costo al metro quadro degli appartamenti del quartiere"}$. Ovviamente
tale costo non è lo stesso per ogni appartamento, per questa ragione si pensa ad $X$ come ad una variabile aleatoria,
la cui distribuzione $F$ coincide con la distribuzione di frequenza cumulata che si otterrebbe se fossero noti i costi al
metro quadro di tutti gli appartamenti del quartiere.

Il costo di ogni singolo appartamento corrisponde invece ad una specifica realizzazione di $X$. Consideriamo ora il parametro
$\mu$ "costo medio (al metro quadro) degli appartamenti del quartiere". Ovviamente non possiamo dire quale sia il valore di
tale parametro, non avendo a disposizione i dati relativi a tutti gli appartamenti. Però possiamo farne una stima considerando
la media aritmetica dei costi degli 80 appartamenti estratti con campionamento casuale, ovvero
$$ \bar{x} = \frac{1}{80} \sum_{i=1}^{80} x_i = 2.68$$
Presumibilmente il valore vero di $\mu$ sarà diverso da 2.68, ma questa è comunque una sua stima, sulla cui accuratezza, come
vedremo, si possono fare diversi commenti.

Si osservi che, a priori, le stime non sono altro che delle realizzazioni di variabili aleatorie definite come funzione del
campione $(X_1, \dots, X_n)$ in cui non compare alcun parametro incognito, ovvero sono del tipo $H_n = h(X_1, \dots, X_n)$ dove $h$
è una funzione in $n$ variabili.

Le variabili aleatorie definite in questo modo sono dette /statistiche campionarie/ e sono dette /distribuzioni campionarie/
le loro distribuzioni.

Come vedremo, la conoscenza della distribuzione campionaria di una statistica è fondamentale nella formulazione e nella
verifica di ipotesi fatte sulla popolazione partendo dai dati campionari. Concludiamo accennando ad alcuni metodi di
campionamento alternativi al campionamento casuale.

Un metodo noto è quello detto /stratificato/, che presuppone una suddivisione preventiva della popolazione in gruppi detti
strati con caratteristiche omogenee.

Effettuata questa operazione gli individui che costituiscono il campione vengono poi estratti da ogni gruppo in
proporzione alla numerosità del gruppo stesso (/campionamento stratificato proporzionale/). Il vantaggio di tale metodo
consiste nel fatto che se i gruppi sono stati creati in maniera appropriata esso permette di ridurre la numerosità finale
del campione. A scapito di ciò però tutte le formule classiche utilizzate nella fase inferenziale devono essere di volta
in volta modificate. Inoltre, la suddivisione della popolazione in strati comporta sempre un aumento del tempo necessario
al campionamento.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-26 22:19:08
[[file:Lezioni/screenshot_2018-04-26_22-19-08.png]]

Un secondo metodo di campionamento alternativo è il /campionamento a grappoli/ che prevede una fase di suddivisione della
popolazione in gruppi, in questo caso gruppi eterogenei, in modo tale che ogni singolo gruppo sia rappresentativo
dell'intera popolazione. Fatta questa operazione è sufficiente limitarsi ad estrarre un singolo gruppo quale campione,
anziché estrarre dei singoli individui da ogni gruppo. Questo metodo presenta dei vantaggi in termini di raccolta dei dati
ma risulta generalmente meno efficiente degli altri in termini di inferenze.

Tra gli altri metodi ricordiamo il /campionamento longitudinale/ ed il /casuale doppio/. Nella pratica è poi uso comune
fare ricorso a più di uno di questi metodi contemporaneamente.

Ricordiamo infine che un altro problema associato al campionamento è quello della scelta della /numerosità del campione/.
I criteri da adottare per questa scelta risulteranno più chiari con le nozioni presentate nei prossimi paragrafi.
*** Principali distribuzioni campionarie
In base a quanto affermato in precedenza possiamo pensare al carattere della popolazione su cui vogliamo fare delle
inferenze come ad una /variabile aleatoria $X$/, avente una /funzione di ripartizione $F$ sconosciuta/, ma /corrispondente
alla distribuzione di frequenza cumulata di tale carattere/, che si potrebbe ottenere se fosse possibile analizzare
per intero la /popolazione/.

Una stima di un parametro $F$ è costituita da una /funzione di una realizzazione/ $(x_1, \dots, x_n)$ di un campione
casuale, che è una \(n\)-pla $(X_1, \dots, X_n)$ di variabili stocasticamente indipendenti ed aventi tutte distribuzione $F$.

In pratica una /stima/ è una /realizzazione di una statistica campionaria/ $H_n = h(X_1, \dots, X_n)$. Ogni statistica campionaria,
essendo una funzione di variabili aleatorie, è una variabile aleatoria, e come tale avrà una sua distribuzione.

Nel seguito prenderemo in considerazione alcune statistiche particolarmente importanti e ne descriveremo le principali
proprietà. Nel seguito denoteremo con $\mu$ e $\sigma^2$ il /valore atteso/ e la /varianza/ della /popolazione $X$/
(distribuita secondo $F$ ignota).

Considerato un campione $(X_1, \dots, X_n)$ estratto da una popolazione con distribuzione $F$, media $\mu$ e deviazione
standard $\sigma$, è detta /media campionaria \(n\)-sima/ la variabile $\bar{X}_n = \frac{X_1 + \dots + X_n}{n}$.

È detta poi "/distribuzione campionaria della media \(n\)-sima/" la distribuzione della variabile $\bar{X}_n$.

È intuitivo pensare che la statistica $\bar{X}_n$ sia appropriata per stimare la media $\mu$ della popolazione e come
vedremo ciò è vero. In genere trovare l'espressione analitica della distribuzione campionaria di $\bar{X}_n$
conoscendo la distribuzione $F$ di $X$ non è facile. È però facile determinare il /valore atteso/ e la /varianza/ di
$\bar{X}_n$.

Ricordando che si assume indipendenza tra le variabili $X_1, \dots, X_n$ del campione e che queste hanno tutte /valore atteso/
uguale a $\mu$ e /varianza/ uguale a $\sigma^2$. Allora in base alla seguente proprietà:
$$E[a \cdot X + b \cdot Y] = a \cdot E[X] + b \cdot E[Y] \text{ per ogni coppia $X, Y$ e per ogni $a, b, \in \mathbb{R}$}$$
otterremo
$$E[\bar{X}_n] = E\left[\frac{X_1 + \dots + X_n}{n}\right] = \frac{1}{n} E[X_1 + \dots + X_n] = \frac{1}{n}(E[X_1] + \dots + E[X_n]) =
\frac{1}{n}(n \mu) = \mu$$
Mentre, in base alle seguenti proprietà:
$$V[a \cdot X + b] = a^2 \cdot V[X] \quad \text{per ogni variabile $X$ e per ogni $a, b \in \mathbb{R}$}$$
otterremo
$$V[\bar{X}_n] = V\left[\frac{S_n}{n}\right] = \frac{1}{n^2} V[S_n] = \frac{1}{n^2} V[X_1 + \dots + X_n] = \frac{1}{n^2}(n \sigma^2) = \frac{\sigma^2}{n}$$
Osserviamo che mentre il valore atteso di $\bar{X}_n$ non dipende dalla numerosità del campione $n$, la sua varianza dipende
anche da $n$ ed è tanto minore quanto $n$ è più grande. Questo vuol dire che le realizzazioni $\bar{x}_n$ di $\bar{X}_n$
saranno tanto più vicine al valore incognito $\mu$ quanto più è grande la numerosità del campione.

Inoltre abbiamo visto in precedenza che per $n$ sufficientemente grande la variabile $\bar{X}_n$ può essere approssimata
con una variabile avente distribuzione normale di parametri $\mu$ e $\frac{\sigma^2}{n}$ e ciò indipendentemente dall'espressione
della distribuzione $F$.

Una considerazione particolare va fatta nel caso in cui la distribuzione della popolazione
sia già normale in partenza. In questo caso, per la proprietà di chiusura rispetto all'operazione di somma di variabili
aleatorie con distribuzione normale, la media campionaria $\bar{X}_n$ è anch'essa una variabile con distribuzione
normale, ovvero vale $\bar{X}_n \sim N\left(\mu, \frac{\sigma^2}{n}\right)$.

La seconda statistica che viene introdotta è solitamente usata per stimare la varianza della popolazione.

Considerato un campione $(X_1, \dots, X_n)$ estratto da una popolazione con distribuzione $F$, media $\mu$ e deviazione standard
$\sigma$, è detta /varianza campionaria \(n\)-sima/ la variabile:
$$S_n^2 = \frac{1}{n} \cdot \sum_{i=1}^n (X_i - \bar{X}_n)^2$$
È detta poi "/distribuzione campionaria della varianza \(n\)-sima/" la distribuzione della variabile $S_n^2$.

Come nel caso precedente, trovare l'espressione analitica della distribuzione di $S_n^2$ conoscendo $F$ non è facile.
È però un po' più facile determinare il valore atteso e la varianza di $S_n^2$. Infatti, è possibile dimostrare che valogno
le seguenti relazioni:
\begin{gather*}
E[S_n^2] = \frac{n-1}{n} \cdot \sigma^2\\
V[S_n^2] = \frac{1}{n}\left(E[(X - \mu)^4] - \frac{n-3}{n-1} \cdot \sigma^4 \right)
\end{gather*}
Anche per questa statistica è possibile dimostrare, facendo ricorso al Teorema Limite Centrale, che per $n$ sufficientemente
grande la sua distribuzione può essere approssimata con una normale con parametri dati dalle formule precedenti.

Notiamo che la presenza del coefficiente $\frac{n-1}{n}$ può talvolta risultare scomoda e, per tale ragione, in molti processi
inferenziali si preferisce considerare alternativamente alla varianza campionaria, una nuova statistica detta
/varianza campionaria \(n\)-sima corretta/:
\begin{gather*}
E[S_n^2] = \sigma^2\\
\hat{S}_n^2 = \frac{n}{n-1} \cdot S_n^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2
\end{gather*}

Nel caso particolare in cui la popolazione sia normalmente distribuita si possono dimostrare due importanti risultati che
coinvolgono la statistica $S_n^2$. A tale scopo è utile introdurre due nuove variabili aleatorie:
\begin{gather*}
Q_n = \frac{n \cdot S_n^2}{\sigma^2} = \frac{(n-1) \cdot \hat{S}_n^2}{\sigma^2}\\
T_n = \frac{\bar{X}_n - \mu}{\frac{\hat{S}_n}{\sqrt{n}}}
\end{gather*}
dove $\hat{S}_n = \sqrt{\hat{S}_n^2}$.

Le due quantità appena introdotte /non vengono dette statistiche/, in quanto esse sono funzioni anche dei parametri $\mu$ e $\sigma^2$,
che vanno pensati come non noti.

Si può dimostrare che nell'ipotesi in cui $(X_1, \dots, X_n)$ sia un campione casuale estratto da una popolazione con distribuzione
Normale con parametri $\mu$ e $\sigma^2$, allora:
- $Q_n$ è distribuito come una /Chi-quadro/: $Q_n = \frac{n \cdot S_n^2}{\sigma^2} = \frac{(n-1) \cdot \hat{S}_n^2}{\sigma^2} \sim \chi_{n-1}^2$;
- $T_n$ è distribuito coma una /t di Student/: $T_n = \frac{\bar{X}_n - \mu}{\frac{\hat{S}_n}{\sqrt{n}}} \sim t_{n-1}$.
Entrambe con $(n-1)$ gradi di libertà.

Si osservi che le distribuzioni delle quantità $Q_n$ e $T_n$ non dipendono dai parametri $\mu$ e $\sigma^2$ poiché la Chi-quadro
e la t di Student hanno i gradi di libertà $(n-1)$ quale unico parametro.
*** Stimatori e stime puntuali
Sia $\theta$ un parametro incognito della popolazione $X$. Una statistica campionaria $H_n = h(X_1, \dots, X_n)$ è detta
/stimatore puntuale/ quando viene utilizzata per stimare il parametro incognito $\theta$.

È detta invece /stima puntuale/ del parametro $\theta$, il valore $\hat{\theta}(x_1, \dots, x_n)$ assunto dallo stimatore puntuale
$H_n = h(X_1, \dots, X_n)$ nella realizzazione $x_1, \dots, x_n$ del campione casuale.

Vi sono due proprietà minime di cui una statistica campionaria deve godere affinché possa essere considerata uno stimatore
puntuale, queste sono:
- /Proprietà di correttezza/: uno /stimatore/ $H_n = h(X_1, \dots, X_n)$ del parametro $\theta$ è detto /corretto/ (/non distorto/),
  se, qualunque sia l'effettivo valore del parametro $\theta$ risulta $E[H_n] = E[h(X_1, \dots, X_n)] = \theta$;
- /Proprietà di consistenza/: uno /stimatore/ $H_n = h(X_1, \dots, X_n)$ del parametro $\theta$ è detto /consistente/ se, qualunque
  sia l'effettivo valore del parametro $\theta$ risulta $\lim_{n \to \infty} P[|H_n - \theta| \leq \epsilon] = 1$ per ogni $\epsilon > 0$.

Talvolta è difficile verificare se uno stimatore è consistente. Comunque, è possibile dimostrare che uno stimatore corretto 
è anche consistente se vale $\lim_{n \to \infty} V[H_n] = 0$.

Per esempio, consideriamo le due statistiche campionarie $\bar{X}_n$ e $\hat{S}_n^2$, stimatori puntuali dei parametri $\mu$ e $\sigma^2$
della popolazione. Questi sono stimatori corretti, essendo:
$$E[\bar{X}_n] = \mu \quad E[\hat{S}_n^2] = \sigma^2$$
Inoltre, sono anche consistenti, infatti valgono
\begin{gather*}
\lim_{n \to \infty} V[\bar{X}_n] = \lim_{n \to \infty} \frac{\sigma^2}{n} = 0\\
\lim_{n \to \infty} V[\hat{S}_n^2] = \lim_{n \to \infty} \frac{n}{(n-1)^2} \left(E\left[(X - \mu)^4 \right] - \frac{n-3}{n-1} \sigma^4 \right) = 0\\
\hat{S}_n^2 = \frac{n}{n-1} \cdot S_n^2\\
V[\hat{S}_n^2] = V\left[\frac{n}{n-1} \cdot S_n^2 \right] = \frac{n^2}{(n-1)^2} V[S_n^2]\\
V[S_n^2] = \frac{1}{n} \left( E \left[(X - \mu)^4 \right] - \frac{n-3}{n-1} \sigma^4 \right)
\end{gather*}

Osservazione: in base a quanto appena affermato la /varianza campionaria corretta/ $\hat{S}_n^2$ è uno /stimatore corretto/
di $\sigma^2$. Occorre però fare attenzione che contrariamente a quanto si è portati a pensare, la sua radice
$\hat{S}_n = \sqrt{\hat{S}_n^2}$ non è uno stimatore corretto della deviazione standard $\sigma$ della popolazione:
$$E[\hat{S}_n] \neq \sigma$$
Proprio per tale ragione, nelle analisi statistiche troviamo sempre riportare le stime delle varianze anziché quelle delle
deviazioni standard.

Osserviamo che per stimare un generico parametro $\theta$ possono essere definiti diversi stimatori. In molti casi è possibile
stabilire un criterio per dire se uno stimatore è preferibile ad un altro.

Siano per questo $H_{1, n} = h_1 (X_1, \dots, X_n)$ e $H_{2, n} = h_2 (X_1, \dots, X_n)$ due diversi stimatori, entrambi corretti, per un unico
parametro $\theta$. Diremo che lo stimatore $H_{1, n}$ è più efficiente di $H_{2, n}$ se vale $V[H_{1, n}] \leq V[H_{2, n}]$ per ogni
numerosità del campione $n$ e per ogni effettivo valore del parametro $\theta$ da stimare.

Per esempio, consideriamo i seguenti stimatori della media $\mu$ di una popolazione:
$$H_{1, n} = \bar{X}_n = \frac{X_1 + \dots + X_n}{n} \quad \quad \quad H_{2, n} = \frac{X_1}{2} + \frac{X_2 + \dots + X_n}{2(n-1)}$$
Il secondo stimatore (definibile solo per $n$ maggiore di 1) è in pratica una variazione del primo e dà maggior 
importanza alla prima componente del campione a cui è assegnato un peso di $\frac{1}{2}$ anziché $\frac{1}{2(n-1)}$
come avviene per le restanti componenti.

Come mostrato nell'esempio precedente, lo stiamtore $H_{1, n}$ è corretto e lo stesso vale anche per $H_{2, n}$, infatti:
$$E[H_{2, n}] = E\left[\frac{X_1}{2}\right] + E \left[\frac{X_2 + \dots + X_n}{2(n-1)}\right] = \frac{1}{2} E[X_1] + \frac{1}{2(n-1)} E[X_2 + \dots + X_n]
= \left[\frac{1}{2} + \frac{n - 1}{2(n-1)}\right] E[X] = E[X] = \mu$$
Osserviamo poi che vale
$$V[H_{2, n}] = V \left[\frac{X_1}{2} \right] + V \left[\frac{X_2 + \dots + X_n}{2(n-1)}\right] = \frac{1}{4}V[X_1] + \frac{1}{4(n-1)^2}
V[X_2 + \dots + X_n] = \frac{1}{4}V[X] + \frac{n-1}{4(n-1)^2} V[X] = \left[\frac{1}{4} + \frac{1}{4(n-1)}\right] \sigma^2$$

Ricordando che $V[H_{1, n}] = \frac{\sigma^2}{n}$, con pochi calcoli non è difficile verificare che $V[H_{1, n}] \leq V[H_{2, n}]$ per
ogni $n$ maggiore o uguale a 2. Pertanto lo stimatore $H_{1, n}$ è più efficiente di $H_{2, n}$ e per questa ragione è preferibile.

Lo stimatore $H_{2, n}$ non è neanche consistente, infatti risulta
$V[H_{2, n}]= \left[\frac{1}{4} + \frac{1}{4(n-1)}\right] \sigma^2 \geq \frac{1}{4} \sigma^2 > 0$, e quindi la sua varianza
non può tendere a 0 per $n$ che tende ad infinito.

In particolare, uno stimatore di un parametro $\theta$ è detto /miglior stimatore/ se è più efficiente di ogni altro stimatore
corretto e consistente.

Abbiamo già visto che la media e la varianza campionaria corretta $\bar{X}_n$ e $\hat{S}_n^2$ sono 2 stimatori corretti e
consistenti dei parametri $\mu$ e $\sigma^2$. Per questi 2 stimatori in realtà è possibile dimostrare qualche cosa in più, ovvero
che essi sono i /migliori stimatori/ per media e varianza della popolazione.

Per esempio, riprendiamo in considerazione la seguente tabella:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-26 22:01:26
[[file:Lezioni/screenshot_2018-04-26_22-01-26.png]]

Se pensiamo a questi 80 costi come alla realizzazione di un campione $(X_1, \dots, X_{80})$ estratto dalla popolazione
$X := \text{"costo al metro quadro degli appartamenti del quartiere"}$, possiamo fornire una stima della media e della
varianza di $X$ per mezzo della media e della varianza campionaria corretta, le cui realizzazioni risultano
\begin{gather*}
\bar{x}_{80} = \frac{x_1 + \dots + x_{80}}{80} = 2.68\\
\hat{s}_{80}^2 = \frac{1}{79} \sum_{i=1}^{80} (x_i -\bar{x}_{80}) = 1.49
\end{gather*}
I due valori ottenuti sono le stime dei 2 parametri.
*** Stime intervallari
Abbiamo visto come trovare un valore approssimato di un parametro incognito della popolazione per mezzo di una stima puntuale.
Tali stime però non forniscono informazioni sul grado di approssimazione delle stesse. Per questo motivo alle stime
puntuali vengono preferite, quando possibile determinarle, le stime intervallari che sono stime espresse sotto forma di
intervalli (/intervalli fiduciari/) all'interno dei quali, con buona probabilità, si trova il valore vero del parametro da
stimare.

Pensiamo ad uno stimatore puntuale $H_n$ di un parametro $\theta$ e sia $\hat{\theta} = h_n (x_1, \dots, x_n)$ una sua realizzazione.

Molto difficilmente il valore del parametro $\theta$ di cui $H_n$ è stimatore corrisponderà alla realizzazione sopra riportata.
Pensiamo allora ad un intervallo $I = [\hat{\theta} - e_1, \hat{\theta} + e_2] \subseteq \mathbb{R}$. Un intervallo di questo tipo
conterrà il reale valore di $\theta$ con maggiore o minore probabilità a seconda della sua ampiezza, vale a dire a seconda dei
valori di $e_1$ e di $e_2$.

Quando sia nota la distribuzione campionaria dello stimatore $H_n$ è possibile calcolare esattamente tali probabilità al variare
di $e_1$ e di $e_2$. In altre parole è possibile, dato $\alpha \in [0, 1] \subseteq \mathbb{R}$, determinare dei corrispondenti valori
di $e_1$ e di $e_2$ tali che risulti $P(\theta \in [\hat{\theta} - e_1, \hat{\theta} + e_2]) = 1 - \alpha$, dove $\hat{\theta}$ è una realizzazione
dello stimatore $H_n$.

In questo caso il valore $\alpha$ è detto /livello di confidenza/ della stima ed il corrispondente intervallo è detto
/intervallo di confidenza/.

I valori solitamente utilizzati come livello di fiducia (confidenza) $\alpha$ sono 0.1, 0.05 e 0.01.

Vedremo qui come si costruiscono degli intervalli di confidenza per la media di una popolazione. Nel farlo distingueremo
tra il caso in cui la popolazione non sia normalmente distribuita ed il caso in cui la popolazione sia normalmente
distribuita.

Suddividiamo il problema della determinazione di un intervallo di confidenza per il valor medio $\mu$ in 4 sottocasi a
seconda che la popolazione sia o non sia normalmente distribuita e che la varianza sia o non sia nota.

**** /Popolazione non normalmente distribuita e varianza $\sigma^2$ nota/

In questo caso la media campionaria $\bar{X}_n = \frac{X_1 + \dots + X_n}{n}$ è approssimabile per $n \geq 30$, tramite una variabile
con distribuzione normale di media $\mu$ e deviazione standard $\frac{\sigma}{\sqrt{n}}$.

Pensiamo allora a $\bar{X}_n \sim N \left(\mu, \frac{\sigma}{\sqrt{n}}\right)$, dove l'unico parametro incognito è $\mu$.

Normalizzando abbiamo quanto segue: $Z = \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0, 1)$.

I valori assunti dalla variabile $Z$ così definita dipenderanno ovviamente dalle realizzazioni di $\bar{X}_n$ e
quindi dalle realizzazioni del campione $(X_1, \dots, X_n)$.

Nonostante ciò, in base alla relazione $Z = \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0, 1)$, siamo in grado di
determinare un valore $z_{1 - \frac{\alpha}{2}}$ per cui valga la seguente relazione
$P\left(Z \in \left[-z_{1 - \frac{\alpha}{2}} , z_{1-\frac{\alpha}{2}}\right]\right) = 1 - \alpha$
e quindi anche
$P \left(\mu - z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \leq \bar{X}_n \leq \mu + z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right) = 1 - \alpha$.

Pertanto, in base all'ultima relazione sappiamo che con probabilità $1 - \alpha$ lo stimatore $\bar{X}_n$ assume valori in
un ben definito intervallo, ma ancora dipendente da $\mu$. A questo punto è possibile scrivere:
$$P \left(\bar{X}_n - z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}_n + z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right) = 1 - \alpha$$

Pertanto, in base all'ultima relazione, possiamo dire che data una realizzazione $\bar{x}_n$ di $\bar{X}_n$, il
parametro incognito $\mu$ è compreso nell'intervallo
$P \left(\bar{x}_n - z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}, \bar{x}_n + z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right)$
con probabilità $(1 - \alpha)$. Pertanto questo è proprio l'intervallo di confidenza cercato.

Occorre fare ancora 2 considerazioni.

La prima è che quello così ricavato non è l'unico intervallo di confidenza
ottenibile per il livello di fiducia $\alpha$. Esso è quello chiamato /simmetrico/ in cui i 2 estremi sono pensati alla stessa distanza
dalla realizzazione $\bar{x}_n$. Avremmo però potuto scegliere anche diversi valori di $z_1$ e $z_2$ per cui
$P(Z \in [-z_1, +z_2]) = 1 - \alpha$. Da questa espressione, ragionando come sopra, saremmo poi arrivati ad ottenere un intervallo di
fiducia alternativo, ovvero:
$P \left(\bar{x}_n - z_1 \frac{\sigma}{\sqrt{n}}, \bar{x}_n + z_2 \frac{\sigma}{\sqrt{n}}\right)$

Una seconda considerazione è che occorre sempre ricordare che questo metodo è applicabile quando la numerosità del campione è
sufficientemente grande. In generale lo si applica quando $n$ è maggiore o uguale a 30, anche se è ammissibile applicarlo
anche per valori più piccoli ($n = 10$) nel caso in cui si sappia che la distribuzione della popolazione non si discosta molto da
una normale o almeno è simmetrica rispetto al suo valor medio.

**** /Popolazione non normalmente distribuita e varianza $\sigma^2$ non nota/

In questo caso si ragiona analogamente a quanto sopra, andando però a sostituire al valore $\sigma$ la sua stima $\hat{s}_n$
ottenuta come realizzazione della statistica $\hat{S}_n = \sqrt{\hat{S}_n^2}$, dove $\hat{S}_n^2$ è la varianza campionaria
corretta definita precedentemente. Si può mostrare infatti che per $n$ sufficientemente grande, anche la variabile
$Z = \frac{\bar{X}_n - \mu}{\frac{\hat{S}_n}{\sqrt{n}}}$ risulta essere approssimativamente una normale standardizzata.

Pertanto per $n \geq 30$, quando non è nota la varianza della popolazione, è possibile definire un intervallo di confidenza
per la media $\mu$:
$$P \left(\bar{X}_n - z_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}} \leq \mu \leq \bar{X}_n + z_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}}\right) = 1 - \alpha$$

Per esempio, riprendiamo nuovamente in considerazione la tabella

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-26 22:01:26
[[file:Lezioni/screenshot_2018-04-26_22-01-26.png]]

e supponiamo di voler determinare un intervallo di confidenza per la media della popolazione richiedendo un livello di fiducia
$\alpha = 0.05$. Poiché nel nostro caso la varianza della popolazione non è nota, avremo
$$\left[\bar{X}_{80} - z_{1-\frac{0.05}{2}} \frac{\hat{S}_{80}}{\sqrt{80}} \leq \mu \leq \bar{X}_{80} + z_{1-\frac{0.05}{2}}
\frac{\hat{S}_{80}}{\sqrt{80}}\right] \subseteq \mathbb{R}$$
L'intervallo ottenuto è $I = [2.41, 2.94]$ e con probabilità $(1 - 0.05) = 0.95$ contiene il valore effettivo della media $\mu$.

Per lo stesso livello di fiducia avremmo potuto scegliere un diverso intervallo di confidenza. Supponiamo ad esempio di
essere interessati a determinare una limitazione superiore per il parametro $\mu$ e sempre con livello di fiducia $\alpha = 0.05$.
In questo caso occorre determinare il valore $z_{1-\alpha}$ tale che $P(Z \in [-\infty, +z_{1-\alpha}]) = 1 - \alpha$.

A questo punto osserviamo che dall'uguaglianza precedente si ottiene:
\begin{align*}
&P \left(\bar{X}_{80} - \infty \frac{\hat{S}_{80}}{\sqrt{80}} \leq \mu \leq \hat{X}_{80} + z_{1 - \alpha} \frac{\hat{S}_{80}}{\sqrt{80}}\right) =\\
&= P\left(-\infty \leq \mu \leq \hat{X}_{80} + z_{1 - \alpha} \frac{\hat{S}_{80}}{\sqrt{80}}\right) =\\
&= 1 - \alpha
\end{align*}

Il valore di $z_{1-\alpha}$ si ricava facilmente dalle tavole della distribuzione normale come il quantile che lascia alla sua
destra un'area uguale a 0.05 e risulta $z_{1 - \alpha} = 1.65$ per cui l'intervallo desiderato è $(- \infty, 2.90)$. In definitiva
è possibile affermare che il parametro $\mu$ è minore di 2.90 con probabilità 0.95.

**** /Popolazione normalmente distribuita e varianza $\sigma^2$ nota/

In questo caso sappiamo che la media campionaria $\bar{X}_n = \frac{X_1 + \dots + X_n}{n}$ è ancora una variabile con distribuzione
normale di media $\mu$ e deviazione standard $\frac{\sigma}{\sqrt{n}}$.

Possiamo ragionare esattamente come per il caso di popolazione non normalmente distribuita e varianza nota con l'unica differenza
che ora non viene fatta alcuna richiesta sulla numerosità del campione. Si può pertanto continuare ad utilizzare la formula:
$$P \left(\bar{X}_n - z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}_n + z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right) = 1 - \alpha$$
**** /Popolazione normalmente distribuita e varianza $\sigma^2$ non nota/
La variabile di cui si fa uso in questo caso è $T_n = \frac{\bar{X}_n - \mu}{\frac{\hat{S}_n}{\sqrt{n}}}$,
che per quanto detto in precedenza è distribuita secondo una t di Student con $(n-1)$ gradi di libertà.

Facendo uso delle tavole della t di Student è possibile determinare un valore $t_{1-\frac{\alpha}{2}}$ per cui valga
$P \left(T_n \in \left[-t_{1-\frac{\alpha}{2}, n-1}, t_{1-\frac{\alpha}{2}, n-1}\right]\right) = 1 - \alpha$
e quindi anche
$P\left(-t_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}} \leq \bar{X}_n - \mu \leq t_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}}\right) = 1 - \alpha$

A questo punto la precedente diseguaglianza può essere riscritta come
$$P\left(\bar{X}_n-t_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}} \leq \mu \leq \bar{X}_n + t_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}}\right) = 1 - \alpha$$

Ciò significa che date le realizzazioni $\bar{x}_n$ di $\bar{X}_n$ ed $\hat{s}_n$ di $\hat{S}_n$, il parametro incognito $\mu$
è compreso nell'intervallo
$$\left[\bar{x}_n - t_{1-\frac{\alpha}{2}} \frac{\hat{s}_n}{\sqrt{n}}, \bar{x}_n + t_{1-\frac{\alpha}{2}} \frac{\hat{s}_n}{\sqrt{n}}\right]$$
con probabilità $(1 - \alpha)$.
Pertanto $\left(\bar{x}_n - t_{1-\frac{\alpha}{2}} \frac{\hat{s}_n}{\sqrt{n}}, \bar{x}_n + t_{1-\frac{\alpha}{2}} \frac{\hat{s}_n}{\sqrt{n}}\right)$
è l'intervallo di confidenza cercato.

Anche in questo caso è necessario precisare che questo intervallo non è l'unico ricavabile per il dato livello di fiducia.

Questo intervallo è /simmetrico/, ovvero i 2 estremi sono pensati alla stessa distanza dalla realizzazione $\bar{x}_n$.
Avremmo però potuto scegliere anche valori diversi di $t_1$ e $t_2$ per cui $P(T_n \in [-t_1, t_2]) = 1 - \alpha$ da cui si ottiene
un intervallo di fiducia alternativo $\left[\bar{x}_n - t_1 \frac{\hat{s}_n}{\sqrt{n}}, \bar{x}_n + t_2 \frac{\hat{s}_n}{\sqrt{n}}\right]$
*** Intervalli di Confidenza per la Varianza
Vediamo ora come determinare un intervallo di confidenza per la varianza $\sigma^2$ della popolazione. Purtroppo non possiamo dire
nulla quando la popolazione non è normalmente distribuita. È vero che la distribuzione campionaria della statistica $S_n^2$
per $n$ sufficientemente grande può essere approssimata da una normale. Purtroppo però l'espressione
$V[S_n^2] = \frac{1}{n} \left(E[(X - \mu)^4] - \frac{n-3}{n-1} \cdot \sigma^4 \right)$ della varianza di tale statistica non consente di far
uso di questa proprietà.

Possiamo invece definire un intervallo fiduciario per $\sigma^2$ quando la popolazione da cui è stato estratto il campione è
normalmente distribuita facendo uso del fatto che in tale caso la variabile $Q_n = \frac{(n-1) \cdot \hat{S}_n^2}{\sigma^2}$ è
distribuita secondo una Chi-Quadro con $(n-1)$ gradi di libertà.

Utilizzando le tavole della distribuzione Chi-Quadro, è possibile determinare almeno 2 valori $q_1$ e $q_2$ per i quali vale
$P(Q_n \in [q_1, q_2]) = 1 - \alpha$ e quindi $P\left(q_1 \leq \frac{(n-1) \cdot \hat{S}_n^2}{\sigma^2} \leq q_2 \right) = 1 - \alpha$.

Risolvendo la diseguaglianza rispetto al parametro incognito $\sigma^2$:
$$P \left(\frac{(n-1) \cdot \hat{S}_n^2}{q_2} \leq \sigma^2 \leq \frac{(n-1) \cdot \hat{S}_n^2}{q_1}\right) = 1 - \alpha$$
Ciò significa che date le realizzazioni $\hat{s}_n^2$ di $\hat{S}_n^2$, il parametro incognito $\sigma^2$ è compreso nell'intervallo
$$\left[\frac{(n-1) \cdot \hat{s}_n^2}{q_2}, \frac{(n-1) \cdot \hat{s}_n^2}{q_1}\right]$$
con probabilità $(1 - \alpha)$ e pertanto questo è l'intervallo di confidenza cercato.

Anche in questo caso occorre sengnalare che quello definito tramite
$$P \left(\frac{(n-1) \cdot \hat{S}_n^2}{q_2} \leq \sigma^2 \leq \frac{(n-1) \cdot \hat{S}_n^2}{q_1}\right) = 1 - \alpha$$
non è l'unico intervallo di confidenza ottenibile per il livello $\alpha$ in quanto è possibile scegliere più coppie di valori
$q_1$ e $q_2$. Solitamente viene utilizzata la coppia $q_{\frac{\alpha}{2}} \quad q_{1-\frac{\alpha}{2}}$, costituita dai 2 quantili
$\frac{\alpha}{2}$ e $1 - \frac{\alpha}{2}$ della distribuzione Chi-Quadro con $(n-1)$ gradi di libertà.

Per esempio, riprendiamo in considerazione i carichi a rottura delle travi di cemento dati dalla seguente tabella:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-27 17:36:57
[[file:Lezioni/screenshot_2018-04-27_17-36-57.png]]

E sotto l'ipotesi di distribuzione normale si voglia determinare un intervallo di confidenza per la varianza con livello
di fiducia $\alpha = 0.01$. Occorre allora far ricorso alla seguente formula:
$$P \left(\frac{(n-1) \cdot \hat{S}_n^2}{q_2} \leq \sigma^2 \leq \frac{(n-1) \cdot \hat{S}_n^2}{q_1}\right) = 1 - \alpha$$
scegliendo 2 opportuni valori $q_1$ e $q_2$.

Potremmo prendere ad esempio i 2 quantili della Chi-Quadro con 14 gradi di libertà che lasciano rispettivamente un'area
uguale a 0.005 alla loro sinistra ed alla loro destra. Consultando le tavole della Chi-Quadro si vede che i 2 quantili
cercati sono $q_{\frac{0.01}{2}} = 4.07 \quad q_{1-\frac{0.01}{2}} = 31.3$.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-27 17:42:20
[[file:Lezioni/screenshot_2018-04-27_17-42-20.png]]

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-27 17:41:35
[[file:Lezioni/screenshot_2018-04-27_17-41-35.png]]

Inoltre la realizzazione $\hat{s}_{15}^2$ di $\hat{S}_{15}^2$ è $\hat{s}_{15}^2 = 48266$.

L'intervallo di confidenza cercato è $I = \left[\frac{14 \cdot 48266}{31.3}, \frac{14 \cdot 48266}{4.07}\right] = [21588, 166025]$.

Ancora una volta avremmo potuto scegliere un intervallo diverso per lo stesso livello di confidenza, ad esempio prendendo
i quantili $q_1 = 0$ e $q_2 = 29.1$, ottenendo $P(q_1 \leq Q_{15} \leq q_2) = 0.99$ quando $Q_{15} \sim \chi_{14}^2$. Ragionando come sopra si ricava
l'intervallo fiduciario $I = \left[\frac{14 \cdot \hat{s}_{15}^2}{q_2}, \frac{14 \cdot \hat{s}_{15}^2}{q_1}\right] = [23220, \infty]$.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-27 17:47:55
[[file:Lezioni/screenshot_2018-04-27_17-47-55.png]]
*** Considerazioni sulla Numerosità del Campione
In base alle formule:
$$P\left(\bar{X}_n-z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}_n + z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}\right) = 1 - \alpha$$
$$P\left(\bar{X}_n-t_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}} \leq \mu \leq \bar{X}_n + t_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}}\right) = 1 - \alpha$$
appare evidente che mantenendo fisso il livello di confidenza $\alpha$, l'ampiezza dell'intervallo si riduce al crescere di $n$.

Questo accade anche nel caso della stima di $\sigma^2$ per popolazioni normalmente distribuite anche se non è evidente dalla formula
$$P \left(\frac{(n-1) \cdot \hat{S}_n^2}{q_2} \leq \sigma^2 \leq \frac{(n-1) \cdot \hat{S}_n^2}{q_1}\right) = 1 - \alpha$$
poiché il restringimento dell'intervallo di fiducia segue dalla variazione dei gradi di libertà della Chi-Quadro con cui è
distribuita la statistica $Q_n$.

Volendo, nella definizione di un intervallo di confidenza per il valor medio è possibile decidere in anticipo l'ampiezza
dell'intervallo e quindi fissare di conseguenza la numerosità del campione come mostrato nell'esempio seguente.

Per esempio, riprendiamo in considerazione il problema nel quale si sono determinati 2 intervalli di confidenza per la
media $\mu$ del costo al metro quadro delgli appartamenti di un dato quartiere di una città italiana.

Si voglia determinare quanto deve essere grande il campione affinché l'intervallo di confidenza simmetrico abbia ampiezza non
superiore a 0.3 mantenendo sempre un livello di confidenza $\alpha = 0.05$.

In base alla formula
$$P\left(\bar{X}_n-t_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}} \leq \mu \leq \bar{X}_n + t_{1-\frac{\alpha}{2}} \frac{\hat{S}_n}{\sqrt{n}}\right) = 1 - \alpha$$
è immediato osservare che l'ampiezza di un intervallo di confidenza simmetrico per $\alpha=0.05$ risulta essere uguale a:
$$ 2 \cdot z_{1 - \frac{\alpha}{2}} \cdot \frac{\hat{S}_n}{\sqrt{n}} = 2 \cdot 1.96 \cdot \frac{\hat{S}_n}{\sqrt{n}} = 3.92 \cdot \frac{\hat{S}_n}{\sqrt{n}}$$
Poiché la deviazione standard non è nota, possiamo sostituirla con una stima e la più indicata è ovviamente:
$\hat{s}_n = \hat{s}_{80} = 1.22$.

Pertanto l'ampiezza risulta essere uguale a
$$\frac{3.92 \cdot 1.22}{\sqrt{n}} = \frac{4.783}{\sqrt{n}}$$
Notiamo che questa diminuisce con l'aumentare della numerosità del campione, pertanto, per avere l'ampiezza minore di 0.3,
occorrerà avere $n$ sufficientemente grande.
$$ \frac{4.783}{\sqrt{n}} \leq 0.3 \iff \frac{4.783}{0.3} \leq \sqrt{n} \iff 15.94 \leq \sqrt{n} \iff 254.13 \leq n$$
In particolare risulta un'ampiezza poco minore a 0.3 quando $n=255$.
*** Stime di parametri
Per effettuare stime puntuali di parametri esistono altri metodi oltre a quelli appena presentati. I due più noti
vengono di seguito descritti.

Il primo prende il nome di /metodo dei momenti/ e si applica nel caso in cui sia nota l'espressione funzionale della densità
di probabilità della popolazione a meno di uno o più parametri incogniti. Supponiamo quindi che la popolazione considerata
$X$ abbia una densità di probabilità $f_X(t)$ dipendente da $m$ parametri incogniti $\theta_1, \dots, \theta_m$.

Consideriamo ora i primi $m$ momenti centrali della variabile $X$: $\mu_k = E[X^k] \ k = 1, \dots, m$. I valori da essi assunti sono
delle funzioni dei parametri, ovvero: $\mu_k = \mu_k (\theta_1, \dots, \theta_m)$ per ogni $k = 1, \dots, m$. Consideriamo poi i corrispondenti primi
$m$ momenti campionari definiti come $\bar{X}_n^k = \frac{1}{n} \sum_{i=1}^n X_i^k$ con $k = 1, \dots, m$ e dove $(X_1, \dots, X_n)$ è il
campione estratto dalla popolazione.

Denotiamo infine con $\bar{x}_n^k$ le realizzazioni (note) dei momenti campionari $\bar{X}_n^k$. Le stime dei parametri
$\theta_1, \dots, \theta_m$ si ottengono risolvendo rispetto ad esse il sistema in $m$ equazioni ed $m$ incognite:
\begin{equation*}
\begin{cases}
\mu_1 (\theta_1, &\dots, \theta_m) = \bar{x}_n^1\\
&\vdots\\
\mu_m (\theta_1, &\dots, \theta_m) = \bar{x}_n^{m}
\end{cases}
\end{equation*}

Per esempio, sia $X$ una variabile che descrive il tempo necessario ad ottenere la concessione per la ristrutturazione di
un immobile in una determinata provincia italiana, in mesi, e si voglia stimare la sua distribuzione in base ad un
campione di numerosità 10 la cui realizzazione è $(x_1, \dots, x_{10}) = (3, 4.1, 2.8, 5.5, 1.5, 2.2, 6, 1.2, 3.2, 0.9)$.

Supponiamo poi sia ragionevole pensare che la distribuzione di $X$ sia una Gamma e di volerne stimare i parametri
$\alpha$ e $\lambda$ con il metodo dei momenti.

Una variabile aleatoria $X$ è distribuita secondo una Gamma di parametri $\alpha$ e $\lambda$, con $\alpha$ e $\lambda \in \mathbb{R}_+$:
\begin{equation*}
f_X(t) =
\begin{cases}
0 &\text{se $t < 0$}\\
\lambda e^{-\lambda t} \frac{(\lambda t)^{\alpha - 1}}{\Gamma (\alpha)} &\text{se $t \geq 0$}
\end{cases}
\end{equation*}
Media e varianza sono date da $E[X] = \frac{\alpha}{\lambda}$ e $V[X] = \frac{\alpha}{\lambda^2}$.

Essendo 2 i parametri da stimare, occorre allora calcolare i primi 2 momenti campionari:
$$\bar{x}_{10}^{1} = \frac{1}{10} \cdot \sum_{i=1}^{10} x_i = 3.04$$
$$\bar{x}_{10}^{2} = \frac{1}{10} \cdot \sum_{i=1}^{10} x_i^2 = 11.95$$
ed i corrispondenti momenti teorici della distribuzione:
$$E[X] = \frac{\alpha}{\lambda}$$
$$E[X^2] = V[X] + (E[X])^2 = \frac{\alpha}{\lambda^2} + \frac{\alpha^2}{\lambda^2} = \frac{\alpha (\alpha +1 )}{\lambda^2}$$

Le stime dei parametri si ottengono risolvendo il sistema:
\begin{equation*}
\begin{cases}
E[X] = \frac{\alpha}{\lambda} &= 3.04 \quad &\implies x_{10}^{-1} = 3.04\\
E[X^2] = \frac{\alpha (\alpha + 1)}{\lambda^2} &= 11.95 \quad &\implies x_{10}^{-2} = 11.95
\end{cases}
\end{equation*}
che fornisce la soluzione $\lambda = 1.123$ e $\alpha = 3.415$.

Le stime ottenute con questo metodo che è storicamente il più antico non sono in genere particolarmente efficienti,
ma grazie alla semplicità con cui possono essere ottenute sono utili soprattutto nel fornire delle prime approssimazioni.

Un secondo metodo, decisamente più valido dal punto di vista matematico, è quello che porta il nome di
/metodo della massima verosimiglianza/. Anche questo metodo è applicabile quando sia nota l'espressione funzionale
della densità di probabilità della popolazione a meno di uno o più parametri incogniti che si vogliono stimare.

Per semplicità, supponiamo questa volta che la popolazione considerata $X$ abbia una densità di probabilità $f_X(t)$
dipendente da un singolo parametro $\theta$ incognito.

Consideriamo la funzione di densità congiunta di un campione $(X_1, \dots, X_n)$ di numerosità $n$. Per via dell'indipendenza
tra le variabili $X_i$ abbiamo $f_{X_1, \dots, X_n} (t_1, \dots, t_n) = f_X(t_1) \cdot \dots \cdot f_X(t_n)$.

Ricordando che l'espressione di $f_X$ è nota a meno del parametro $\theta$, osserviamo che tale densità congiunta può essere
riscritta come $f_{X_1, \dots, X_n} (t_1, \dots, t_n) = L(t_1, \dots, t_n, \hat{\theta})$.

La funzione $L$ prende il nome di /funzione di verosimiglianza/ di dimensione $n$. Sia ora $(x_1, \dots, x_n)$ la realizzazione
del campione $(X_1, \dots, X_n)$. Notiamo che al variare del valore $\hat{\theta}$ assegnato al parametro $\theta$, varia anche la
quantità $L(x_1, \dots, x_n, \hat{\theta})$ che possiamo pensare come la probabilità che il campione assuma valore $(x_1, \dots, x_n)$
quando il parametro assume valore $\hat{\theta}$.

Ovviamente il valore $\hat{\theta}$ è tanto più indicato ad essere l'effettivo valore del parametro $\theta$ quanto più è alta
tale probabilità, cioè quanto più esso è verosimile.

È detta quindi /stima di massima verosimiglianza/ per il parametro $\theta$ quel valore $\hat{\theta}$ che massimizza la funzione
di verosimiglianza $L(x_1, \dots, x_n, \hat{\theta})$.

Si può dimostrare che le stime così ottenute sono sempre consistenti e nella maggior parte dei casi sono anche corrette
e con la massima efficienza.

Per esempio, si consideri lo stesso problema dell'esempio precedente supponendo però questa volta di ritenere ragionevole
pensare che la variabile $X$ sia distribuita secondo un'esponenziale il cui parametro $\lambda$ è da stimare.

Data quindi una realizzazione $(x_1, \dots, x_{10}) = (3, 4.1, 2.8, 5.5, 1.5, 2.2, 6, 1.2, 3.2, 0.9)$ del campione,
la funzione di verosimiglianza risulta:
$$L(x_1, \dots, x_n, \lambda) = \prod_{i=1}^{10} \lambda \cdot e^{-\lambda x_i} = \lambda^{10} \cdot e^{-\lambda \cdot \sum_{i=1}^{10} x_i} = \lambda^{10} \cdot e^{-30.4 \lambda}$$
La stima di massima verosimiglianza del parametro si ricava andando a determinare il valore di $\lambda$ per cui risulta
massima la funzione $L(x_1, \dots, x_n, \lambda)$. Calcolandone la derivata
$$L'(x_1, \dots, x_n, \lambda) = 10 \cdot \lambda^9 e^{-30.4 \cdot \lambda} - 30.4 \cdot \lambda^{10} e^{-30.4 \cdot \lambda} = \lambda^9 e^{-30.4 \cdot \lambda} \cdot (10 - 30.4 \lambda)$$
È facile ora osservare che il massimo si ottiene per quel valore di $\lambda$ per cui $(10 - 30.4 \lambda) = 0$.

Pertanto per $\lambda = \frac{10}{30.4} = 0.329$.

Si osservi che in realtà, in questo particolare caso, la stima appena ricavata corrisponde a quella che avremmo ottenuto
stimando il valore atteso $\mu$ con la media campionaria $\bar{x}_{10} = 3.04$, e quindi ricordando che per una variabile
esponenziale vale:
$$\lambda = \frac{1}{E[X]} = \frac{1}{\lambda} \implies \lambda = \frac{1}{3.04} = 0.329$$
Tale osservazione non può comunque essere generalizzata a tutte le stime ottenute con il metodo della massima
verosimiglianza.
** Verifica di ipotesi: test parametrici
Un'affermazione relativa ad una caratteristica di una popolazione è detta /ipotesi statistica/ quando essa viene formulata sulla base
dell'esperienza o sulla base di considerazioni teoriche. Il problema della /verifica di ipotesi/ consiste nella verifica, da effettuarsi
tramite l'analisi di informazioni campionarie, della validità di un'ipotesi statistica. Per esempio affermare in base alla propria
esperienza che mediamente in una data città il costo degli appartamenti è di 3,000 euro al metro quadro o che tale costo è indipendente
dal quartiere in cui è situato l'appartamento vuol dire formulare delle ipotesi che devono essere controllate tramite un'analisi prima di
essere accettate come veritiere.

Per effettuare tali verifiche si utilizzano procedure statistiche dette: /test di ipotesi/. esse di suddividono in:
- /Test parametrici/ che si riferiscono ad ipotesi relative ai parametri della distribuzione della popolazione (media o varianza);
- /Test non parametrici/ che riguardano il tipo di distribuzione ipotizzabile per la popolazione o altre caratteristiche non
  esprimibili come parametri.

*** Caratteristiche generali di un test di ipotesi
Sia $X$ una popolazione su cui vogliamo effettuare un test per confermare (o rifiutare) una particolare ipotesi che denotiamo con $H_0$.
Sia poi $T$ una statistica campionaria la cui distribuzione è nota quando l'ipotesi $H_0$ da controllare è vera, i test di ipotesi si
basano sull'osservazione delle realizzazioni di statistiche campionarie di questo tipo. Infatti, essendo nota la distribuzione di $T$
quando $H_0$ è vera abbiamo un'idea dei valori che essa tende ad assumere in questo caso. Saremo quindi portati ad /accettare l'ipotesi
\(H_0\)/, o meglio a /non rifiutarla/ quando il valore assunto da $T$ si trova in un sottoinsieme di valori altamente probabili tra
quelli da essa assumibili quando $H_0$ è vera e a rifiutarla in caso contrario.

Più specificamente ogni test di ipotesi è caratterizzato da:
- /Una popolazione statistica $X$/ sulla quale viene effettuato il test;
- /Un'ipotesi nulla $H_0$/: ipotesi da convalidare (o rifiutare) sulla base dei valori assunti da un campione $(X_1, \dots, X_n)$ estratto da $X$;
- /Un'ipotesi alternativa $H_1$/: ovvero un'ipotesi da considerare valida quando si rifiuta l'ipotesi nulla;
- /Una statistica campionaria $T = T(X_1, \dots, X_n)$/ di cui è nota la distribuzione quando l'ipotesi nulla è vera;
- /Una regione di accettazione/, denotata con $\bar{C}$ che è l'insieme di valori assumibili dalla statistica $T$ che portano ad
  un'accettazione dell'ipotesi nulla;
- /Una regione critica/, denotata con $C$ che è l'insieme di valori assumibili dalla statistica $T$ che portano ad un rifiuto dell'ipotesi nulla;
- /Un livello di significatività $\alpha$/, permette di individuare la regione di accettazione ed è tale che quando l'ipotesi nulla è vera
  allora la statistica $T$ assume valori nella regione critica con probabilità $\alpha$.

Per esempio, si supponga di sapere che il reddito medio annuo per famiglia di un determinato quartiere di una città italiana sia distribuito
secondo una normale il cui parametro $\sigma^2$ è noto ed è di 144K euro mentre il parametro $\mu$ è incognito. In base alla sua esperienza
personale, un nostro collaboratore sostiene che la media è uguale a 30K euro. Si supponga di voler controllare tale affermazione con
un'indagine campionaria e per tale ragione intervistare 10 famiglie del quartiere scelte a caso. Si supponga infine di accettare
l'affermazione del collaboratore se la media aritmetica dei redditi annui delle famiglie è compresa tra 25K e 35K euro e di rifiutarla in
caso contrario, decidendo in tal caso di effettuare un'indagine campionaria più approfondita.

Vediamo allora quali sono in questo caso gli oggetti sopra elencati che caratterizzano il test. La /popolazione $X$/ è costituita dai redditi
annui dei nuclei familiari del quartiere, mentre /l'ipotesi nulla/ è $H_0 : \mu = 30K €$ e /l'ipotesi alternativa/ è $H_1 : \mu \neq 30K €$.
La /statistica campionaria $T$/ è la media aritmetica $\bar{X}_{10} = \frac{X_1 + \dots + X_{10}}{10}$ dove $(X_1, \dots, X_{10})$ è un /campione casuale/ di
numerosità 10 estratto da $X$. Poiché accettiamo $H_0$ quando $\bar{X}_{10}$ è compresa tra 25K e 35K euro allora la /regione di accettazione/
è $\bar{C} = [25, 35] \subseteq \mathbb{R}$, mentre la /regione critica/ $C = (-\infty, 25) \cup (35, \infty) \subseteq \mathbb{R}$.

Per determinare il /livello di significatività/ occorre ricordare qual è la distribuzione campionaria di $\bar{X}_{10}$.
A tale proposito sappiamo che se la popolazione $X$ è normalmente distribuita con parametri $\mu$ e $\sigma$ allora anche la media
campionaria $\bar{X}_{10}$ risulta essere normalmente distribuita con parametri $\mu$ e $\frac{\sigma}{\sqrt{10}}$.

Nel caso in cui sia vera $H_0$, allora $\bar{X}_{10} \sim N(30, 3.79)$ e la probabilità che $\bar{X}_{10}$ cada nella regione critica risulta
$$P(\bar{X} \in C) = 1 - P(\bar{X}_{10} \in [25, 35]) = 1 - P\left(\frac{\bar{X} - 30}{3.79} \in [-1.32, +1.32]\right) = 0.18$$

Diverse considerazioni si potrebbero fare in merito all'esempio appena proposto. Una di queste segue dall'osservazione che esiste
una probabilità non nulla che la statistica $\bar{X}_{10}$ cada nella regione critica anche quando $H_0$ è vera: $P(\bar{X}_{10} \in C) = 0.18$.
Può quindi capitare che il test porti ad un rifiuto di $H_0$ nonostante essa sia vera. Similmente può accadere che sia vera l'ipotesi
alternativa ma nonostante ciò la statistica $\bar{X}_{10}$ cada nella regione di accettazione $\bar{C}$. In questo caso il test porta
ad accettare $H_0$ quando in realtà essa è falsa.

Occorre allora tener presente che, come nell'esempio, i test di ipotesi non portano mai ad ottenere risposte certe. Tutte le accettazioni
(non rifiuti) di ipotesi (o i loro rifiuti) possono essere sempre soggette ad errore e devono essere considerate valide solo in via
provvisoria per poter essere rimesse in discussione in presenza di nuovi dati. Lo stesso termine utilizzato per indicare l'insieme di
valori che portano a rifiutare $H_0$ sottolinea la presenza del dubbio: infatti la regione è detta "critica" perché è improbabile che
$H_0$ sia vera quando la statistica campionaria assume valori appartenenti ad essa ma non possiamo comunque escluderlo. Inoltre, è
più corretto non dire che un'ipotesi nulla viene accettata quando la statistica test cade nella regione di accetazione, preferendo
affermare invece che essa "non può essere rifiutata". Questo accade proprio perché ciò che si controlla effettuando il test è se
il dato fornito dalla statistica è compatibile con l'ipotesi nulla sottoposta a test.

Per chiarire questo fatto supponiamo di aver accettato un'ipotesi nulla dopo aver svolto un test appropriato e proviamo a variare tale
ipotesi magari di poco (passando da $\mu = 30$ a $\mu = 31$). In molti casi ripetendo il test gli stessi dati porterebbero ad una
nuova accettazione e ci troveremmo ad aver accettato due ipotesi che non sono identiche. È meglio affermare che queste due ipotesi
non possono essere rifiutate sulla base delle nostre osservazioni.

Detto questo, osserviamo che gli errori che si possono commettere nell'effettuazione di un test sono di due tipi:
- Errori di prima specie (I specie): sulla base delle osservazioni campionarie si rifiuta $H_0$ quando è vera;
- Errori di seconda specie (II specie): sulla base delle osservazioni campionarie si accetta $H_0$ quando è falsa.

La probabilità di compiere errori di prima specie è nota e coincide con il livello di significatività $\alpha$ del test, mentre in genere non
è nota la probabilità di compiere errori di seconda specie $\beta$. Tornando all'esempio precedente è possibile osservare che in esso si
è fissata una regione di accettazione per $H_0$ e solo successivamente è stato determinato il livello di significatività $\alpha$, la
probabilità di compiere errori di prima specie. Nella pratica si procede all'inverso, fissando un valore per $\alpha$ e solo successivamente,
in base anche alla dimensione del campione di cui si dispone, la regione critica viene determinata.

La /procedura per la formulazione di un test di ipotesi/ solitamente prevede, nell'ordine:
- L'individuazione dell'ipotesi nulla $H_0$ e dell'ipotesi alternativa $H_1$;
- La scelta della significatività $\alpha$;
- La determinazione della statistica campionaria $T$ da utilizzare nel test;
- La determinazione delle regioni di accettazione e critica;
- L'accettazione o il rifiuto di $H_0$ in base all'osservazione dei dati campionari.

Una terza considerazione che può essere fatta in merito all'esempio è relativa alla scelta delle ipotesi $H_0$ e $H_1$. In effetti si
sarebbe potuti essere interessati a controllare l'ipotesi che il reddito medio annuo delle famiglie fosse superiore a 30K euro o
sufficientemente vicino a 30K o ancora inferiore a 20K euro. Avremmo allora potuto considerare ipotesi nulle diverse a cui sarebbero
state associate regioni critiche diverse.

Un'ipotesi statistica è detta /semplice/ se il sottoinsieme di valori che essa assegna ad un parametro è costituito da un solo elemento,
è detta /composta/ in caso contrario.

L'ipotesi considerata nell'esempio precedente è un caso di ipotesi semplice, mentre sarebbero state ipotesi composte le
$H_0' : \mu \in [25, 35] \quad H_0'' : \mu > 30 \quad H_0''' : \mu = 30 \text{ oppure } 31$.
Infine, volendo controllare un'ipotesi composta del tipo della $H_0''$ non avrebbe avuto senso considerare come regione critica quella
proposta nell'esempio precedente: $C = (-\infty, 25) \cup (35, +\infty) \subseteq \mathbb{R}$. Infatti, sarebbe errato rifiutare l'ipotesi che la
media sia maggiore di 30K quando la statistica campionaria assume valori grandi. Una regione critica appropriata è $C = (-\infty, 25) \subseteq \mathbb{R}$.

Un test statistico è detto /bidirezionale/ se la regione critica è costituita dall'unione di due sottoinsiemi disgiunti mentre diciamo
che un test è /unidirezionale/ se la regione critica è costituita da un solo sottoinsieme. Ovviamente i /test bidirezionali/ si utilizzano
quando si controllano ipotesi quali $H_0 : \mu = 30K € \quad H_0' : \mu \in [25, 35]$, mentre i /test unidirezionali/ vengono utilizzati per controllare
ipotesi quali $H_0'' : \mu > 30$.
*** Considerazioni sugli errori di I e II specie
Limitiamoci per il momento a considerare test statistici in cui l'ipotesi nulla è semplice. Abbiamo visto che in questo caso nella fase
di costruzione del test siamo in grado di calcolare la probabilità di compiere errori di prima specie. Abbiamo anche accennato
al fatto che in realtà le regioni di accettazione e di rifiuto dell'ipotesi nulla vengono determinate proprio in funzione di
tale proprietà, detta livello di significatività. In generale non siamo in grado di calcolare la probabilità di compiere errori
di II specie. Saremmo in grado di farlo se sapessimo quale è il reale valore del parametro a cui si riferisce il test, ma
purtroppo ciò non è possibile.

Per chiarire i termini del problema facciamo riferimento ancora una volta all'esempio precedente, supponendo che il reale
valore del parametro $\mu$ sia 32K euro. Allora la popolazione $X$ è distribuita normalmente con parametri $\mu = 32K$ e
$\sigma^2 = 144K$ e la probabilità $\beta(32)$ di compiere un errore di II specie risulta
\begin{align*}
\beta(32) &= P(\text{accettare } H_0 \text{ quando } \mu = 32)\\
&= P(\bar{X}_{10} \in [25, 35] \mid \bar{X}_{10} \approx N(32, 3.79))\\
&=P\left(\frac{\bar{X}_{10} - 32}{3.79} \in \left[\frac{25 - 32}{3.79}, \frac{35 - 32}{3.79}\right]\right)\\
&=P\left(\frac{\bar{X}_{10} - 32}{3.79} \in [-1.85, +0.79]\right)\\
&= 0.75
\end{align*}
Graficamente tale probabilità corrisponde all'area sottesa alla normale compresa tra i valori 25 e 35.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-07 16:24:31
[[file:Lezioni/screenshot_2018-06-07_16-24-31.png]]

In questo caso è stato possibile determinare la probabilità di compiere un errore della II specie solo perché abbiamo
assunto come ipotesi alternativa $\mu = 32$, escludendo che la media della popolazione possa assumere qualsiasi altro valore oltre che
30 e 32. In realtà però $\mu$ può assumere qualsiasi altro valore $\hat{\mu} \in \mathbb{R}-\{30\}$ ed ognuno di tali valori corrisponde
ad una diversa probabilità $\beta(\hat{\mu})$ di compiere errori di II specie. Per tale ragione, nel descrivere le caratteristiche di un
test di ipotesi, la probabilità di compiere errori di II specie va pensata come una funzione anziché come uno specifico valore
numerico.

In particolare, è uso comune considerare a tal fine una funzione detta /curva di potenza del test/ che viene di seguito introdotta.
Sia a tal fine $\theta$ il parametro a cui si riferisce il test e sia $\theta^*$ il valore specificato dall'ipotesi nulla
$H_0 : \theta = \theta^*$, denotiamo poi $\beta(\hat{\theta}) = P(\text{accettare } H_0 \mid H_0 \text{ è falsa e } \theta = \hat{\theta})$, allora viene detta
/curva di potenza del test/ la funzione $\pi(\hat{\theta}) = 1 - \beta(\hat{\theta})$. Un test risulta tanto migliore quanto più la funzione
$\pi(\hat{\theta})$ si avvicina, al variare di $\hat{\theta}$ al valore 1.

Per esempio, supponiamo di voler effettuare un test di ipotesi relativo ad una popolazione $X$ che sappiamo essere distribuita
secondo un'esponenziale di parametro $\lambda$, e di voler controllare con significatività $\alpha = 0.1$ l'ipotesi nulla $H_0 : \lambda = 2$
contro l'ipotesi alternativa $H_1 : \lambda \neq 2$. Per rendere più comprensibile l'esempio supponiamo di effettuare il test estraendo
un campione di numerosità 1, ovvero $X_1$, e di rifiutare $H_0$ se $X_1$ assume valori "troppo piccoli" o "troppo grandi" per
poter pensare che sia distribuita secondo un'esponenziale di parametro $\lambda = 2$. Prendiamo allora come regione di accettazione
del test l'intervallo $[t_1, t_2] \subseteq \mathbb{R}^+$, dove $t_1$ e $t_2$ sono tali che
$$P(X_1 \leq t_1 \mid X_1 \approx \text{Exp}(2)) = P(X_1 \geq t_2 \mid X_1 \approx \text{Exp}(2)) = \frac{\alpha}{2} = 0.05$$
In questo modo infatti risulta
\begin{align*}
P(\text{compiere errori di I specie}) &= P(\text{rifiutare $H_0$ quando $H_0$ è vera})\\
&= P(X_1 \in (0, t_1) \cup (t_2, \infty) \mid X_1 \approx \text{Exp}(2))\\
&= \frac{\alpha}{2} + \frac{\alpha}{2} = \alpha
\end{align*}
Per determinare $t_1$ e $t_2$ ricordiamo che la funzione di ripartizione di una variabile aleatoria con distribuzione
Esponenziale di parametro 2 è:
\begin{equation*}
F_2(t) =
\begin{cases}
0 &\text{se $t < 0$}\\
1 - e^{-2t} &\text{se $t \geq 0$}
\end{cases}
\end{equation*}
e che $t_1$ e $t_2$ devono essere tali da soddisfare
$$1 - e^{-2t_1} = F_2(t_1) = P(X_1 \leq t_1 \mid X_1 \approx \text{Exp}(2)) = 0.05$$
$$1 - e^{-2t_2} = F_2(t_2) = P(X_1 \leq t_2 \mid X_1 \approx \text{Exp}(2)) = 0.95$$
Con la formula inversa si ricava:
$$t_1 = -\frac{\ln{(1-0.05)}}{2} = 0.03 \quad t_2 = -\frac{\ln{(1 - 0.95)}}{2} = 1.50$$
Pertanto la regione di accettazione del test è $\bar{C} = [0.03, 1.50]$.

Vediamo ora di determinare le probabilità di compiere errori di II specie al variare dei possibili valori alternativi del
parametro
\begin{align*}
\beta(\hat{\lambda}) &= P(\text{accettare $H_0$ quando $\lambda = \hat{\lambda}$})\\
&= P(0.03 \leq X_1 \leq 1.50 \mid X_1 \approx \text{Exp}(\hat{\lambda}))\\
&= F_{\hat{\lambda}}(1.50) - F_{\hat{\lambda}}(0.03) = (1 - e^{-1.50 \hat{\lambda}}) - (1 - e^{0.03 \hat{\lambda}})\\
&= e^{-0.03 \hat{l}} - e^{-1.50 \hat{\lambda}}
\end{align*}
Come si può osservare, tale quantità è una funzione di $\hat{\lambda}$:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-07 17:31:26
[[file:Lezioni/screenshot_2018-06-07_17-31-26.png]]

La /curva di potenza/ è la funzione $\pi(\hat{\lambda}) = 1 - \beta(\hat{\lambda}) = 1 - e^{-0.03 \hat{l}} + e^{-1.50 \hat{\lambda}}$.

$\pi(\hat{\lambda})$ si avvicina a 0 in corrispondenza di $\hat{\lambda} = 2$ in quanto è facile sbagliare ed accettare $H_0$ quando il reale
valore di $\lambda$ non si discosta troppo da quanto specificato dall'ipotesi nulla. Al contrario esso assume valori vicini ad 1 in
prossimità di $\hat{\lambda} = 0$ e per valori grandi di $\hat{\lambda}$ poiché in questi casi difficilmente la statistica $X_1$ assume
valori compresi nella regione di accettazione.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-07 18:27:55
[[file:Lezioni/screenshot_2018-06-07_18-27-55.png]]

Se nella costruzione di un test si desidera diminuire il livello di significatività, vale a dire la probabilità di compiere errori
di I specie, si può ampliarne la regione di accettazione. In questo modo però, com'è facile osservare, diminuisce anche la
potenza del test, ovvero aumenta la probabilità di compiere errori della II specie.

Considerazioni analoghe a quelle fatte relativamente all'ampiezza degli intervalli fiduciari per la stima dei parametri
portano ad osservare che l'unica possibilità di far contemporaneamente diminuire il livello di significatività e aumentare
la potenza è quella di aumentare la numerosità del campione. Questo infatti porta generalmente ad una diminuzione della
varianza della statistica campionaria $T$ e quindi ad una maggiore sicurezza sui valori da essa assumibili condizionatamente
alle due ipotesi.

Per esempio, si consideri una popolazione $X$ con distribuzione Bernoulliana di parametro incognito $p$ e si supponga che $p$
possa assumere solo i due valori $0.40$ e $0.25$. Si supponga di voler controllare l'ipotesi $H_0 : p = 0.25$ e di
estrarre per questo un campione di numerosità 30, considerarne la media campionaria $\bar{X}_{30}$ e rifiutare l'ipotesi
nulla se essa assume un valore maggiore di 0.30. Vediamo quanto valgono in
questo caso il livello di significatività $\alpha$ del test e al probabilità $\beta(0.40)$ di compiere un errore di II specie.

Essendo la numerosità del campione sufficientemente grande si può utilizzare un'approssimazione normale per $\bar{X}_{30}$.
Ricordando che per una Bernoulliana di parametro $p$ la media e la varianza valgono rispettivamente $p$ e $(1-p)p$, e
considerando il Teorema Limite Centrale, si ricava che $\bar{X}_{30} \sim N\left(p, \sqrt{\frac{(1-p)p}{30}}\right)$.

Se è vera l'ipotesi nulla, allora:
\begin{align*}
\alpha &= P(\text{rifiutare $H_0 \mid H_0$ è vera})\\
&= P(\bar{X}_{30} > 0.3 \mid \bar{X}_{30} \approx N(0.25, 0.08))\\
&= P\left(\frac{\bar{X}_{30} - 0.25}{0.08} > \frac{0.3 - 0.25}{0.08}\right)\\
&= 0.26
\end{align*}
Per determinare $\beta(0.40)$ è sufficiente osservare che
\begin{align*}
\beta &= P(\text{accettare $H_0 \mid H_1$ è vera})\\
&= P(\bar{X}_{30} \leq 0.3 \mid \bar{X}_{30} \approx N(0.40, 0.09))\\
&= P\left(\frac{\bar{X}_{30} - 0.4}{0.09} \leq \frac{0.3 - 0.4}{0.09}\right)\\
&= 0.13
\end{align*}
Supponiamo ora di voler avere $\alpha \leq 0.1$ e contemporaneamente $\beta(0.40) \leq 0.1$. Per far diminuire la probabilità di compiere errori
della I specie basterebbe restringere la regione critica, ad esempio rifiutando $H_0$ quando $\bar{X}_{30} > 0.4$. In questo
caso infatti risulterebbe $\alpha = 0.03$. Purtroppo però, così facendo, aumenterebbe $\beta(0.40) = 0.5$. Per avere contemporaneamente
$\alpha \leq 0.1$ e $\beta(0.40) \leq 0.1$ occorre allora aumentare la numerosità del campione.

Se ad esempio viene posta uguale a 123 e se si mantengono la stessa regione critica e di accettazione, con i consueti calcoli si
verifica che $\alpha = 0.1$ e $\beta(0.40) = 0.012$.

Osserviamo che in questo esempio la probabilità di compiere errori di II specie è un singolo valore numerico anziché una funzione
e ciò perché l'ipotesi alternativa $H_1 : p = 0.40$ è un'ipotesi semplice, ovvero un solo valore assumibile alternativamente
a 0.25 dal parametro incognito.
*** Test sulla media di una popolazione
Vediamo ora come si costruisce un test sulla media $\mu$ di una popolazione $X$ quando si formula l'ipotesi nulla che tale media
sia un valore fissato $\mu_0$, ovvero quando si consideri $H_0 : \mu = \mu_0$ e quando si dispone di un campione casuale
$(X_1, \dots, X_n)$ di numerosità $n$ estratto da $X$. Similmente a quanto fatto nel capitolo precedente per gli intervalli di confidenza,
la statistica utilizzata per effettuare il test varia a seconda che la popolazione considerata sia o meno normalmente distribuita
e che la sua varianza sia o non sia sconosciuta:
1. /Popolazione normalmente distribuita e varianza $\sigma^2$ nota/:

   Sia $\mu$ il valore reale del parametro. Ricordiamo che sotto le ipotesi di normalità di $X$ la media campionaria
   $\bar{X}_n = \frac{X_1 + \dots + X_n}{n}$ ha distribuzione normale di media $\mu$ e deviazione standard $\frac{\sigma}{\sqrt{n}}$, ovvero
   vale $\tilde{Z} = \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0, 1)$. Notiamo che $\tilde{Z}$ non è una statistica, infatti il valore
   del parametro $\mu$ non è noto. Sia ora $Z_n = \frac{\bar{X}_n - \mu_0}{\frac{\sigma}{\sqrt{n}}}$ che è una statistica essendo $\mu_0$
   una costante fissata. Ovviamente vale $Z_n \sim N(0, 1)$ se e solo se vale $H_0$. Il test si basa allora sull'osservazione
   del valore assunto dalla realizzazione della statistica $Z_n$, in particolare:
   - Rifiuteremo $H_0$ se $Z_n$ assumerà valori "poco probabili" per una $N(0, 1)$.
   La definizione della regione critica per $Z_n$ dipende dal livello di significatività adottato e dalla forma dell'ipotesi
   alternativa. Possiamo infatti considerare le seguenti 3 ipotesi alternative:
   1. Test bidirezionale: $H_1' : \mu^2 \neq \mu_0^2$;
   2. Test unidirezionale con coda a sinistra: $H_1'' : \mu^2 < \mu_0^2$;
   3. Test unidirezionale con coda a destra: $H_1''' : \mu^2 > \mu_0^2$.
   Più in dettaglio:
   1. /Test bidirezionale/: $H_1' : \mu \neq \mu_0$;

      Ovviamente rifiuteremo l'ipotesi $H_0$ in favore di $H_1'$ quando $Z_n$ assumerà valori eccessivamente grandi o eccessivamente
      piccoli per una $N(0, 1)$. Specificamente, ragionando come nella definizione degli intervalli di confidenza per $\mu$ o
      come nel primo esempio del capitolo, fissato il livello di significatività $\alpha$ del test, la regione critica per la
      statistica $Z_n$ risulta essere:
      - $C' = (-\infty, -z_{1-\frac{\alpha}{2}}) \cup (+z_{1-\frac{\alpha}{2}}, +\infty)$ se $H_1'$ è l'ipotesi alternativa;
      - $C'' = (-\infty, -z_{1-\alpha})$ se $H_1''$ è l'ipotesi alternativa;
      - $C''' = (+z_{1-\alpha}, +\infty)$ se $H_1'''$ è l'ipotesi alternativa.
      dove $z_{1-\frac{\alpha}{2}}$ e $z_{1-\alpha}$ sono rispettivamente i quantili di ordine $1-\frac{\alpha}{2}$ e $1-\alpha$ della normale standardizzata,
      ovvero i valori per cui risulta $P(Z \leq z_{1-\frac{\alpha}{2}}) = 1 - \frac{\alpha}{2}$ e $P(Z \leq z_{1-\alpha}) = 1 - \alpha$ con $Z_n \sim N(0, 1)$ e
      sono ricavabili dalle tavole della normale standard.

      Si perviene a tali regioni critiche osservando infatti che deve essere
      $\alpha = P(\text{rifiutare \(H_0 \mid H_0\) è vera}) = P(Z_n \in C \mid Z_n \sim N(0, 1))$. Notiamo che in base all'esplicita
      relazione che lega la statistica $Z_n$ alla media campionaria $\bar{X}_n$ avremmo potuto usare $\bar{X}_n$ come
      statistica test a cui corrispondono le regioni critiche:
      - $C' = \left(-\infty, \mu_0 -z_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}\right) \cup \left(\mu_0 +z_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}, +\infty\right)$ se $H_1'$ è l'ipotesi alternativa;
      - $C'' = \left(-\infty, \mu_0 -z_{1-\alpha} \frac{\sigma}{\sqrt{n}}\right)$ se $H_1''$ è l'ipotesi alternativa;
      - $C''' = \left(\mu_0 +z_{1-\alpha} \frac{\sigma}{\sqrt{n}}, +\infty\right)$ se $H_1'''$ è l'ipotesi alternativa.
   2. /Test unidirezionale con coda a sinistra/: $H_1'' : \mu < \mu_0$.

      Rifiuteremo l'ipotesi $H_0$ in favore di $H_1''$ quando $Z_n$ assumerà valori fortemente negativi, in tal caso $\mu_0$ è una
      sovrastima della reale media, per cui $Z_n$ tende ad assumere valori "negativi" essendo sicuramente
      $Z_n > \tilde{Z} \sim N(0, 1)$.

      #+DOWNLOADED: /tmp/screenshot.png @ 2018-06-07 18:44:25
      [[file:Lezioni/screenshot_2018-06-07_18-44-25.png]]
   3. /Test unidirezionale con coda a destra/: $H_1''' : \mu > \mu_0$.

      Rifiuteremo l'ipotesi $H_0$ in favore di $H_1'''$ quando $Z_n$ assumerà valori fortemente positivi, in tal caso $\mu_0$ è una
      sottostima della reale media, per cui $Z_n$ tende ad assumere valori "positivi" essendo sicuramente
      $Z_n < \tilde{Z} \sim N(0, 1)$.
      #+DOWNLOADED: /tmp/screenshot.png @ 2018-06-07 18:48:11
      [[file:Lezioni/screenshot_2018-06-07_18-48-11.png]]
2. /Popolazione normalmente distribuita e varianza $\sigma^2$ non nota/:

   Ricordiamo che sotto le ipotesi di normalità di $X$, la variabile $\tilde{T} = \frac{\bar{X}_n - \mu}{\frac{\hat{S}_n}{\sqrt{n}}}$ è
   distribuita come una t di Student con $(n-1)$ gradi di libertà. In questo caso $\hat{S}_n$ è la radice della varianza
   campionaria corretta. Notiamo però che $\tilde{T}$ non è una statistica poiché dipende dal parametro $\mu$ che non è noto.
   Sia ora $T_n = \frac{\bar{X}_n - \mu_0}{\frac{\hat{S}_n}{\sqrt{n}}}$, la quale è una statistica, poiché il valore $\mu_0$ è una
   costante fissata. Ovviamente vale $T_n \sim t_{n-1}$ se vale l'ipotesi nulla.

   Il test si basa allora sull'osservazione del valore assunto dalla realizzazione della statistica $T_n$, in particolare:
   - Rifiuteremo $H_0$ se $T_n$ assumerà valori "poco probabili" per una $t_{n-1}$.
   Anche in questo caso la definizione della regione critica per $T_n$ dipende dal livello di significatività prescelto e dalla
   forma dell'ipotesi alternativa. Infatti, possiamo considerare le 3 ipotesi alternative:
   - Test bidirezionale: $H_1' : \mu \neq \mu_0$;
   - Test unidirezionale con coda a sinistra: $H_1'' : \mu < \mu_0$;
   - Test unidirezionale con coda a destra: $H_1''' : \mu > \mu_0$.
   Definite precedentemente rifiutando $H_0$ in favore di $H_1'$ quando $T_n$ assume valori troppo distanti dall'origine, in favore
   di $H_1''$ quando $T_n$ assume valori fortemente negativi e in favore di $H_1'''$ quando $T_n$ assume valori fortemente positivi.

   Specificamente, fissato il livello di significatività \alpha del test, la regione critica per la statistica $T_n$ risulta essere:
   1. $C' = (-\infty, -t_{1-\frac{\alpha}{2}}) \cup (+t_{1-\frac{\alpha}{2}}, +\infty)$ se $H_1'$ è l'ipotesi alternativa;
   2. $C'' = (-\infty, -t_{1-\alpha})$ se $H_1''$ è l'ipotesi alternativa;
   3. $C''' = (+t_{1-\alpha}, +\infty)$ se $H_1'''$ è l'ipotesi alternativa.
   dove $t_{1-\frac{\alpha}{2}}$ e $t_{1-\alpha}$ sono rispettivamente i quantili di ordine $1-\frac{\alpha}{2}$ e $1-\alpha$ della t di Student con $(n-1)$ gradi di
   libertà, ovvero i valori per cui risulta $P(T \leq t_{1-\frac{\alpha}{2}}) = 1 - \frac{\alpha}{2}$ e $P(T \leq t_{1-\alpha}) = 1 - \alpha$ con $T_n \sim t_{n-1}$ e
   sono ricavabili dalle tavole della t di Student.

   Anche in questo caso notiamo che in base all'esplicita relazione che lega la statistica $T_n$ alla media campionaria $\bar{X}_n$
   avremmo potuto usare $\bar{X}_n$ come statistica test, a cui corrispondono le regioni critiche.
   1. $C' = \left(-\infty, \mu_0 -t_{1-\frac{\alpha}{2}}\frac{\hat{s}_n}{\sqrt{n}}\right) \cup \left(\mu_0 +t_{1-\frac{\alpha}{2}}\frac{\hat{s}_n}{\sqrt{n}}, +\infty\right)$
      se $H_1'$ è l'ipotesi alternativa;
   2. $C'' = \left(-\infty, \mu_0 -t_{1-\alpha} \frac{\hat{s}_n}{\sqrt{n}}\right)$ se $H_1''$ è l'ipotesi alternativa;
   3. $C''' = \left(\mu_0 +t_{1-\alpha} \frac{\hat{s}_n}{\sqrt{n}}, +\infty\right)$ se $H_1'''$ è l'ipotesi alternativa.
   dove $\hat{s}_n$ è la realizzazione di $\hat{S}_n$.
3. /Popolazione non normalmente distribuita/:

   In questo caso per poter definire un test occorre avere un campione di numerosità sufficientemente elevate ($n \geq 30$).
   Abbiamo già visto che quando $H_0$ sussiste allora la media campionaria $\bar{X}_n$ è approssimabile con una variabile
   con distribuzione normale di media $\mu$ e deviazione standard $\frac{\sigma}{\sqrt{n}}$ quando $\sigma$ è nota, o di deviazione
   $\frac{\hat{s}_n}{\sqrt{n}}$ quando $\sigma$ è sconosciuta. Ci si può allora ricondurre al caso di popolazione con
   distribuzione normale e varianza nota, adottando le stesse regioni critiche sopra descritte.

   Non si può invece fare nulla, in generale, se la numerosità del campione è inferiore a 30.
   
*** Test sulla varianza di una popolazione
Vediamo ora come si costruisce un test sulla varianza $\sigma^2$ di una popolazione $X$ quando si formula l'ipotesi nulla che tale varianza
è pari ad un valore fissato $\sigma_0^2$, vale a dire quando si consideri $H_0 : \sigma^2 = \sigma_0^2$ e quando si dispone di un campione casuale
$(X_1, \dots, X_n)$ di numerosità $n$ estratto da $X$.

Purtroppo non è possibile effettuare un test quando la popolazione ha una distribuzione diversa dalla normale, anche nel caso
in cui la numerosità del campione sia elevata, e questo per le stesse ragioni per cui non è possibile definire un intervallo
di confidenza per la varianza di popolazioni non normalmente distribuite.

Al contrario, se la popolazione $X$ è normalmente distribuita, allora si può costruire un test per l'ipotesi $H_0$ facendo
ricorso al fatto che in tal caso la variabile $\tilde{Q} = \frac{(n-1) \cdot \hat{S}_n^2}{\sigma^2}$ è distribuita come una Chi-quadro con
$(n-1)$ gradi di libertà e dove $\hat{S}_n^2$ è la varianza campionaria corretta. Si osservi che $\tilde{Q}$ non è una statistica,
infatti non è noto il valori di $\sigma^2$. Quando però l'ipotesi nulla $H_0$ è vera, allora vale
$Q_n = \frac{(n-1) \cdot \hat{S}_n^2}{\sigma_0^2} \sim \chi_{n-1}^2$. Il test si basa sull'osservazione del valore assunto dalla realizzazione della
statistica.

Rifiuteremo $H_0$ se $Q_n$ assumerà valori poco probabili per una Chi-quadro con $(n-1)$ gradi di libertà.

Come nei test per il valor medio $\mu$, la definizione della regione critica per $Q_n$ dipende dal livello di significatività
adottato e dalla forma dell'ipotesi alternativa. Possiamo infatti considerare le seguenti 3 ipotesi alternative:
1. Test bidirezionale: $H_1' : \sigma^2 \neq \sigma_0^2$;
2. Test unidirezionale con coda a sinistra: $H_1'' : \sigma^2 < \sigma_0^2$;
3. Test unidirezionale con coda a destra: $H_1''' : \sigma^2 > \sigma_0^2$.
Rifiuteremo $H_0$ in favore di:
1. $H_1'$ quando $Q_n$ assume valori troppo grandi o piccoli per una Chi-quadro con $(n-1)$ gradi di libertà;
2. $H_1''$ quando $Q_n$ assume valori troppo vicini a 0, infatti quando vale $H_1''$, $\sigma_0^2$ è una sovrastima di $\sigma^2$;
3. $H_1'''$ quando $Q_n$ assume valori fortemente positivi (per ragioni opposte a quanto sopra).
Specificamente, ragionando come nella definizione degli intervalli di confidenza per $\sigma^2$ fissato il livello di significatività
$\alpha$ del test, la regione critica per la statistica $Q_n$ risulta essere:
1. $C' = (0, q_{\frac{\alpha}{2}}) \cup (q_{1-\frac{\alpha}{2}}, +\infty)$ se $H_1'$ è l'ipotesi alternativa;
2. $C'' = (0, q_{\alpha})$ se $H_1''$ è l'ipotesi alternativa;
3. $C''' = (q_{1-\alpha}, +\infty)$ se $H_1'''$ è l'ipotesi alternativa.
   Dove con il generico $q_{\gamma}$ si intende il quantile di ordine $\gamma$ della Chi-quadro con $(n-1)$ gradi di libertà, ovvero quel
   valore per cui risulta $P(Q \leq q_{\gamma}) = \gamma$ con $Q \sim \chi_{n-1}^2$ ed è ricavabile dalle tavole della Chi-quadro.

Si perviene a tali regioni critiche osservando che deve essere:
$$\alpha = P(\text{rifiutare $H_0 \mid H_0$ è vera}) = P(Q_n \in C \mid Q_n \approx \chi_{n-1}^2)$$
Notiamo che in base all'esplicità relazione che lega la statistica $Q_n$ alla varianza campionaria $S_n^2$ avremmo potuto usare
$S_n^2$ come statistica test a cui corrispondono le regioni critiche.
1. $C' = \left(0, q_{\frac{\alpha}{2}}\frac{\sigma_0^2}{n}\right) \cup \left(q_{1-\frac{\alpha}{2}}\frac{\sigma_0^2}{n}, +\infty\right)$ se $H_1'$ è l'ipotesi alternativa;
2. $C'' = \left(0, q_{\alpha}\frac{\sigma_0^2}{n}\right)$ se $H_1''$ è l'ipotesi alternativa;
3. $C''' = \left(q_{1-\alpha}\frac{\sigma_0^2}{n}, +\infty\right)$ se $H_1'''$ è l'ipotesi alternativa.
Si osservi infine che per effettuare un test sulla varianza di una popolazione normalmente distribuita non occorre
conoscere il valor medio della popolazione stessa. Ciò si deduce dal fatto che il parametro $\mu$ e le sue stime non compaiono
nella definizione delle regioni critiche del test.

Per esempio, si riconsideri il problema del reddito medio annuo per famiglia (in migliaia di euro) di un determinato quartiere
di una città. Si supponga, come nel primo esempio del capitolo, che tale reddito sia normalmente distribuito ma che non sia nota
la sua varianza $\sigma^2$. Supponiamo di voler controllare con un livello di significatività $\alpha = 0.05$ l'ipotesi che tale
varianza sia uguale a $\sigma_0^2 = 25$ e di poter disporre di un campione di numerosità $n = 10$. Vediamo allora qual è la regione
critica del test quando l'ipotesi alternativa è $H_1 : \sigma^2 \neq 25$.

In base a quanto affermato sopra, la statistica da utilizzare è la $Q_n = \frac{(n-1) \cdot \hat{S}_n^2}{\sigma_0^2}$. Pertanto la regione
critica deve essere $C = (0, q_{\frac{\alpha}{2}}) \cup (q_{1-\frac{\alpha}{2}}, +\infty)$ dove $q_{\frac{\alpha}{2}} = q_{0.025} = 2.70$ e $q_{1-\frac{\alpha}{2}} = q_{0.975} = 19$.
In definitiva, la regione critica sarà $C = (0, 2.7) \cup (19, +\infty)$.

Supponiamo ora di effettuare l'indagine e di osservare che la realizzazione del campione fornisca una varianza campionaria corretta.
$\hat{s}_{10}^2 = 35.5$. La realizzazione della statistica risulta essere $Q_{10} = \frac{(10-1) \cdot \hat{S}_{10}^2}{\sigma_0^2} = \frac{9 \cdot 35.5}{25} = 12.8$.
E poiché tale valore non fa parte della regione critica del test accettiamo l'ipotesi $H_0 : \sigma^2 = 25$, o meglio diremo che
l'ipotesi $H_0$ non può essere rifiutata.
*** Test sulla differenza delle medie di due popolazioni
Il test che verrà ora descritto non è propriamente un test di tipo parametrico, in quanto non viene utilizzato per controllare
un possibile valore assunto da un parametro, bensì per controllare l'ipotesi che le medie di due distinte popolazioni $X$ ed $Y$
siano identiche. Nonostante ciò viene solitamente considerato appartenente alla famiglia dei test parametrici per via delle
ipotesi di normalità a cui devono sottostare le popolazioni considerate.

Siano quindi $X$ ed $Y$ due popolazioni normalmente distribuite aventi rispettivamente valor medio $\mu_x$ e $\mu_y$ ed identica
varianza $\sigma^2$ incognita. Sia $\mu = \mu_X - \mu_Y$ e si voglia controllare l'ipotesi $H_0 : \mu = 0$ disponendo di due campioni casuali
$(X_1, \dots, X_n)$ e $(Y_1, \dots, Y_m)$. Siano $\bar{X}_n$ e $\bar{Y}_m$ le medie campionarie e $S_{X, n}^2$ e $S_{Y, m}^2$ le varianze campionarie.

Il test si basa sul fatto che quando valgono le ipotesi sopra elencate, allora:
$$\tilde{T} = \frac{\bar{X}_n - \bar{Y}_m - \mu}{\sqrt{\frac{(n+m)(n \cdot S_{X, n}^2 + m \cdot S_{Y, m}^2)}{n \cdot m \cdot (n + m - 2)}}} \sim t_{n+m-2}$$
Si possono pertanto definire le regioni critiche del test considerando tale proprietà e ragionando come nel caso del test per
la media di una popolazione normalmente distribuita con varianza incognita. Considerando cioè le ipotesi alternative.
Sia $\mu = \mu_X - \mu_Y$:
1. Test bidirezionale: $H_1' : \mu \neq \mu_0$;
2. Test unidirezionale con coda a sinistra: $H_1'' : \mu < \mu_0$;
3. Test unidirezionale con coda a destra: $H_1''' : \mu > \mu_0$
Le corrispondenti regioni critiche per la statistica
$$T_{n, m} = \frac{\bar{X}_n - \bar{Y}_m}{\sqrt{\frac{(n+m)(n \cdot S_{X, n}^2 + m \cdot S_{Y, m}^2)}{n \cdot m \cdot (n + m - 2)}}}$$
risultano essere:
1. $C' = (-\infty, -t_{1-\frac{\alpha}{2}}) \cup (+t_{1-\frac{\alpha}{2}}, +\infty)$ se $H_1'$ è l'ipotesi alternativa;
2. $C'' = (-\infty, -t_{1-\alpha})$ se $H_1''$ è l'ipotesi alternativa;
3. $C''' = (+t_{1-\alpha}, +\infty)$ se $H_1'''$ è l'ipotesi alternativa.
Dove $t_{1-\frac{\alpha}{2}}$ e $t_{1-\alpha}$ sono rispettivamente i quantili di ordine $1-\frac{\alpha}{2}$ e $1-\alpha$ della t di Student con $(n+m-2)$ gradi di
libertà, ovvero i valori per cui risulta $P(T \leq t_{1-\frac{\alpha}{2}}) = 1 - \frac{\alpha}{2}$ e $P(T \leq t_{1-\alpha}) = 1 - \alpha$ con $T_n \sim t_{n+m-2}$ e
sono ricavabili dalle tavole della t di Student.

Nota: ripetendo la sequenza di calcoli usata nel capitolo precedente, è possibile utilizzare la variabile aleatoria:
$$\tilde{T} = \frac{\bar{X}_n - \bar{Y}_m - \mu}{\sqrt{\frac{(n+m)(n \cdot S_{X, n}^2 + m \cdot S_{Y, m}^2)}{n \cdot m \cdot (n + m - 2)}}} \sim t_{n+m-2}$$
per ottenere un intervallo di confidenza sulla differenza delle medie:
$$P\left[(\bar{X}_n - \bar{Y}_m) - t_{1 - \frac{\alpha}{2}}\sqrt{\frac{(n+m)(n \cdot S_{X, n}^2 + m \cdot S_{Y, m}^2)}{n \cdot m \cdot (n + m - 2)}} \leq \mu \leq
(\bar{X}_n - \bar{Y}_m) + t_{1 - \frac{\alpha}{2}}\sqrt{\frac{(n+m)(n \cdot S_{X, n}^2 + m \cdot S_{Y, m}^2)}{n \cdot m \cdot (n + m - 2)}}\right] = 1 - \alpha$$
Si osservi che tra le ipotesi necessarie per effettuare questo test compare la richiesta che le due popolazioni abbiano identica
varianza anche se sconosciuta. Questo potrebbe sembrare un grosso problema che può essere risolto tramite il test nel prossimo
paragrafo.

Un test per la differenza delle medie di due popolazioni può essere definito anche quando le varianze delle due popolazioni sono
diverse (purché note). Si può dimostrare infatti che in questo caso vale
$$Z_{n, m} = \frac{(\bar{X}_n - \bar{Y}_m) - \mu}{\sqrt{\frac{\sigma_X^2}{n} + \frac{\sigma_Y^2}{m}}} \sim N(0, 1)$$
purché:
- O $X$ ed $Y$ siano normalmente distribuite;
- O le numerosità $n$ ed $m$ dei campioni siano sufficientemente grandi.
La definizione delle regioni critiche segue quindi con il solito procedimento.

In realtà comunque non capita spesso di conoscere esattamente il valore delle due varianze.

*** Test sull'uguaglianza delle varianze di due popolazioni
Abbiamo appena visto che per poter effettuare un test sulla differenza delle medie di due popolazioni normalmente distribuite
di cui non sono note le varianze occorre assumere che tali varianze siano identiche. Vediamo quindi come sia possibile
controllare tale ipotesi. Il test in questione si basa sul fatto che se $X \sim N(\mu_X, \sigma_X)$ e $Y \sim N(\mu_Y, \sigma_Y)$ allora la variabile
$\tilde{V} = \frac{\sigma_Y^2 \cdot \hat{S}_{X, n}^2}{\sigma_X^2 \cdot \hat{S}_{Y, m}^2}$ risulta essere distribuita come una F con $(n-1)$ e $(m-1)$ gradi di libertà.
$\hat{S}_{X, n}$ e $\hat{S}_{Y, m}^2$ sono le varianze campionarie corrette.

La $\tilde{V}$ non è propriamente una statistica poiché essa dipende dai due parametri $\sigma_Y^2$ e $\sigma_X^2$ che sono incogniti, però quando
risulti soddisfatta l'ipotesi $H_0 : \sigma_X^2 = \sigma_Y^2$ allora $V_{n, m} = \frac{\hat{S}_{X, n}^2}{\hat{S}_{Y, m}^2}$ risulta essere distribuita come
una F con $(n-1)$ e $(m-1)$ gradi di libertà. Si osservi che $V_{n, m}$ invece è proprio una statistica essendo definita solo
in funzione dei due campioni $(X_1, \dots, X_n)$ e $(Y_1, \dots, Y_m)$.

Consideriamo ora le 3 ipotesi alternative:
1. Test bidirezionale: $H_1' : \frac{\sigma_Y^2}{\sigma_X^2}\neq 1$;
2. Test unidirezionale con coda a sinistra: $H_1'' : \frac{\sigma_Y^2}{\sigma_X^2} < 1$;
3. Test unidirezionale con coda a destra: $H_1''' : \frac{\sigma_Y^2}{\sigma_X^2} > 1$.
Rifiuteremo $H_0$ in favore di:
1. $H_1'$ quando $V_{n, m}$ assume valori troppo grandi o piccoli per una $F(n-1, m-1)$;
2. $H_1''$ quando $V_{n, m}$ assume valori troppo vicini a 0;
3. $H_1'''$ quando $V_{n, m}$ assume valori fortemente positivi.
Specificamente, ragionando come nella definizione degli intervalli di confidenza per $\sigma^2$ fissato il livello di significatività
$\alpha$ del test, la regione critica per la statistica $V_{n, m}$ risulta essere:
1. $C' = [0, f_{\frac{\alpha}{2}}) \cup (f_{1-\frac{\alpha}{2}}, +\infty)$ se $H_1'$ è l'ipotesi alternativa;
2. $C'' = [0, f_{\alpha})$ se $H_1''$ è l'ipotesi alternativa;
3. $C''' = (f_{1-\alpha}, +\infty)$ se $H_1'''$ è l'ipotesi alternativa.
Dove con il generico $f_{\gamma}$ si intende il quantile di ordine $\gamma$ della $F(n-1, m-1)$, ovvero quel
valore per cui risulta $P(V \leq f_{\gamma}) = \gamma$ con $V \sim F(n-1, m-1)$ ed è ricavabile dalle tavole della F.

Per esempio, consideriamo due diversi quartieri della stessa città e supponiamo di essere interessati a confrontare il valor medio
dei redditi annui familiari in migliaia di euro. Si supponga che sia lecito assumere che tali redditi siano normalmente distribuiti.
Si supponga di estrarre un campione casuale per ogni quartiere di numerosità rispettivamente pari a 10 e 15. Utilizzando le notazioni
introdotte in precedenza, sia $X$ la popolazione del primo quartiere ed $Y$ la popolazione del secondo. Supponiamo che le
realizzazioni dei campioni relativi ad $X$ ed $Y$ siano $(22, 48, 51, 20, 28, 35, 38, 26, 50, 36)$ e
$(40, 42. 50, 26, 30, 34, 37, 25, 30, 32, 38, 22, 55, 40)$.

In base ai due campioni si ottiene: $\bar{x}_{10} = 35.4$, $\bar{y}_{15} = 35.3$, $s_{X, 10}^2 = 118.2$ e $s_{Y, 15}^2 = 79.7$.

Volendo controllare l'ipotesi $H_0 : \mu_X = \mu_Y$ occorre considerare la statistica
$$T_{n, m} = \frac{\bar{X}_n - \bar{Y}_m}{\sqrt{\frac{(n+m)(n \cdot S_{X, n}^2 + m \cdot S_{Y, m}^2)}{n \cdot m \cdot (n + m - 2)}}}$$
che nel caso particolare assume valore $t_{10, 15} = 0.024$.

Il valore ottenuto porta ad accettare $H_0$ comunque si scelga l'ampiezza $\alpha$ del test. Osserviamo però che tra le ipotesi
necessarie per effettuare il test della differenza delle medie con la statistica $T_{n, m}$ vi è quella che le due popolazioni
abbiano identica varianza. Prima di accettare definitivamente l'ipotesi $H_0 : \mu_X = \mu_Y$ occorre quindi controllare l'ipotesi
$H_0' : \sigma_X^2 = \sigma_Y^2$.

Utilizziamo per questo il test appena descritto e basato sulla statistica $V_{n, m} = \frac{\hat{S}_{X, n}^2}{\hat{S}_{Y, m}^2}$
la cui realizzazione nel nostro caso è $V_{10, 15} = \frac{\hat{S}_{X, 10}^2}{\hat{S}_{Y, 15}^2} = 1.54$.

Vediamo se questo nuovo dato ci porta a non rifiutare $H_0'$ contro $H_1' : \sigma_X^2 \neq \sigma_Y^2$ con un livello di significatività $\alpha = 0.1$.
In questo caso, dalle tavole della F si ricava che per 9 e 14 gradi di libertà i quantili sono $f_{\frac{\alpha}{2}} = f_{0.05} = 0.38$ e
$f_{1 - \frac{\alpha}{2}} = f_{0.95} = 2.65$.

Pertanto la regione di accettazione di $H_0'$ è $(0.38, 2.65)$.

Il valore della statistica $V_{n, m} = \frac{\hat{S}_{X, n}^2}{\hat{S}_{Y, m}}$ è $v_{10, 15} = 1.54$.

Possiamo allora affermare, con un livello di significatività pari a 0.1, che le ipotesi per la validità del test per la differenza
delle medie non possono essere rifiutate e di conseguenza che non si può rifiutare l'ipotesi $H_0$ che siano uguali anche le due medie.
*** Test di incorrelazione
Il test che viene ora presentato si riferisce ad un parametro che descrive il grado di correlazione tra due popolazioni $X$ ed $Y$,
ovvero tra due diversi caratteri di una popolazione bidimensionale $(X, Y)$. Tale parametro è il coefficiente di correlazione lineare
di Pearson: $\rho_{XY} = \frac{\sigma_{XY}}{\sigma_X \cdot \sigma_Y}$, dove $\sigma_{XY} = E[(X - \mu_X) \cdot (Y - \mu_Y)]$ è la covarianza tra i due caratteri.

Osserviamo che quando sono noti i dati $(x_i, y_i)$ di tutti gli individui della popolazione bidimensionale $(X, Y)$ allora $\rho_{XY}$ coincide con
il coefficiente di correlazione lineare $r_{xy}$ descritto nel primo capitolo:
$$r_{xy} = \frac{c_{xy}}{s_x \cdot s_y} = \frac{\frac{1}{N} \cdot \sum_{i=1}^N (x_i - \mu_X) \cdot (y_i - \mu_Y)}{\sigma_X \cdot \sigma_Y}$$
dove $N$ è il numero complessivo di individui della popolazione bidimensionale $(X, Y)$.

Ricordiamo che i parametri $\rho_{XY}$ e $r_{xy}$ possono assumere valori in $[-1, 1]$ e sono indici del grado di allineamento delle coppie di dati
$(x_i, y_i)$. Sono nulli quando la covarianza dei due caratteri è nulla e sono uguali ad 1 in valore assoluto quando le coppie di dati si
trovano lungo una retta.

Data una popolazione bidimensionale $(X, Y)$ in contesti reali non è mai possibile determinare il valore del coefficiente di correlazione
lineare $\rho_{XY}$ e questo per gli stessi motivi per i quali non è possibile determinare esattamente il valore dei parametri $\mu_X$, $\mu_Y$, $\sigma_X$ e
$\sigma_Y$. Come per questi parametri è però possibile effettuarne una stima. A tal fine si usa lo stimatore $R_n$ basato su un campione
$((X_1, Y_1), \dots, (X_n, Y_n))$ definito come:
$$R_n = \frac{\sum_{i=1}^n (X_i - \bar{X}) \cdot (Y_i - \bar{Y})}{n \cdot S_{X, n} \cdot S_{Y, n}}$$
dove $\bar{X}$ e $\bar{Y}$ sono le medie campionarie dei due caratteri mentre $S_{X, n}$ e $S_{Y, n}$ sono le radici delle due varianze campionarie.

Il test di incorrelazione che ora presentiamo serve a verificare che il coefficiente $\rho_{XY}$ assuma valore 0, cioè che sussista incorrelazione tra
i due caratteri. Il test è basato sul fatto che sotto l'ipotesi $H_0 : \rho_{XY} = 0$, la statistica $\hat{T}_n = R_n \cdot \sqrt{\frac{n-2}{1-R_n^2}}$ risulta
essere distribuita come una t di Student con $(n-2)$ gradi di libertà. Questa proprietà infatti può essere utilizzata per confrontare $H_0$ con
l'ipotesi alternativa $H_1 : \rho_{XY} \neq 0$ non rifiutando $H_0$ quando $\hat{T}_n$ assume valori non troppo distanti dallo 0 e rifiutandola in favore
di $H_1$ in caso contrario.

Ragionando al solito modo, si ottiene allora che per un livello di significatività $\alpha$, la regione critica della statistica $\hat{T}_n$ è
$C = \left(-\infty, -t_{1-\frac{\alpha}{2}}\right) \cup \left(+t_{1-\frac{\alpha}{2}}, +\infty\right)$, dove $t_{1 - \frac{\alpha}{2}}$ è il quantile di ordine $1 - \frac{\alpha}{2}$ della t
di Student con $(n-2)$ gradi di libertà.

Per esempio, si consideri una popolazione $(X, Y)$ di travi di cemento, in cui:
- $X$ indica il carico di prima lesione;
- $Y$ indica il carico a rottura finale.
Si supponga di avere un'ampia popolazione, ovvero di avere un elevato numero di queste travi, e di voler controllare, con livello di
significatività $\alpha = 0.05$, se esiste una correlazione tra il carico di prima lesione ed il carico a rottura finale. A tal scopo si
supponga di aver effettuato delle prove su 15 travi ottenendo i risultati riportati nella seguente tabella:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-07 23:41:55
[[file:Lezioni/screenshot_2018-06-07_23-41-55.png]]

In base ai consueti calcoli si verifica che una stima del coefficiente di correlazione lineare è
$$R_n = \frac{\sum_{i=1}^{15} (X_i - \bar{X}) \cdot (Y_i - \bar{Y})}{15 \cdot S_{X, 15} \cdot S_{Y, 15}} = 0.9195$$
che risulta uguale al coefficiente di correlazione lineare trovato nell'esempio del primo capitolo, nel quale si supponeva che l'intera popolazione
fosse costituita solo dalle 15 travi esaminate. La statistica $\hat{T}_n$ assume invece valore $\hat{t}_{15} = 0.9195 \cdot \sqrt{\frac{15 - 2}{1 - 0.9195^2}} = 8.43$.

Vediamo ora se il valore ottenuto consente di non rifiutare l'ipotesi $H_0$ che non esiste correlazione tra i due caratteri. Per questo è
sufficiente notare che la regione critica del test, relativamente alla statistica $\hat{T}_{15}$ è 
$C = \left(-\infty, -t_{0.975}\right) \cup \left(+t_{0.975}, +\infty\right) = (-\infty, -2.16) \cup (+2.16, +\infty)$,
dove il quantile $t_{0.975} = 2.16$ è stato ricavato dalla tavola della distribuzione t di Student con 13 gradi di libertà. Poiché il valore da noi
osservato di $\hat{T}_{15}$ cade nella regione critica del test è lecito supporre che vi sia un legame lineare tra il carico di prima lesione
e il carico a rottura, ovvero rifiutando l'ipotesi nulla di incorrelazione tra carico di prima lesione e carico di rottura.

Osserviamo che la procedura descritta in questo paragrafo consente di sottoporre a test l'ipotesi di incorrelazione lineare tra due diversi
caratteri di una popolazione, vale a dire $\rho_{XY} = 0$. Si tenga comunque presente che è anche possibile effettuare dei test sul coefficiente
di correlazione di Pearson più generici rispetto a quello qui considerato, cioè con ipotesi nulle del tipo $H_0 : \rho_{XY} = \rho^*$ con $\rho^*$ diverso
da 0. In tal caso occorre però far ricorso a statistiche campionarie che non riteniamo opportuno introdurre qui.
** Verifica di Ipotesi: Test non parametrici
Nel capitolo precedente abbiamo visto dei test che si riferiscono ad alcuni parametri delle popolazioni. Essi richiedono sempre ipotesi forti
sulla distribuzione della popolazione, è quasi sempre richiesto, ad esempio, che essa sia normalmente distribuita.
Ma come possiamo stabilire
quando una popolazione è normalmente distribuita? Oppure, cosa possiamo fare quando non abbiamo elementi per affermare che una popolazione
non è normalmente distribuita o quando siamo certi che non lo è?
Per rispondere a queste domande sono stati studiati dei test appositi;
alcuni possono essere effettuati anche quando le ipotesi di normalità non sono soddisfatte, altri servono invece per stabilire quali tipi
di distribuzioni sono ipotizzabili per la popolazione considerata.

Tali test presentano inoltre altri vantaggi:
- Sono utilizzabili anche nel caso in cui i campioni siano piccoli;
- Sono di più facile calcolo rispetto ai corrisponendenti test parametrici.
A loro discapito bisogna dire che:
- I calcoli diventano più complessi quando i campioni sono numerosi;
- Sono meno potenti dei corrispondenti test parametrici (maggiore probabilità di compiere errore di II specie).

In questo capitolo verranno introdotti:
- /Test per la bontà dell'adattamento/ che servono per verificare se una popolazione segue una distribuzione prestabilita;
- /Test per confrontare le distribuzioni di due popolazioni/ quando queste non siano necessariamente distribuite normalmente;
- /Test per verificare se sussiste indipendenza o incorrelazione tra due diversi caratteri di una popolazione/.
Anche per questi test valgono le considerazioni viste in merito agli errori di I e II specie ed alle regioni critiche e di accettazione
espresse relativamente ai test parametrici. Per tale ragione non le ripeteremo.
Infine, ricordiamo che tali test, proprio per la particolarità di non richiedere assunti sulla distribuzione dei dati, sono detti
/indipendenti dalla distribuzione/.
*** Test per la bontà dell'adattamento
I due test che verranno presentati servono per rispondere alla seguente domanda: "possiamo affermare che la popolazione $X$ esaminata è
distribuita secondo una specifica funzione di ripartizione $F$?"

Questi test sono detti di /bontà dell'adattamento/ proprio perché quello che si chiede è se la distribuzione specificata $F$ è adatta a
descrivere la variazioni nella popolazione $X$. Pertanto per entrambi i test considereremo l'ipotesi nulla
$H_0 : F_X (t) = F(t)$ per ogni $t \in \mathbb{R}$, dove $F_X$ è la reale distribuzione della popolazione $X$, mentre $F$ è quella da
noi specificata. In entrambi i casi, l'ipotesi alternativa sarà: $H_1 : F_X(t) \neq F(t)$ per almeno un $t \in \mathbb{R}$.
**** Test di Kolmogorov-Smirnov
Il test di Kolmogorov-Smirnov si basa sulla nozione di distribuzione empirica che ora introdurremo. Sia $(X_1, \dots, X_n)$ un campione di numerosità
$n$ estratto da una popolazione $X$. È detta /funzione di ripartizione empirica/ della popolazione $X$ basata su $(X_1, \dots, X_n)$ la funzione
aleatoria
$$\hat{F}_{X, n}(t) = \frac{1}{n}\sum_{i=1}^n U_{(-\infty, t]}(X_i) \quad \text{per ogni $t \in \mathbb{R}$}$$
dove
\begin{equation*}
U_{(-\infty, t]}(X_i) =
\begin{cases}
1 &\text{se $X_i \in (-\infty, t]$}\\
0 &\text{altrimenti}
\end{cases}
\end{equation*}
Notiamo che non possiamo sapere come questa funzione è fatta fino a che non conosciamo la realizzazione $(x_1, \dots, x_n)$ di $(X_1, \dots, X_n)$.

Per esempio, in un passato esempio si assumeva che la variabile $X$ che descrive il tempo necessario per ottenere la concessione per la
ristrutturazione di un immobile fosse distribuita con legge Esponenziale, il cui parametro $\lambda$ veniva stimato facendo uso dei dati
$(3.0, 4.1, 2.8, 5.5, 1.5, 2.2, 6.0, 1.2, 3.2, 0.9)$ assunti da un campione di numerosità 10 estratto da $X$. Vediamo ora qual è
la funzione di ripartizione empirica che si ottiene considerando tale realizzazione del campione. A tale scopo conviene riordinare in maniera
crescente i dati di tale campione, ottenendo la sequenza $(0.9, 1.2, 1.5, 2.2, 2.8, 3.0, 3.2, 4.1, 5.5, 6.0)$.

La funzione di ripartizione empirica è tale che in corrispondenza di ciascuno di tali valori essa compie un salto di ampiezza 0.1, partendo da
0 ed arrivando ad 1 in corrispondenza dell'ultimo valore, ovvero 6.0:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 00:25:02
[[file:Lezioni/screenshot_2018-06-08_00-25-02.png]]

Si osservi che le funzioni di ripartizione empiriche sono sempre delle funzioni crescenti a "gradino".

È facile rendersi conto poi che al crescere della numerosità del campione $n$, la funzione di ripartizione empirica assomiglia
sempre di più alla reale funzione di ripartizione della popolazione fino a coincidere con essa quando $n$ corrisponde alla
numerosità dell'intera popolazione.

Il /test di Kolmogorov-Smirnov/ si basa sulla statistica $D_n = \text{sup}_{t \in \mathbb{R}} | F(t) - \hat{F}_{X, n}(t)|$ che specifica l'estremo superiore delle distanze
in valore assoluto tra la funzione di ripartizione che vogliamo controllare come possibile per $X$ e quella empirica ottenuta
tramite il campione disponibile.

Si può dimostrare che quando vale $H_0$ e quando $F$ è una funzione continua, allora tale statistica è indipendente dalla forma
di $F$, ovvero ha la stessa distribuzione qualunque sia $F$, ma ovviamente varierà al variare di $n$.

La distribuzione della statistica campionaria $D_n$ al variare di $n$ è stata studiata dagli inventori del test, che hanno
fornito apposite tabelle per determinare i quantili di cui riportiamo un estratto:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 12:51:39
[[file:Lezioni/screenshot_2018-06-08_12-51-39.png]]

Pertanto è possibile fissare delle regioni di accettazione e critiche al variare di $n$ e del livello di significatività. Per
questo notiamo che è logico aspettarsi che $D_n$ assuma valori piccoli se l'ipotesi nulla è vera, ed assuma valori grandi quando
$H_0$ è falsa. Infatti, per ogni ampiezza $\alpha$ del test, la regione critica risulta essere $C = (d_{1-\alpha}, +1]$, dove il quantile
$d_{1-\alpha}$ è quel valore per cui risulta $P(D_n \leq d_{1-\alpha}) = 1 - \alpha$ che può essere determinato sulle apposite tavole. Si noti che $D_n$
non può sicuramente assumere valori maggiori di 1.

Riprendiamo in considerazione l'esempio precedente e controlliamo l'ipotesi che la popolazione segua una distribuzione
esponenziale di parametro 0.35 con ampiezza $\alpha = 0.05$. Per far ciò occorre sovrapporre il grafico di ripartizione empirica
trovato nell'esempio precedente con il grafico della funzione di ripartizione
\begin{equation*}
F(t) =
\begin{cases}
1 \cdot e^{-0.35 t} &\text{se $t \geq 0$}\\
0 &\text{se $t < 0$}
\end{cases}
\end{equation*}
che vogliamo controllare.

Tale operazione viene riportata nella figura di seguito, dove è possibile osservare che la massima distanza in valore assoluto
si ottiene per $t=0.9$ e vale $0.27$.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 12:59:48
[[file:Lezioni/screenshot_2018-06-08_12-59-48.png]]

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 13:00:15
[[file:Lezioni/screenshot_2018-06-08_13-00-15.png]]

Si osservi che la massima distanza tra la funzione di ripartizione $F$ e la funzione di ripartizione empirica viene raggiunta
sempre in corrispondenza di uno dei salti della distribuzione empirica. Questo fatto può essere utile nella determinazione di
$D_n$, quando non è possibile fare uso di rappresentazioni grafiche.

Osserviamo che il test di Kolmogorov-Smirnov non richiede particolari assunti sui dati o sulla distribuzione $F$ per essere
impiegato, se non quello di continuità della $F$ stessa.
*** Test del Chi-Quadro
Il test Chi-Quadro per la bontà dell'adattamento può essere utilizzato senza porre condizioni sulla $F$.
Anche per poter descrivere questo test occorre introdurre alcune nozioni e notazioni e come al solito supporremo di poter estrarre un
campione $(X_1, \dots, X_n)$ di numerosità $n$ della popolazione $X$.

Sia $F$ la funzione di ripartizione di cui vogliamo controllare quale sia la possibile distribuzione della popolazione $X$.
Dividiamo il supporto di $F$ in un numero finito di intervalli che formino una /partizione del supporto/ stesso. Denotiamo con
$I_k = [t_k, t_{k+1})$ gli intervalli così ottenuti per $k = 1, \dots, K+1$. Qui $K$ rappresenta il numero di intervalli in cui abbiamo
diviso il supporto di $F$, mentre i valori $t_k$ sono gli estremi che delimitano gli intervalli. Per ogni $k = 1, \dots, K$
consideriamo poi le seguenti quantità:
- $n_k =$ numero di elementi $X_i$ del campione che cadono in $I_k$;
- $p_k =$ probabilità che il singolo $X_i$ cada in $I_k$ se $H_0$ è vera.
$$p_k = P(X \in I_k \mid X \approx F) = F(t_{k+1}) - F(t_k)$$
Le quantità $n_k$ sono dette /frequenze osservate degli intervalli $I_k$/, mentre le $p_k$ sono dette /probabilità teoriche/.
Notiamo che per ogni $k = 1, \dots, K$, il prodotto $n p_k$ fornisce un numero atteso di elementi del campione che dovrebbe cadere
in $I_k$. Pertanto se $H_0$ è vera, le differenze $n_k - np_k$ dovrebbero essere piccole in valore assoluto.
Consideriamo infine la statistica
$$W = \sum_{k=1}^K \frac{(n_k - np_k)^2}{np_k}$$
Si può dimostrare che quando $H_0$ è vera e quando le $n_k$ sono sufficientemente grandi (almeno $\geq$ 5), la $W$ è approssimativamente
distribuita come una Chi-Quadro con:
- $K-1$ gradi di libertà se la funzione di ripartizione $F$ è stata decisa arbitrariamente senza fare uno presentivo dei dati campionari;
- $K-r-1$ gradi di libertà se nella funzione di ripartizione $F$ compaiono $r$ parametri che sono stati stimati facendo uso dei
  dati campionari.
In base alle considerazioni appena fatte saremo portati a rifiutare l'ipotesi nulla quando $W$ assume valori troppo lontani dallo 0
per essere una Chi-Quadro con opportuni gradi di libertà.

Specificamente, per ogni livello di significatività $\alpha$, la regione critica per $W$ risulta essere $C = (\chi_{1-\alpha}^2, +\infty)$, dove $\chi_{1-\alpha}^2$
rappresenta il quantile di ordine $(1-\alpha)$ della Chi-Quadro con $K-1$ o $K-r-1$ gradi di libertà a seconda che i valori dei
parametri di $F$ siano stati stimati con i dati campionari oppure no.

Riprendiamo in considerazione l'esempio precedente e come in precedenza controlliamo l'ipotesi che la popolazione segua una
distribuzione esponenziale di parametro $0.35$ con ampiezza $\alpha = 0.05$, facendo uso del test Chi-Quadro.
$$(0.9, 1.2, 1.5, 2.2, 2.8, 3.0, 3.2, 4.1, 5.5, 6.0)$$
Il supporto di $F$ in questo caso può essere suddiviso in 3 intervalli: $I_1 = [0, 2)$, $I_2 = [2, 4)$ e $I_3 = [4, +\infty)$.
Per questi tre intervalli avremo le seguenti frequenze osservate: $n_1 = 3$, $n_2 = 4$ e $n_3 = 3$. Notiamo che il numero di
elementi per ogni classe è inferiore di 5, mentre il metodo può essere utilizzato solo se tale numero è maggiore o uguale a 5.

Pertanto è necessario o diminuire il numero delle classi o aumentare il campione. Supponiamo di poter aumentare la numerosità del
campione, arrivando ad $n=20$ e supponiamo che con l'aggiunta dei nuovi 10 casi, il campione assuma valori
$(3.0, 4.1, 2.8, 5.5, 1.5, 2.2, 6.0, 1.2, 3.2, 0.9)$ e $(2.9, 1.7, 4.8, 4.3, 2.0, 7.1, 5.4, 0.9, 1.3, 3.4)$.

Allora avremo $n_1 = 6 \quad p_1 = F(2) - F(0) = 0.50$, $n_2 = 7 \quad p_2 = F(4) - F(2) = 0.25$ ed $n_3 = 7 \quad p_3 = F(+\infty) - F(4) = 0.25$,
utilizzando $F(t) = 1 - e^{-0.35 t}$.

Di conseguenza, la statistica $W$ assume valore
\begin{align*}
W &= \frac{(n_1 - 20p_1)^2}{20p_1} + \frac{(n_2 - 20p_2)^2}{20p_2} + \frac{(n_3 - 20p_3)^2}{20p_3}\\
&= \frac{((6 - 20 \cdot 0.5))^2}{20 \cdot 0.5} + \frac{((7 - 20 \cdot 0.25))^2}{20 \cdot 0.25} + \frac{((7 - 20 \cdot 0.25))^2}{20 \cdot 0.25}\\
&= 3.2
\end{align*}
Poiché il valore del parametro $\lambda$ della distribuzione esponenziale che stiamo controllando quale possibile distribuzione di $X$
non è stato ricavato con stime campionarie, ma arbitrariamente, nel nostro caso la statistica $W$ è distribuita coma una Chi-Quadro
con $K - 1 = 2$ gradi di libertà.

La regione critica si ricava cercando nelle tavole della Chi-Quadro, nella riga relativa ai 2 gradi di libertà, che per $\alpha = 0.05$
offre $C = (5.99, +\infty)$, da cui segue il non rifiuto dell'ipotesi nulla.

Ripetiamo il test effettuato nell'esempio precedente, sempre con ampiezza $\alpha = 0.05$, controllando questa volta l'ipotesi che la
popolazione segua una distribuzione esponenziale il cui parametro $\lambda$ viene stimato utilizzando i dati del campione
$(X_1, \dots, X_{20})$. Per stimare $\lambda$ potremmo usare il metodo di massima verosimiglianza, oppure ricordare che $\lambda = \frac{1}{E[X]}$
e quindi ricavarlo stimando $E[X]$ con la media campionaria $\bar{X}_{20} = 3.075 \implies \lambda = 0.325$.

Suddividiamo ora il supporto di $F$ negli stessi 3 intervalli in cui era suddiviso in precedenza, ovvero gli intervalli
$I_1 = [0, 2)$, $I_2 = [2, 4)$ e $I_3 = [4, +\infty)$.

In questo caso avremo $n_1 = 6 \quad p_1 = F(2) - F(0) = 0.48$, $n_2 = 7 \quad p_2 = F(4) - F(2) = 0.25$ ed $n_3 = 7 \quad p_3 = F(+\infty) - F(4) = 0.27$.

Di conseguenza la statistica $W$ assume valore
\begin{align*}
W &= \sum_{i=1}^3 \frac{(n_i - 20 \cdot p_i)^2}{20 \cdot p_i}\\
&= \frac{(6 - 20 \cdot 0.48)^2}{20 \cdot 0.48} + \frac{(4 - 20 \cdot 0.25)^2}{20 \cdot 0.25} + \frac{(7 - 20 \cdot 0.27)^2}{20 \cdot 0.27}\\
&= 2.62
\end{align*}
Poiché il valore del parametro $\lambda$ è stato ricavato da stime campionarie, allora questa volta la statistica $W$ è distribuita
come una Chi-Quadro con $K-r-1 = 3-1-1 = 1$ gradi di libertà.

Qui $r=1$ poiché 1 è il numero di parametri di $F$ che sono stati stimati.

La regione critica si trova andando a cercare sulle tavole della Chi-Quadro, nella riga relativa ad 1 grado di libertà, ottenendo
così per $\alpha = 0.05$ la regione critica $C = (3.84, +\infty)$ ($W = 2.62$), da cui segue il non rifiuto dell'ipotesi nulla.

Notiamo che partendo dagli stessi dati campionari negli ultimi 2 esempi, abbiamo accettato due ipotesi diverse, ovvero abbiamo
accettato inizialmente l'ipotesi che $X$ fosse distribuita secondo un'esponenziale di parametro $\lambda = 0.35$ e poi che fosse
distribuita secondo un'esponenziale di parametro $\lambda = 0.325$. In effetti però occorre segnalare che in entrambi i casi non
abbiamo realmente accettato le ipotesi nulle, bensì ci siamo limitati a non rifiutarle.

Dall'ultimo esempio abbiamo poi rilevato che una prima importante differenza tra il test di Kolmogorov-Smirnov ed il test Chi-Quadro
è che il secondo necessita di un campione di maggiori dimensioni rispetto al primo per poter essere applicato. In compenso però,
il test Chi-Quadro può essere utilizzato anche quando la distribuzione $F$ da controllare non è continua.

Per esempio, un nostro collaboratore sostiene che non sussista relazione tra il momento in cui viene presentata la domanda per la
concessione di una ristrutturazione ed il momento in cui questa viene approvata. Egli sostiene invece che il comune rilascia
mensilmente un numero di concessioni limitato e distribuito secondo una legge di Poisson indipendentemente da quante domande
sono state presentate e quando. Per questa ragione decidiamo di controllare la sua ipotesi andando a rilevare il numero di
concessioni rilasciate negli ultimi 24 mesi, ottenendo i seguenti dati:
$$(3, 5, 2, 8, 3, 4, 2, 3, 0, 5, 7, 5, 3, 2, 1, 1, 2, 4, 0, 4, 6, 2, 3, 1)$$
e di controllare l'ipotesi del nostro collaboratore con il metodo del Chi-Quadro con livello di significatività $\alpha = 0.05$.

Decidiamo di stimare il valore del parametro $\lambda$ della distribuzione di Poisson facendo uso dei dati del campione, ricordando
che per tale distribuzione vale $\lambda = E[X]$. Possiamo pertanto utilizzare $\bar{X}_{24}$ come stima puntuale di $\lambda$, ottenendo il
valore $\bar{x}_{24} = 3.17$.

Decidiamo poi di dividere il supporto della distribuzione di Poisson in 4 intervalli, scegliendoli in modo che le corrispondenti
frequenze osservare $f_k$ siano sufficientemente grandi.

Si decide per esempio di considerare i seguenti intervalli: $I_1 = \{0, 1\}$, $I_2 = \{2\}$, $I_3 = \{3\}$, $I_4 = \{4, \dots, +\infty\}$,
ai quali corrispondono le seguenti frequenze: $n_1 = 5$, $n_2 = 5$, $n_3 = 5$ e $n_4 = 9$.

Determiniamo ora le probabilità teoriche di ogni intervallo. Supponiamo per questo che sia vera l'ipotesi nulla, ovvero che $X$
sia distribuita secondo una Poisson di parametro $\lambda = 3.17$ e quindi sia $P(X = k) = \frac{\lambda^k}{k!} e^{-\lambda} = \frac{(3.17)^k}{k!}e^{-3.17}$
per ogni $k \in \mathbb{N}$.

Pertanto avremo
\begin{gather*}
p_1 = P(X=0 \text{ oppure } X=1) = P(X=0) + P(X=1) =\\
=\frac{(3.17)^0}{0!}e^{-3.17} + \frac{(3.17)^1}{1!}e^{-3.17} = 0.175\\
p_2 = P(X=2) = \frac{(3.17)^2}{2!}e^{-3.17} = 0.211\\
p_3 = P(X=3) = \frac{(3.17)^3}{3!}e^{-3.17} = 0.223\\
p_4 = 1 - P(X=0 \text{ oppure } X=1 \text{ oppure } X=3) = 0.391
\end{gather*}
La statistica $W$ vale quindi $w = \sum_{i=1}^4 \frac{(n_i - 24 \cdot p_i)^2}{24 \cdot p_i} = 0.17$.

Poiché il valore del parametro $\lambda$ è stato ricavato da stime campionarie, allora la statistica $W$ risulta essere distribuita
come una Chi-Quadro con $K-r-1 = 4-1-1 = 2$ gradi di libertà. Abbiamo posto $r=1$ in quanto 1 è il numero di parametri di $F$
che sono stati stimati facendo uso del campione.

La regione critica si trova andando a cercare sulle tavole della Chi-Quadro, nella riga relativa a 2 gradi di libertà, ottenendo
così per $\alpha = 0.05$, la regione critica $C = (5.99, +\infty)$ ($w = 0.17$).
*** Test per il confronto delle distribuzioni di due popolazioni
Di seguito vengono presentati 3 test per verificare o rifiutare l'ipotesi che le distribuzioni di due distinte popolazioni $X$ ed $Y$
siano identiche (non necessariamente gaussiane).

Denotate con $F_X$ la distribuzione di $X$ e con $F_Y$ la distribuzione di $Y$, sia $H_0 : F_X(t) = F_Y(t)$ per ogni $t \in \mathbb{R}$.

Quale ipotesi alternativa considereremo sempre e soltanto $H_1 : F_X(t) \neq F_Y(t)$ per almeno un $t \in \mathbb{R}$.

Il primo di questi metodi prende il nome di /test dei segni/, e si basa sui segni negativi o positivi delle differenze tra le
coppie di elementi presi dai campioni estratti dalle due popolazioni considerate.

Siano $(X_1, \dots, X_n)$ e $(Y_1, \dots, Y_n)$ due /campioni appaiati/, estratti da $X$ ed $Y$ rispettivamente. Questo significa che ogni
coppia $(X_i, Y_i)$ è relativa allo stesso individuo o elemento (es.: il livello $X_i$ di colesterolo di un paziente prima di aver
ricevuto una cura ed il livello di colesterolo $Y_i$ dello stesso paziente dopo aver ricevuto la cura). Contiamo allora quante
volte si verifica $X_i > Y_i$, $X_i < Y_i$ e $X_i = Y_i$.

Siano:
- $S^+ = \text{numero di volte che \(X_i > Y_i\)}$;
- $S^- = \text{numero di volte che \(X_i < Y_i\)}$;
- $S^= = \text{numero di volte che \(X_i = Y_i\)}$.

Se l'ipotesi nulla è vera, allora è logico aspettarsi che le quantità $S^+$ ed $S^-$ non si discostino molto tra loro. Infatti
se l'ipotesi nulla è vera e se $n-S^=$ è sufficientemente grande ($n - S^= \geq 10$), la quantità $S_n = S^+ - S^-$ risulta essere
normalmente distribuita con $\mu = 0$ e $\sigma^2 = \frac{n - S^=}{2}$.

Si può quindi costruire una regione critica per $S_n$ ragionando nel solito modo, rifiutando $H_0$ quando $S_n$ si discosta troppo
dallo 0 per essere una normale $N\left(0, \sqrt{\frac{n-S^=}{2}}\right)$.
Specificamente, per un fissato livello di significatività $\alpha$, la regione critica per $S_n$ risulta essere
$C = \left(-\infty, -z_{1 - \frac{\alpha}{2}}\right) \cup \left(+z_{1 - \frac{\alpha}{2}}, +\infty\right)$, dove $z_{1-\frac{\alpha}{2}}$ è il quantile di ordine $1 - \frac{\alpha}{2}$
della normale standardizzata.

È possibile effettuare il test anche quando la quantità $n - S^=$ è piccola. In questo caso si può considerare la statistica
$S^+$ e tener conto del fatto che quando sussiste l'ipotesi nulla, essa è distribuita come una Binomiale di parametri
$n - S^=$ e $\frac{1}{2}$.

Segnaliamo l'esistenza di un test simile a quello dei segni ma più elaborato, nel senso che tiene conto non solo dei segni
delle differenze ma anche delle ampiezze di queste differenze, e che porta il nome di /test dei segni di Wilcoxon/.

Per esempio, facciamo riferimento al problema già descritto nel capitolo precedente, del confronto tra i redditi medi annui
delle famiglie di due diversi quartieri di una data città. Ora trascuriamo però l'assunto che tali redditi siano normalmente
distribuiti e domandiamoci se è ammissibile pensare che le due distribuzioni dei redditi siano identiche, utilizzando a tale
scopo il test dei segni appena introdotto, con un livello di significatività $\alpha = 0.10$.

Sia $X$ la popolazione del primo quartiere ed $Y$ quella del secondo quartiere.

I campioni riportati nell'esempio citato non sono adatti a questo test, poiché non hanno la stessa numerosità. Per tale ragione,
del secondo campione considereremo ora solo i primi 10 dati, ottenendo: $(22, 48, 51, 20, 28, 35, 39, 26, 50, 36)$ e
$(40, 42, 50, 26, 30, 34, 37, 28, 25, 30)$.

Per le 10 coppie considerate i segni delle differenze $x_i - y_i$ che sono riportati nella tabella sotto:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 15:11:55
[[file:Lezioni/screenshot_2018-06-08_15-11-55.png]]

Necessario sottolineare che i campioni devono essere appaiati, in altre parole la coppia $(X_i, Y_i)$ deve essere relativa alla
stessa unità, entità o individuo. Nel nostro caso potremmo ipotizzare che l'indice $i$ sia relativo allo stesso nucleo
famigliare che viveva in precedenza nel quartiere della città, associato alla popolazione $X$ e poi andato a vivere in un
quartiere della città associato alla popolazione $Y$.

La quantità $X_i - Y_i$ misura la differenza di reddito tra la famiglia $i$, quando abitava nel quartiere $X$, e adesso che si
è trasferita nel quartiere $Y$ (misura il prima - dopo, sebbene impropriamente detto).

Abbiamo quindi: $S^+ = 6$, $S^- = 4$ e $S^= = 0$.

La statistica $S_{10} = S^+ - S^- = 2$.

Notiamo che per $\alpha = 0.10$, la regione critica del test, usando un'approssimazione alla normale, risulta essere
$C = \left(-\infty, -z_{0.95}\sqrt{\frac{10}{2}}\right) \cup \left(+z_{0.95}\sqrt{\frac{10}{2}}, +\infty\right) = (-\infty, -3.67) \cup (+3.67, +\infty)$.

Pertanto non possiamo rifiutare l'ipotesi che le due distribuzioni siano le stesse, infatti il valore della statistica non
appartiene alla regione critica.

Il secondo test per il confronto delle distribuzioni di due popolazioni che ora consideriamo è il /test dei ranghi di Wilcoxon/
o /test U di Mann-Whitney/.

Fornire un'espressione funzionale della statistica su cui questo test è basato è abbastanza complesso, pertanto ci limiteremo
a descrivere il procedimento da seguire per effettuare il test senza cercare di fornire un'interpretazione anche solo
intuitiva di esso. Sottolineiamo comunque che rispetto al test dei segni, ha il vantaggio di essere più potente ed utilizzabile
anche per campioni non appaiati.

Siano quindi $(X_1, \dots, X_n)$ e $(Y_1, \dots, Y_m)$ due campioni di numerosità $n$ ed $m$ estratti da $X$ ed $Y$ rispettivamente.
Le fasi della procedura del test sono le seguenti:
- Si ordina in senso crescente l'insieme di tutti i dati e si associa a ciascun dato il proprio rango, ovvero la posizione
  in cui si trova nella sistemazione in ordine crescente dei dati;
- Si sommano separatamente i ranghi relativi ai due campioni;
- Si calcolano le statistiche $U_X = n \cdot m + \frac{n \cdot (n+1)}{2} - R_X$ e $U_Y = n \cdot m + \frac{m \cdot (m+1)}{2} - R_Y$.
  
  Se i conti sono corretti la somma di $U_X$ ed $U_Y$ deve essere uguale al prodotto tra le numerosità dei due campioni, vale
  a dire che deve essere $U_X + U_Y = n \cdot m$;
- Si considera poi la statistica $U = \text{min}\{U_X, U_Y\}$.

  Si può dimostrare che per $n$ ed $m$ sufficientemente grandi (in genere maggiori di 8), quando vale $H_0$, la $U$ è approssimabile
  tramite una normale con parametri $\mu_U = \frac{n \cdot m}{2}$ e $\sigma_U^2 = \frac{n \cdot m \cdot (n + m + 1)}{12}$.

  Vale quindi $\hat{Z}_{n, m} = \frac{U - \frac{n \cdot m}{2}}{\sqrt{\frac{n \cdot m . (n+m+1)}{12}}} \sim N(0, 1)$.

  Per definire la regola di decisione del test si procede nel solito modo, ovvero fissata l'ampiezza $\alpha$ del test, la regione
  critica risulta essere $C = \left(-\infty, -z_{1 - \frac{\alpha}{2}}\right) \cup \left(+z_{1 - \frac{\alpha}{2}}, +\infty\right)$, dove $z_{1-\frac{\alpha}{2}}$ è il
  quantile di ordine $1 - \frac{\alpha}{2}$ della normale standardizzata.

  Nel caso in cui non sia verificata la condizione $n, m > 8$, allora è possibile ricorrere ad apposite tavole per determinare
  la regione critica per $U$.
  
Riconsideriamo ancora il problema citato nell'ultimo esempio, del confronto tra i redditi medi annui delle famiglie di due
diversi quartieri di una città. Come nell'esempio precedente trascuriamo l'assunto che tali redditi siano normalmente
distribuiti e domandiamoci se è ammissibile pensare che le due distribuzioni dei redditi siano identiche. Utilizziamo questa
volta il test dei ranghi appena descritto, sempre con un livello di significatività $\alpha = 0.10$ e, poiché non è necessario che
i due campioni abbiano egual numerosità, riconsideriamo i campioni così come erano stati dati originariamente, vale a dire:
$(22, 48, 51, 20, 28, 35, 38, 26, 50, 36)$ e $(40, 42, 50, 26, 30, 34, 37, 28, 25, 30, 32, 38, 22, 55, 40)$.

Nella tabella sottostante vengono riportati i ranghi associati a ciascuno dei dati che compaiono nei due campioni:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 15:36:38
[[file:Lezioni/screenshot_2018-06-08_15-36-38.png]]

Notiamo che tra i dati alcuni valori compaiono più volte. In questo caso i loro ranghi sono definiti come il valor medio tra le
loro posizioni in ordine crescente. Sommiamo ora i ranghi di ciascuno dei due campioni ottenendo $r_X = 127.5$ e $r_Y = 197.5$

Calcoliamo poi le realizzazioni delle statistiche $U_X$ ed $U_Y$, ottenendo $u_X = 10 \cdot 15 + \frac{10 \cdot (10 + 1)}{2} - r_X = 77.5$ e
$u_Y = 10 \cdot 15 + \frac{15 \cdot (15 + 1)}{2} - r_Y = 72.5$.

Notiamo che vale $u_X + u_Y = 10 \cdot 15 = 150$.

La realizzazione di $U = \text{min}\{U_X, U_Y\}$ risulta essere il valore $u = 72.5$.

Poiché i due campioni sono entrambi di numerosità maggiore di 8, si può approssimare $U$ con una normale con
$\mu_U = \frac{10 \cdot 15}{2} = 75$ e $\sigma_U = \sqrt{\frac{10 \cdot 15 \cdot (10 + 15 + 1)}{12}} = 18$.

A questo punto consideriamo la $\hat{Z}_{10, 15} = \frac{U - 75}{18}$ che assume valore $\hat{z}_{10, 15} = 0.14$.

Pertanto accetteremo (non rifiuteremo) l'ipotesi $H_0$, essendo $C = (-\infty, -1.65) \cup (1.65, +\infty)$ la regione critica con un
livello di significatività $\alpha = 0.10$.

Il terzo test per il confronto delle distribuzioni due due popolazioni che ora presentiamo è un /adattamento del test di
Kolmogorov-Smirnov/ al caso di due campioni.

Questo adattamento non richiede che i due campioni siano appaiati, cioè con pari numerosità ed estratti da $X$ ed $Y$,
ma in compenso può essere utilizzato solo quando ci siano validi motivi per pensare che la distribuzione delle popolazioni
da confrontare sia continua.

Siano quindi $(X_1, \dots, X_n)$ e $(Y_1, \dots, Y_m)$ due campioni di numerosità $n$ ed $m$ estratti da $X$ ed $Y$ rispettivamente. Siano
poi $F_{X, n}$ ed $F_{Y, m}$ le funzioni di ripartizione empiriche di $X$ ed $Y$ ottenute con i campioni e sia
$$D_{n, m} = \text{sup}_{t \in \mathbb{R}}|F_{X, n}(t) - F_{Y, m}(t)|$$
la statistica che specifica l'estremo superiore delle distanze, in valore assoluto, tra le due funzioni di ripartizione
empiriche. Anche in questo caso si può dimostrare che quando $H_0$ è vera e quando $F_X$ è una funziona continua, allora $D_{n, m}$
è indipendente dalla forma di $F_X$, ovvero ha la stessa distribuzione qualunque sia $F_X$ (ma varierà al variare di $n$).

Anche per la distribuzione della statistica campionaria $D_{n, m}$, al variare di $n$ ed $m$, esistono apposite tabelle per
determinare i quantili. È quindi possibile fissare, al solito, delle regioni di accettazione e critiche al variare di $n$
ed $m$ e del livello di significatività.

Specificamente rifiuteremo $H_0$ quando $D_{n, m}$ assume valori "grandi", cioè quando cade nella regione critica $C = (d_{1 - \alpha}, 1]$,
dove il quantile $d_{1 - \alpha}$ è quel valore per cui risulta $P(D_{n, m} \leq d_{1 - \alpha}) = 1 - \alpha$, che può essere determinato tramite le
apposite tavole.
*** Test di indipendenza
Un problema che si pone frequentemente nelle applicazioni è quello di stabilire se due caratteri di una popolazione bidimensionale
sono tra loro stocasticamente indipendenti oppure no. Per rispondere a questa domanda sono stati inventati diversi test, ma uno
in particolare viene sempre utilizzato. Esso prende il nome di /test del Chi-Quadro per l'indipendenza/, e come vedremo,
ricorda molto, nella sua formulazione, il test del Chi-Quadro per la bontà dell'adattamento.

Si consideri una popolazione bidimensionale $(X, Y)$ e si supponga di poter estrarre da essa un campione casuale
$((X_1, Y_1), \dots, (X_n, Y_n))$. Si vuole controllare, con livello di significatività $\alpha$, l'ipotesi :
- $H_0 : \text{ i caratteri \(X\) ed \(Y\) sono indipendenti}$,
considerando quale ipotesi alternativa quella ovvia, ovvero,
- $H_1 : \text{ i caratteri \(X\) ed \(Y\) non sono indipendenti}$.

Per effettuare il test del Chi-Quadro, occorre inizialmente fare una partizione dei due supporti dei caratteri $X$ ed $Y$ in
un numero finito di intervalli ciascuno. Denotiamo con $I_k^X$, per $k = 1, \dots, M_X$ ed $I_j^Y$, per $j = 1, \dots, M_Y$, gli
intervalli che formano una partizione del supporto di $X$ ed $Y$.

Per ogni coppia $(k, j)$ consideriamo le seguenti quantità:
- $n_k^X = \text{ numero di elementi \(X_i\) del campione che cadono in } I_k^X$;
- $n_j^Y = \text{ numero di elementi \(Y_i\) del campione che cadono in }I_j^Y$;
- $n_{k, j} = \text{ numero di elementi \((X_i, Y_i)\) del campione che cadono in } I_k^X \times I_j^Y$;
- $f_k^X = \frac{n_k^X}{n} = \text{ frequenza relativa di }I_k^X$;
- $f_j^Y = \frac{n_j^Y}{n} = \text{ frequenza relativa di } I_j^Y$;
- $f_{k, j} = \frac{n_{k, j}}{n} = \text{ frequenza relativa di } I_k^X \times I_j^Y$.
  
Le quantità $f_{k, j}$ sono dette /frequenze relative osservate/ delle regioni $I_k^X \times I_j^Y$.

Notiamo che per ogni coppia $(k, j)$, il prodotto $f_k^X \cdot f_j^Y$ fornisce invece una /frequenza relativa attesa/ di elementi del
campione che dovrebbero cadere in $I_k^X \times I_j^Y$ se fosse vera l'ipotesi nulla $H_0$, poiché in questo caso la frequenza di ogni regione
deve essere uguale al prodotto delle frequenze marginali di quella regione.

Se $H_0$ è vera quindi, le differenze $f_{k, j} - f_k^X \cdot f_j^Y$ dovrebbero essere piccole in valore assoluto.

Consideriamo infine la statistica $W = n \cdot \sum_{k=1}^{M_X}\sum_{j=1}^{M_Y} \frac{(f_{k, j} - f_k^X \cdot f_j^Y)^2}{f_k^X \cdot f_j^Y}$.

Si può mostrare che quando l'ipotesi nulla è vera e quando le $n_{k, j}$ sono sufficientemente grandi, almeno maggiori o uguali a 5,
allora $W$ è approssimativamente distribuita come una Chi-Quadro con $(M_X - 1) \cdot (M_Y - 1)$ gradi di libertà.

La regola di decisione del test segue in base alle considerazioni fatte sopra, saremo portati a rifiutare l'ipotesi nulla
quando la $W$ assume valori troppo lontani dallo 0 per essere una Chi-Quadro con opportuni gradi di libertà. Specificamente
per ogni livello di significatività $\alpha$ la regione critica per $W$ risulta essere $C = (\chi_{1 - \alpha}^2, +\infty)$, dove $\chi_{1-\alpha}^2$ rappresenta
il quantile di ordine $(1 - \alpha)$ della Chi-Quadro con $(M_X - 1)(M_Y - 1)$ gradi di libertà.

Per esempio, considerata la popolazione costituita dai nuclei familiari residenti in un quartiere di una data città, si
supponga di voler stabilire se esiste una dipendenza tra i due caratteri:
- $X = \text{ reddito medio annuo della famiglia (in migliaia)}$;
- $Y = \text{ età media del nucleo famigliare}$.
A tale scopo si effettua un'indagine su un totale di 200 nuclei familiari scelti in modo casuale tra quelli del quartiere
andando ad osservare, per ogni nucleo, i valori di questi due caratteri.

Come risultato dell'indagine si ottiene la tabella sotto riportata:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 17:36:00
[[file:Lezioni/screenshot_2018-06-08_17-36-00.png]]

Viene spontaneo, in questo caso, considerare le cinque classi in cui è stato suddiviso il supporto del carattere $X$ e le quattro
classi in cui è stato suddiviso il supporto del carattere $Y$, come agli intervalli su cui costruire il test. Avremo
$M_X = 5$ ed $M_Y = 4$. Inoltre avremo $I_1^X = [0, 20] \quad I_2^X = (20, 30] \quad I_3^X = (30, 40] \quad I_4^X = (40, 50] \quad I_5^X = (50, \infty]$
ed $I_1^Y = [0, 20] \quad I_2^Y = (20, 30] \quad I_3^Y = (30, 50] \quad I_4^Y = (50, \infty]$.

Le frequenze assolute $n_{k, j}$ sono quindi quelle che compaiono nella tabella introdotta in precedenza. Notiamo che in tale tabella
sono riportate anche le frequenze assolute marginali dei due caratteri. Poiché le frequenze assolute
osservate assumono sempre valore non inferiore
a 5, non abbiamo problemi ad applicare il metodo del Chi-Quadro per effettuare il test.

Calcoliamo quindi le frequenze relativa, ottenendo quanto riportato nella tabella seguente:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 17:42:04
[[file:Lezioni/screenshot_2018-06-08_17-42-04.png]]

Questa tabella contiene tutto quanto necessario per calcolare la realizzazione della statistica $W$ su cui si base il test.
In essa, infatti, si trovano le quantità $f_{k, j}, f_k^X, f_j^Y$ per ogni $k = 1, \dots, 5$ e $j = 1, \dots, 4$.

Si ottiene allora il seguente valore: $W = 200 \cdot \sum_{k=1}^5 \sum_{j=1}^4 \frac{(f_{k, j} - f_k^X \cdot f_j^Y)^2}{f_k^X \cdot f_j^Y} = 26.4$.

Supponiamo di aver fissato un livello di significatività $\alpha = 0.05$ per il test, allora per determinare l'estremo $\chi_{1-\alpha}^2$ della
regione critica, occorre cercare il quantile di ordine 0.95 relativo alla Chi-Quadro con $(5-1)(4-1) = 12$ gradi di libertà.
Essendo in questo caso $\chi_{0.95}^2 = 21$ la regione critica per $W$, risulta essere $C = (21, +\infty)$, portando a rifiutare l'ipotesi
nulla che i due caratteri siano indipendenti.

Come nel caso del test Chi-Quadro per la bontà dell'adattamento, anche il test Chi-Quadro per l'indipendenza ha il vantaggio di
essere utilizzabile quando si considerano caratteri con distribuzione non continue.

Addirittura esso può essere utilizzato considerando caratteri con modalità non numeriche.

Ora riconsideriamo come popolazione quella costituita dai nuclei familiari residenti nel solito quartiere della solita città.
Si supponga di voler stabilire se esiste una dipendenza tra i due caratteri:
- $X = \text{ reddito medio annuo della famiglia (in migliaia)}$;
- $Y = \text{ titolo di studio del capofamiglia}$.
A tale scopo si effettua una nuova indagine su un totale di 100 nuclei familiari scelti in modo casuale tra quelli del
quartiere andando ad osservare per ogni nucleo i valori di questi due caratteri.

Come risultato dell'indagine si ottiene la tabella sotto riportata:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 17:55:03
[[file:Lezioni/screenshot_2018-06-08_17-55-03.png]]

Viene spontaneo in questo caso considerare le 5 classi in cui è stato suddiviso il supporto del carattere $X$ ed alle 4 in cui
è stato suddiviso il supporto del carattere $Y$, come agli intervalli su cui costruire il test. Avremo $M_X = 5$ e $M_Y = 4$.

In questo caso però vi sono diverse frequenze assolute osservate che hanno valore minore di 5. Occorre pertanto raggruppare
ulteriormente le osservazioni in modo da avere tutte frequenze osservare maggiori o uguali a 5.

Nella tabella seguente è proposto un nuovo raggruppamento che soddisfa la condizione richiesta:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 17:58:00
[[file:Lezioni/screenshot_2018-06-08_17-58-00.png]]

Utilizzando questo raggruppamento per effettuare il test, avremo allora $M_X = 3$ ed $M_Y = 4$.

Inoltre, avremo $I_1^X = [0, 30] \quad I_2^X = (30, 50] \quad I_3^X = (50, \infty)$ e
$I_1^Y = \text{media inferiore} \quad I_2^Y = \text{media superiore} \quad I_3^Y = \text{maturità} \quad I_4^Y = \text{laurea}$.

Le frequenze assolute $n_{k, j}$ sono quindi quelle che compaiono nella tabella introdotta in precedenza. Notiamo che in tale
tabella sono riportate anche le frequenze assolute marginali dei due caratteri.

Poiché le frequenze assolute osservate assumono sempre valore non inferiore a 5, non abbiamo problemi ad applicare il metodo del
Chi-Quadro per effettuare il test.
Calcoliamo quindi le frequenze relativa ottenendo quanto riportato nella tabella seguente:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 18:03:57
[[file:Lezioni/screenshot_2018-06-08_18-03-57.png]]

Si ottiene allora il seguente valore: $W = 100 \cdot \sum_{k=1}^3 \sum_{j=1}^4 \frac{(f_{k, j} - f_k^X \cdot f_j^Y)^2}{f_k^X \cdot f_j^Y} = 3.15$.

Supponiamo di aver fissato un livello di significatività $\alpha = 0.05$ per il test, allora per determinare l'estremo $\chi_{1-\alpha}^2$ della
regione critica, occorre cercare il quantile di ordine 0.95 relativo alla Chi-Quadro con $(3-1)(4-1) = 6$ gradi di libertà.
Essendo in questo caso $\chi_{0.95}^2 = 12.6$, la regione critica per $W$ risulta essere $C = (12.6, +\infty)$ ($w = 3.15$) che non consente
di rifiutare l'ipotesi nulla che sussista indipendenza tra i redditi medi annui familiari ed i titoli di studio.
*** Test di incorrelazione
Il test di incorrelazione che presentiamo ora non è basato sul coefficiente di correlazione lineare di Pearson, ma su una
statistica campionaria che porta il nome di /coefficiente di correlazione dei ranghi $R_S$ di Spearman/. Esso viene incluso
nei test di tipo non-parametrico in quanto la determinazione di $R_S$ non coinvolge direttamente i valori numerici assunti
dai dati campionari, ma solo i loro ranghi di cui viene fornita la definizione.

Consideriamo un campione $((X_1, Y_1), \dots, (X_n, Y_n))$ di numerosità $n$ estratto in modo casuale dalla popolazione bidimensionale
$(x, y)$. Ordiniamo in senso crescente prima l'insieme dei dati di tipo $x_i$ e successivamente quelli di tipo $y_i$.

È detto rango di ciascun dato la posizione che esso assume nella sequenza così ottenuta di dati dello stesso tipo.

Ad ogni coppia $(x_i, y_i)$ associamo ora la corrispondente coppia di ranghi $(r_i^x, r_i^y)$ e denotiamo con $d_i$ le loro
differenze $d_i = r_i^x - r_i^y$.

è detto /coefficiente di correlazione dei ranghi di Spearman/ la statistica
$$R_s = 1 - \frac{6 \cdot \sum_{i=1}^n d_i^2}{n^3 - n}$$
Il coefficiente $R_S$ soddisfa proprietà simili a quelle di cui gode il coefficiente di correlazione lineare di Pearson e
consente di definire un nuovo tipo di incorrelazione (che chiameremo incorrelazione nel senso di Spearman) che si ha quando
$R_S$ assume valore zero.

La possibilità di utilizzare $R_S$ in un test di ipotesi deriva dal fatto che indipendentemente dalla forma delle distribuzione
congiunte $(X, Y)$, quando i caratteri $X$ ed $Y$ sono incorrelati, nel senso di Spearman, e la numerosità del campione è maggiore
di 10, allora la statistica
$$\tilde{T}_n = R_S \cdot \sqrt{\frac{n-2}{1 - R_S^2}}$$
risulta essere approssimativamente distribuita come una t di Student con $(n-2)$ gradi di libertà.

Dovendo controllare l'ipotesi
- $H_0 : \text{ i caratteri \(X\) ed \(Y\) sono incorrelati secondo Spearman}$;
contro
- $H_1 : \text{ i caratteri \(X\) ed \(Y\) sono correlati secondo Spearman}$.
Possiamo pensare di non rifiutare $H_0$ quando $\tilde{T}_n$ assume valori non troppo distanti dallo 0 e di rifiutarla in favore di
$H_1$ in caso contrario.

Ragionando nel solito modo, si ottiene che per un livello di significatività $\alpha$, la regione critica per la statistica $\tilde{T}_n$
è la seguente: $C = \left(-\infty, -t_{1-\frac{\alpha}{2}}\right) \cup \left(+t_{1-\frac{\alpha}{2}}, +\infty\right)$ dove $t_{1 - \frac{\alpha}{2}}$ è il quantile di ordine
$1 - \frac{\alpha}{2}$ della t di Student con $(n-2)$ gradi di libertà.

Prima di mostrare un esempio di applicazione del test di incorrelazione basato sul coefficiente di Spearman, osserviamo che esso può
essere utilizzato alternativamente al test Chi-Quadro per verificare la dipendenza tra 2 caratteri di una popolazione.

Infatti, ricordiamo che l'indipendenza di due caratteri è una proprietà più forte dell'incorrelazione, nel senso che
l'indipendenza implica l'incorrelazione. Viceversa possiamo affermare che se non sussiste incorrelazione allora non sussiste
neanche l'indipendenza.

Disponendo di un campione di numerosità limitata, come visto, in genere non è possibile applicare il test Chi-Quadro; per tale
ragione si può allora provare a controllare l'incorrelazione tra i caratteri ed affermare che tra essi sussiste dipendenza
quando l'ipotesi nulla di incorrelazione viene rifiutata.

Per esempio, si riconsideri il problema presentato precedentemente relativo alla popolazione $(X, Y)$ dei redditi medi annui
(in migliaia) e dell'età media dei nuclei familiari di un quartiere di una data città.

Ora però si supponga di disporre solo di un campione di numerosità 10 la cui realizzazione è
$((21.4, 51), (23.0, 25), (40.5, 35), (57.0, 48), (30.0, 20))$ e
$((32.5, 22), (35.5, 68), (60.0, 41), (29.0, 33), (42.0, 27))$.

Con tale numerosità è impensabile effettuare un test Chi-Quadro per l'indipendenza poiché esso richiede frequenze osservate
maggiori o uguali a 5 per ogni classe.

Proviamo allora a chiederci solo se sussiste incorrelazione tra i due caratteri della popolazione utilizzando il coefficiente
di correlazione dei ranghi di Spearman e facendo un test con significatività $\alpha = 0.05$.

A tale scopo associamo i ranghi ai dati campionari e riportiamo il risultato nella tabella seguente:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-08 18:42:41
[[file:Lezioni/screenshot_2018-06-08_18-42-41.png]]

La correlazione $R_S$ di Spearman assume allora valore $r_S = 1 - \frac{6 \cdot \sum_{i=1}^n d_i^2}{10^3 - 10} = 0.212$, da cui si ricava la
realizzazione della statistica $\tilde{T}_n$: $\tilde{t}_{10} = r_S \cdot \sqrt{\frac{10-2}{1-r_S^2}} = 0.212 \cdot \sqrt{\frac{8}{0.995}} = 0.613$.

Controlliamo ora se il valore ottenuto dalla statistica consente di accettare l'ipotesi $H_0$ che non esiste correlazione tra i
due caratteri. Per questo è sufficiente notare che la regione critica del test, relativamente alla statistica $\tilde{T}_n$ è
la seguente: $C = (-\infty, -t_{0.975}) \cup (+t_{0.975}, +\infty) = (-\infty, -2.30) \cup (+2.30, +\infty)$, dove il quantile $t_{0.975} = 2.30$ è stato ricavato
dalla tavola della distribuzione t di Student sulla riga relativa ad 8 gradi di libertà.

Poiché il valore da noi osservato cade nella regione di accettazione del test, non possiamo rifiutare l'ipotesi che vi sia
incorrelazione tra reddito ed età.

Il fatto che non si possa escludere l'incorrelazione purtroppo non fornisce informazioni sull'indipendenza; infatti i caratteri
possono essere incorrelati ma non indipendenti.

Concludiamo con due brevi considerazioni sull'ultimo test descritto.

Abbiamo visto che esso può essere effettuato anche con campioni di piccole dimensioni. In effetti si può dire che oltre le 30
misure, l'aumento del tempo nella sua esecuzione non è compensato da un aumento della potenza del test. Bisognerebbe poi
considerare l'eventualità che alcuni dati compaiano più volte nel campione. In questo caso la definizione del coefficiente
$R_S$ va modificata con opportuni termini correttivi.

Come si può notare, per determinare il coefficiente $R_S$ non occorre conoscere esattamente i valori numerici dei dati campionari,
ma è succifiente poter definire un ordinamento tra essi (dovendo definire i ranghi). Per questa ragione, il coefficiente $R_S$
di Spearman viene talvolta utilizzato come indice di correlazione tra caratteri di tipo qualitativo per cui sia possibile
creare una relazione d'ordine nell'insieme di qualità da essi assumibili.

** Regressione Lineare
Consideriamo una popolazione $(m+1)$ dimensionale $(X_1, \dots, X_m, Y)$. In molti casi applicativi risulta particolarmente utile o
interessante stabilire se tra i caratteri della popolazione sussistano legami di dipendenza che descrivano uno di essi come
espressione funzionale degli altri, ovvero se esista una relazione del tipo $Y = f(X_1, \dots, X_m)$ dove la funzione $f$ può
essere indifferentemente deterministica o contenente parametri aleatori.

Trovarsi a conoscenza di una relazione di questo tipo consente infatti di risolvere molti problemi pratici soprattutto quando
si incontrano difficoltà a rilevare i valori assunti dal carattere $Y$. Si pensi ad esempio ad una popolazione costituita dagli
immobili di una grande città, al carattere $Y$ come al loro valore commerciale ad ai caratteri $X_k$ come ai valori associati a
diverse loro caratteristiche (anno di costruzione, distanza dal centro, metratura, ecc.). Solitamente è difficile fornire un
valore commerciale dell'immobile, mentre risulta facile determinare altre caratteristiche. Per tale ragione sarebbe utile
poter esprimere il valore $Y$ in funzione di quelli assunti dagli altri caratteri.

Osserviamo poi che è intuitivo pensare che tra il valore commerciale e le altre caratteristiche dell'immobile sussista una
relazione tipo quella introdotta in precedenza, ovvero $Y = f(X_1, \dots, X_m)$, seppure senza conoscerne l'espressione, ovvero la
forma della funzione $f$.

Un'equazione del tipo $Y = f(X_1, \dots, X_m)$ viene detta /equazione di regressione del carattere $Y$ rispetto ai caratteri
$X_1, \dots, X_m$/. È detta /variabile di risposta/, o dipendente, la $Y$, mentre sono dette /variabili esplicative/, o
indipendenti, o regressori, le $X_1, \dots, X_m$.

Con /analisi di regressione/ si intende invece la verifica dell'ammissibilità di una relazione del tipo $Y = f(X_1, \dots, X_m)$ e,
in caso affermativo, la determinazione di una stima della funzione $f$ che lega la variabile di risposta alle variabili
esplicative, o ad un sottoinsieme di esse.

Si noti che un'analisi di regressione deve essere effettuata facendo ricorso all'estrazione di un campione di numerosità finita,
e quindi verificando l'ammissibilità della relazione $Y = f(X_1, \dots, X_m)$ ricorrendo ai metodi della statistica inferenziale,
poiché, come al solito, non possiamo pensare di poter analizzare tutti gli individui appartenenti alla popolazione. Lo stesso
vale per la determinazione di una stima di $f$.

In questo capitolo ci limiteremo a considerare un caso particolare di equazione di regressione lineare a cui è dedicata vasta
letteratura statistica. È questo il caso in cui si suppone che la $f$ sia una funzione lineare nelle variabili esplicative
$X_k$ e in un addendo aleatorio $E$, ovvero il caso in cui la relazione cercata è del tipo
$$Y = \alpha_0 + \alpha_1 \cdot X_1 + \dots + \alpha_m \cdot X_m + E$$
dove $\alpha_k, k = 0, \dots, m \in \mathbb{R}$ ed $E \sim N(0, \sigma^2)$.

In questo caso si parla di /regressione lineare/ riferendosi in particolare alla /regressione lineare semplice/ se nella relazione
sopra compare una sola variabile esplicativa e alla /regressione lineare multipla/ in caso contrario.

Il termine aleatorio $E$ è detto solitamente /residuo/ o /errore/. Considerando il valore che ci si deve attendere per la
variabile di risposta conoscendo le realizzazioni degli altri caratteri $\hat{Y} = \alpha_0 + \alpha_1 \cdot X_1 + \dots + \alpha_m \cdot X_m$, il residuo
descrive scostamenti da tale valore atteso, imputabili a cause aleatorie e non controllabili quali ad esempio, errori di misura,
dipendenze da variabili non considerate o valutazioni soggettive.

Si osservi che la scrittura $Y = \alpha_0 + \alpha_1 \cdot X_1 + \dots + \alpha_m \cdot X_m + E$ potrebbe risultare ambigua, nel senso che potrebbe portare a pensare
che il residuo abbia lo stesso valore per ogni singolo individuo della popolazione. In realtà il residuo ha solo la stessa
distribuzione per ogni singolo valore.

L'equazione $Y = \alpha_0 + \alpha_1 \cdot X_1 + \dots + \alpha_m \cdot X_m + E$ descrive infatti la distribuzione del carattere $Y$ come distribuzione della somma
di $m+1$ variabili, tra cui una che esprime gli scostamenti aleatori.

Per ovviare al rischio di incomprensioni preferiamo quindi riscrivere la precedente relazione esplicitando il fatto che tanto
il residuo quanto i valori assunti dalle variabili esplicative variano per ogni individuo della popolazione, considerando
d'ora in poi l'equazione:
$$Y_i = \alpha_0 + \alpha_1 \cdot X_{1, i} + \dots + \alpha_m \cdot X_{m, i} + E_i$$
dove l'indice $i$ varia da 1 ad $N$, totale degli individui della popolazione.

In questo capitolo tratteremo in modo particolare la regressione lineare semplice che è il caso più interessante da un punto di
vista didattico. Dedicheremo solo una sezione conclusiva alla regressione lineare multipla, accennando rapidamente ai
problemi ad essa connessi.

In particolare vedremo prima come si stimano le costanti del modello lineare, ovvero la varianza dei residui e le costanti
$\alpha_k$, e poi come si controlla l'ammissibilità del modello lineare per descrivere le eventuali dipendenze tra i caratteri della
popolazione.

Notiamo che istintivamente verrebbe naturale provare subito a controllare la validità del modello lineare e solo successivamente,
in caso di risultati positivi, determinare la costanti che in esso compaiono.

Si procede in realtà al contrario per due motivi fondamentali:
1. Abbiamo visto nel capitolo 6 che lo statistico non effettua mai un test per vedere se una certa ipotesi è vera, bensì per vedere
   se tale ipotesi non può essere rifiutata. Nella regressione lineare, in genere si assume a priori la validità della relazione
   $Y_i = \alpha_0 + \alpha_1 \cdot X_{1, i} + \dots + \alpha_m \cdot X_{m, i} + E_i$ e solo successivamente si controlla se i dati portano ad un rifiuto di tale ipotesi;
2. I test di ammissibilità della relazione $Y_i = \alpha_0 + \alpha_1 \cdot X_{1, i} + \dots + \alpha_m \cdot X_{m, i} + E_i$ si basano principalmente su particolari
   statistiche, la cui realizzazione può essere determinata solo dopo aver effettuato le stime delle costanti (varianza dei
   residui e costanti $\alpha_k$).
*** Stima delle costanti del modello
Consideriamo una popolazione bidimensionale $(X, Y)$ ed assumiamo che tra i due caratteri esista un /legame di dipendenza
esprimibile tramite un'equazione di regressione lineare (semplice)/. Per ogni individuo $i = 1, \dots, N$, sia quindi
$Y_i = \alpha_0 + \alpha_1 \cdot X_i + E_i$. In primo luogo potremmo essere interessati a determinare il /valore delle costanti/ $\alpha_0$ e $\alpha_1$.
Ovviamente, a meno di non disporre dei dati relativi a tutti gli $N$ individui della popolazione, di tali costanti non
potremo fare altro che /determinarne delle stime numeriche/ che denoteremo con $a_0$ e $a_1$.

Supponiamo quindi di disporre di un campione $\{(X_1, Y_1), \dots, (X_n, Y_n)\}$ di numerosità $n$. Tre ipotesi devono essere fatte per
determinare le stime $a_0$ e $a_1$. Per ora le assumeremo valide, rimandando ai prossimi paragrafi le tecniche utilizzabili per
accertarne la validità. Tali ipotesi, che si riferiscono ai residui, sono le seguenti:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro.

L'ipotesi $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$ viene solitamente detta /ipotesi di omoschedasticità/.

Ovviamente, pur supponendo la validità di queste 3 ipotesi, ancora non conosciamo il /valore della varianza/ che viene per ora
comunque /indicato come costante nota/.

Gli /stimatori puntuali/ utilizzati per /stimare le costanti/ sono le statistiche
- $A_0 = \bar{Y} - A_1 \cdot \bar{X}$;
- $A_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})\cdot(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}$.
Dove $\bar{X} = \frac{\sum_{i=1}^n X_i}{n}$ e $\bar{Y} = \frac{\sum_{i=1}^n Y_i}{n}$.

È possibile osservare che tali stimatori sono definiti analogamente alle costanti $q$ ed $m$ della /retta dei minimi quadrati/
$Y = m \cdot X + q$ per /descrivere al meglio un legame lineare tra due caratteri/ di una serie di dati.

In effetti, gli /stimatori $A_0$ ed $A_1$/ vengono determinati allo stesso modo, come /valori per cui risulta minima la quantità/
$\sum_{i=1}^n [A_1 \cdot X_i + A_0 - Y_i]^2$.

Supposto quindi che $\{(x_1, y_1), \dots, (x_n, y_n)\}$ sia una realizzazione del campione $\{(X_1, Y_1), \dots, (X_n, Y_n)\}$, due stime puntuali
di $\alpha_0$ e $\alpha_1$ sono i valori:
- $a_0 = \bar{Y} - a_1 \cdot \bar{X}$;
- $a_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})\cdot(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}$.
Dove $\bar{x} = \frac{\sum_{i=1}^n x_i}{n}$ e $\bar{y} = \frac{\sum_{i=1}^n y_i}{n}$.

I due /stimatori $A_0$ e $A_1$/ proposti, sono tali che, /sotto le tre ipotesi specificate/ precedentemente, essi /soddisfano le
seguenti proprietà/:
- $E[A_0] = \alpha_0 \quad V[A_0] = \sigma^2 \cdot \left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}\right]$;
- $E[A_1] = \alpha_1 \quad V[A_1] = \sigma^2 \cdot \left[\frac{1}{\sum_{i=1}^n (x_i - \bar{x})^2}\right]$;
Da cui si deduce che $A_0$ e $A_1$ sono due /stimatori corretti/ e pertanto adatti a fare inferenze sui coefficienti $\alpha_0$ e $\alpha_1$.

In particolare, se valgono le ipotesi
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
si può dire qualcosa di ancora più forte, infatti in questo caso, /$A_0$ ed \(A_1\)/ sono, tra gli /stimatori/ corretti per
$\alpha_0$ e $\alpha_1$, i /più efficienti/, ovvero quelli cui corrisponde varianza minima.

Questa proprietà è solitamente riferita in letterature con il termine di /Teorema di Gauss-Markov/.

Dall'espressione delle varianze dei due stimatori si vede che esse dipendono non solo dalla numerosità $n$ del campione,
ma anche dalla disposizione sull'asse delle $x$ delle osservazioni $x_i$.

Se la dispersione di tali osservazioni sull'asse è piccola, allora la quantità $\sum_{i=1}^n (x_i - \bar{x})^2$ risulta piccola e le
varianze diventano grandi. In questo caso allora cresce la probabilità che le stime $a_0$ e $a_1$ siano lontane dai valori reali
$\alpha_0$ e $\alpha_1$. Per questa ragione sarebbe preferibile avere campioni in cui siano presenti grandi scostamenti dei valori assunti
dal carattere $X$.

Supponiamo di voler effettuare delle stime intervallari per le costanti $\alpha_0$ e $\alpha_1$. Occorre allora aggiungere una /nuova ipotesi
sui residui/. Tale ipotesi è la seguente:
- Le variabili $E_i$ sono normalmente distribuite.
Notiamo che essa, aggiunga alle altre, equivale a dire che /ogni residuo è normalmente distribuito con media nulla ed identica
varianza \(\sigma^2\)/.

Questa /non è un'ipotesi molto restrittiva o irrealistica/, in effetti è logico aspettarsi che gli /errori/ siano /normalmente
distribuiti, in quanto somma di numerosi fattori casuali indipendenti/.

Si ricordi infatti che per il /Teorema Centrale Limite/, la /somma di numerosi fattori aleatori, tende a distribuirsi come una
normale/.

Si può mostrare che condizionalmente alle 4 ipotesi:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
- Le variabili $E_i$ sono normalmente distribuite;
anche gli /stimatori $A_0$ ed $A_1$ sono normalmente distribuiti/, con:
- $A_0 \cong N\left(\alpha_0, \sigma^2 \cdot \left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}\right]\right)$;
- $A_1 \cong N\left(\alpha_1, \sigma^2 \cdot \left[\frac{1}{\sum_{i=1}^n (x_i - \bar{x})^2}\right]\right)$;
La conoscenza della distribuzione dei due stimatori potrebbe essere utilizzata per determinare degli intervalli, come visto nel
Capitolo 5.

Notiamo però che il parametro $\sigma^2$ che compare nelle formule precedenti non è noto. Per ovviare a tale mancanza si sostituisce con
una stima data dalla realizzazione della statistica $S_{\text{RES}} = \frac{1}{n-2} \cdot \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$, dove le quantità $\hat{Y}_i$
sono i valori teorici che dovrebbero essere assunti dal carattere $Y$ se si trovassero sulla retta di regressione stimata, ovvero
$\hat{Y}_i = a_0 + a_1 \cdot X_i, \forall i = 1, \dots, n$.

Notiamo che la statistica $S_{\text{RES}}^2$ non è altro che una /varianza campionaria che descrive lo scostamento tra le osservazioni
$y_i$ del carattere $Y$ e le loro stime $\hat{y}_i = a_0 + a_1 \cdot x_i$ ottenute tramite la regressione/.

Quindi essa è proprio la /varianza campionaria dei residui/ $E_i = Y_i -(\alpha_0 + \alpha_1 \cdot X_i)$.

La presenza del termine $\frac{1}{n-2}$ è giustificata dal fatto che così definito, lo stimatore $S_{\text{RES}}^2$ di $\sigma^2$ risulta
corretto, ovvero vale $E[S_{\text{RES}}] = \sigma^2$.

Andiamo quindi a sostituire nelle
- $A_0 \cong N\left(\alpha_0, \sigma^2 \cdot \left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}\right]\right)$;
- $A_1 \cong N\left(\alpha_1, \sigma^2 \cdot \left[\frac{1}{\sum_{i=1}^n (x_i - \bar{x})^2}\right]\right)$;
la quantità $\sigma^2$ con il suo stimatore $S_{\text{RES}}^2$ e consideriamo le variabili:
- $T_0 = \frac{A_0 - \alpha_0}{\sqrt{S_{\text{RES}}^2 \cdot \left[\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^n (X_i - \bar{X})^2}\right]}}$;
- $T_1 = \frac{A_1 - \alpha_1}{\sqrt{S_{\text{RES}}^2 \cdot \left[\frac{1}{\sum_{i=1}^n (X_i - \bar{X})^2}\right]}}$.
Condizionatamente alle consuete ipotesi:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
- Le variabili $E_i$ sono normalmente distribuite;
le variabili $T_0$ e $T_1$ risultano essere distribuite come delle t di Student con $n-2$ gradi di libertà.

Considerate allora le realizzazioni:
- $s_{\text{RES}}^2$;
- $a_0$;
- $a_1$;
- $x_i \quad \forall i = 1, \dots, n$;
- $\bar{x}$.
Si ottengono gli intervalli di confidenza per $\alpha_0$ e $\alpha_1$ che per un fissato livello di confidenza $\alpha$ risultano essere:
- $\left[a_0 - t_{1-\frac{\alpha}{2}} \cdot \sqrt{s_{\text{RES}}^2 \cdot \left[\frac{1}{n} + \frac{\bar{x}^2}{s_x^2}\right]},
  a_0 + t_{1-\frac{\alpha}{2}} \cdot \sqrt{s_{\text{RES}}^2 \cdot \left[\frac{1}{n} + \frac{\bar{x}^2}{s_x^2}\right]}\right]$;
- $\left[a_1 - t_{1-\frac{\alpha}{2}} \cdot \sqrt{s_{\text{RES}}^2 \cdot \frac{1}{s_x^2}},
  a_0 + t_{1-\frac{\alpha}{2}} \cdot \sqrt{s_{\text{RES}}^2 \cdot \frac{1}{s_x^2}}\right]$.
Dove $s_x^2 = \sum_{i=1}^n (x_i - \bar{x})^2$, mentre $t_{1 - \frac{\alpha}{2}}$ è il quantile di ordine $1 - \frac{\alpha}{2}$ della t di Student con
$n-2$ gradi di libertà.

Per quanto riguarda le stime del parametro $\sigma^2$ abbiamo già detto che uno stimatore corretto è la variabile $S_{\text{RES}}^2$.

Anche di questa variabile è nota la distribuzione quando valgono le ipotesi:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
- Le variabili $E_i$ sono normalmente distribuite.
In questo caso, infatti, il rapporto $\frac{(n-2) \cdot S_{\text{RES}}^2}{\sigma^2}$ risulta essere distribuito come una Chi-Quadro con $n-2$
gradi di libertà.

Mediante i soliti ragionamenti si deduce allora che un intervallo di confidenza per $\sigma^2$ con un livello di fiducia $\alpha$ è
$$\left[\frac{(n-2) \cdot s_{\text{RES}}^2}{q_{1 - \frac{\alpha}{2}}}, \frac{(n-2) \cdot s_{\text{RES}}^2}{q_{\frac{\alpha}{2}}}\right]$$
dove $q_{\frac{\alpha}{2}}$ e $q_{1-\frac{\alpha}{2}}$ sono i quantili della Chi-Quadro con $n-2$ gradi di libertà.
*** Attendibilità del modello lineare
Abbiamo già detto che /dopo aver stimato i parametri di un modello lineare occorre verificare l'attendibilità del modello stesso/,
ovvero verificare l'ipotesi che le relazioni tra i due caratteri siano esprimibili tramite l'equazione
$Y_i = \alpha_0 + \alpha_1 \cdot X_i + E_i$.

Un primo criterio adottabile per procedere in queste verifiche consiste nell'effettuare uno dei /test di/ ipotesi, descritti nei
Capitoli 6 e 7, relativi alla /incorrelazione lineare tra due caratteri di una popolazione/.

Se con l'uso di questi test si /dovesse rifiutare l'ipotesi di incorrelazione/, allora ha senso provare ad /effettuare test
più approfonditi sulla validità del modello lineare/, /altrimenti/ si dovrebbe passare a /considerare qualche altro modello/.

È preferibile utilizzare il test basato sulla statistica $\hat{T}_n = R_n \cdot \sqrt{\frac{n-2}{1 - R_n^2}}$ descritto nel paragrafo 6.7 se si
posseggono dei dati accurati.

Useremo invece il test basato sulla statistica $\tilde{T}_n = R_S \cdot \sqrt{\frac{n-2}{1 - R_S^2}}$ descritto nel paragrafo 7.4 se non si posseggono
dei dati accurati.

Un test più specifico per la regressione lineare è basato su un approccio di /analisi della varianza/ del carattere dipendente $Y$.

Dato un campione $\{(X_1, Y_1, \dots, (X_n, Y_n))\}$ estratto da $(X, Y)$, si considerino gli stimatori $A_0$ ed $A_1$ definiti in
precedenza, e quindi gli stimatori $\hat{Y}_i = A_0 + A_1 \cdot X_i$.

Si considerino poi le seguenti statistiche:
- /Devianza Totale/: $D_{\text{TOT}} = \sum_{i=1}^n (Y_i - \bar{Y})^2$.

  Si riferisce agli scostamenti tra le osservazioni $Y_i$ e la loro media campionaria $\bar{Y}$;
- /Devianza Spiegata/: $D_{\text{SP}} = \sum_{i=1}^n (\hat{Y}_i - \bar{Y})^2$.

  Si riferisce agli scostamenti tra le stime $\hat{Y}_i$ e la $\bar{Y}$.
- /Devianza dei Residui/: $D_{\text{RES}} = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$.

  Si riferisce agli scostamenti tra le osservazioni $Y_i$ e le loro stime $\hat{Y}_i$.

Queste tre quantità sono gli indici degli scostamenti tra le osservazioni campionarie del carattere $Y$, la media di queste
osservazioni e le loro stime tramite la regressione.

Si può dimostrare che tra le 3 devianze introdotte sussiste la seguente relazione: $D_{\text{TOT}} = D_{\text{SP}} + D_{\text{RES}}$.

Possiamo allora affermare che le variazioni tra le osservazioni $Y_i$ e la loro media $\bar{Y}$ sono da attribuire in parte alle
/"variazioni spiegate"/ dalla retta di regressione, ed in parte al fatto che le osservazioni non si trovano esattamente su tale
retta (cioè ai residui).

In altri termini, possiamo dire che se non fosse per i residui, allora la regressione spiegherebbe lo scostamento totale tra le
osservazioni e la loro media.

È logico pensare che il modello è adatto a descrivere variazioni del carattere $Y$ se queste sono causate principalmente dalla
regressione, ovvero se il rapporto $R^2 = \frac{D_{\text{SP}}}{D_{\text{TOT}}} = 1 - \frac{D_{\text{RES}}}{D_{\text{TOT}}}$ è prossimo a 1.

In effetti si può verificare che $R^2$ sia il quadrato del coefficiente di correlazione lineare $r_{XY}$ introdotto nei capitolo
precedenti, cioè vale $\sqrt{R^2} = |r_{XY}|$.

Mentre però il coefficiente di correlazione lineare ha senso solo se si considera una regressione lineare semplice, il coefficiente
$R^2$ presenta il vantaggio di essere utilizzabile come misura di adattamento qualunque sia la forma della curva di regressione e
qualunque sia il numero di variabili esplicative considerate. Per tale ragione è chiamato /coefficiente di correlazione
generalizzato/.

Similmente a quanto sopra, possiamo pensare che il modello lineare è adatto a descrivere le variazioni del carattere $Y$ se la
quantità $D_{\text{RES}}$ risulta essere molto inferiore alla $D_{\text{SP}}$, ovvero se il rapporto $\frac{D_{\text{SP}}}{D_{\text{RES}}}$ è molto
grande.

Un test sull'attendibilità del modello lineare molto utilizzato si basa proprio su questo rapporto, e specificamente sul fatto
che la /statistica/ $\tilde{F} = (n-2) \cdot \frac{D_{\text{SP}}}{D_{\text{RES}}} = \frac{D_{\text{SP}}}{S_{\text{RES}}^2}$, con
$S_{\text{RES}}^2 = \frac{1}{n-2} \cdot \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$ è distribuita come una F con $(1, n-2)$ gradi di libertà, quando siano
soddisfatte le ipotesi:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
- Le variabili $E_i$ sono normalmente distribuite;
e quando valga $\alpha_1 = 0$, ovvero non esiste dipendenza tra i valori assunti dal carattere $X$ e quelli assunti dal carattere $Y$.

È possibile pertanto effettuare un /test per l'ipotesi nulla/ $H_0 : \alpha_1 = 0$ (regressione non significativa) /rifiutando/
quando la realizzazione della statistica $\tilde{F} = (n-2) \cdot \frac{D_{\text{SP}}}{D_{\text{RES}}} = \frac{D_{\text{SP}}}{S_{\text{RES}}^2}$
assume valori grandi, ovvero /nel caso di un test di ampiezza $\alpha$, quando/ $\tilde{f} > F_{1-\alpha}$, dove $F_{1-\alpha}$ è il
/quantile di ordine/ $1-\alpha$ /della \(F(1, n-2)\)/.

Ci si ricordi però che prima di effettuare questo test, occorre verificare la sussistenza delle ipotesi:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
- Le variabili $E_i$ sono normalmente distribuite;
sui residui, con i metodi presentati nel prossimo paragrafo.

Ricordiamo inoltre che, in generale, il nostro obiettivo è rifiutare l'ipotesi nulla, vale a dire l'inconsistenza del modello di
regressione lineare.

Un secondo test che si può adottare per verificare l'attendibilità del modello lineare si basa sul fatto che /assumere l'esistenza
di una relazione lineare tra i due caratteri $X$ ed $Y$ corrisponde ad assume/ $\alpha \neq 0$.

Se fosse infatti $\alpha_1 = 0$, allora /le variazioni delle $Y$ non dipenderebbero da quelle delle $X$/. Si può quindi effettuare un
/test avente come ipotesi nulla/ la $H_0 : \alpha_1 = 0$ e come /ipotesi alternativa/ $H_1 : \alpha_1 \neq 0$.

In tal caso si utilizza il fatto che quando siano soddisfatte le seguenti ipotesi sui residui:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
- Le variabili $E_i$ sono normalmente distribuite;
la variabile $T_1$, definita come $T_1 = \frac{A_1 - \alpha_1}{\sqrt{S_{\text{RES}}^2 \cdot \left[\frac{1}{\sum_{i=1}^n (X_i - \bar{X})^2}\right]}}$
è distribuita come una t di Student con $n-2$ gradi di libertà.

/Rifiuteremo l'ipotesi nulla/ quando la sua realizzazione $\tilde{t}$ /assume valori troppo distanti dallo zero/ per poter pensare
che essa sia distribuita secondo una t di Student con $n-2$ gradi di libertà.

Ad esempio, per un /test di ampiezza $\alpha$, rifiutiamo $H_0$ quando/ $|\tilde{t}| > t_{1-\frac{\alpha}{2}}$, dove $t_{1-\frac{\alpha}{2}}$ è il quantile di
ordine $1 - \frac{\alpha}{2}$ di una t di Student con $n-2$ gradi di libertà.

Ricordiamo che anche con questo test la nostra speranza è di rifiutare l'ipotesi nulla, ovvero la non dipendenza di $Y$ da $X$.

Relativamente ai due test di ipotesi appena presentati, occorre fare una breve considerazione. Dalla definizione delle
distribuzioni t di Student ed F si può notare che se una variabile aleatoria $Z$ ha distribuzione $t_{n-2}$, allora il suo
quadrato $Z^2$ ha distribuzione $F(1, n-2)$.

In effetti, nel caso di regressione lineare semplice per le variabili $\tilde{T}$ ed $\tilde{F}$ sopra definite, vale la relazione:
- $\tilde{T} = \frac{A_1}{\sqrt{S_{\text{RES}}^2 \cdot \left[\frac{1}{\sum_{i=1}^n (X_i - \bar{X})^2}\right]}}$;
- $\tilde{T}^2 = \tilde{F}$;
- $\tilde{F} = (n-2) \cdot \frac{D_{\text{SP}}}{D_{\text{RES}}} = \frac{D_{\text{SP}}}{S_{\text{RES}}^2}$.
Che per brevità non dimostreremo.

Se ne deduce facilmente che i due test appena proposti sono equivalenti.

Li abbiamo comunque presentati separatamente perché tale equivalenza non sussiste più nel caso di regressione lineare multipla
e nel caso di regressione non-lineare.
*** Analisi dei residui
Nei paragrafi precedenti abbiamo definito i /residui/ $E_i = \hat{Y}_i - Y_i$ ed $\hat{Y}_i = A_0 + A_1 \cdot X_i$ come differenze,
per ogni individuo, tra i valori assunti dal carattere $Y$ e le sue stime secondo la regressione.

Abbiamo inoltre fatto diverse /ipotesi su tali residui/, che essi:
- Abbiano media nulla;
- Siano incorrelati;
- Abbiano identica varianza;
- Siano normalmente distribuiti.
In un'analisi di regressione è opportuno pertanto /controllare la validità di tali ipotesi/.

Nei test si fa uso del livello di significatività, il quale implicitamente definisce la regione critica: se il valore della
statistica calcolato dai dati campionari cade nella regione critica, si rifiuta l'ipotesi.

Non si sa però, se si modificasse la significatività, se l'ipotesi continuerebbe ad esser rifiutata o no, salvo ricalcolare la
regione: non si sa cioè se il valore ottenuto sia significativamente o solo marginalmente interno alla regione critica predefinita.

Un modo alternativo e più informativo è quello di usare il $p-\text{value}$.

Sia $t^*$ il valore assunto dalla statistica sui dati campionari e si considerino le regioni critiche a seconda dell'ipotesi
alternativa:
- Test bidirezionale: $H_1' : \mu \neq \mu_0$;
- Test unidirezionale con coda a sinistra: $H_1'' : \mu < \mu_0$;
- Test unidirezionale con coda a destra: $H_1''' : \mu > \mu_0$
- $C' = \left(-\infty, -t_{1-\frac{\alpha}{2}}^* \right) \cup \left(\mu_0 +t_{1-\frac{\alpha}{2}}^*, +\infty\right)$
  se $H_1'$ è l'ipotesi alternativa;
- $C'' = \left(-\infty, -t_{1-\alpha}^*\right)$ se $H_1''$ è l'ipotesi alternativa;
- $C''' = \left(t_{1-\alpha}^*, +\infty\right)$ se $H_1'''$ è l'ipotesi alternativa.
Il $p-\text{value}$ è definito come la /probabilità che la statistica valida sotto l'ipotesi nulla assuma valore nella regione critica
così definita/.

Il $p-\text{value}$ /misura/ quindi la "/verosimiglianza/" (o l'"inverosimiglianza") /del valore campionario ottenuto, dalla distribuzione
della statistica sotto l'ipotesi nulla/.

Allora è chiaro che, quanto più piccolo è il $p-\text{value}$, tanto più "inverosimile" è che il risultato campionario della statistica
provenga dalla distribuzione della statistica valida sotto l'ipotesi nulla: quindi con tanta maggior sicurezza si rifiuta l'ipotesi
nulla in favore dell'ipotesi alternativa.

Invece, quanto più grande è il $p-\text{value}$, tanto più "verosimile" è che si verifichi quel tal risultato campionario della
statistica, sotto l'ipotesi nulla: quindi con tanta maggior sicurezza si esclude che l'evidenza ottenuta contrasti con
l'ipotesi nulla.

Come esempio, consideriamo un /test t sulla media nell'ipotesi di popolazione normale/:
- $H_0 : \mu = 0$;
- $H_1' : \mu \neq 0$;
- $\alpha = 0.05$;
- Un risultato del test t* che porta a $p-\text{value} = 0.4 \implies P(t > |t^*|) = 0.4$ dice che c'è una buona probabilità che sotto l'ipotesi nulla
  possa verificarsi una realizzazione della statistica di test con un valore assoluto maggiore di quello che si è realizzato nel nostro
  campione, per cui non vi è evidenza che l'ipotesi nulla sia falsa;
- Un risultato del test t* che porta a $p-\text{value} = 0.0001 \implies P(t > |t^*|) = 0.0001$ dice che c'è una probabilità molto piccola che sotto l'ipotesi nulla
  possa verificarsi una realizzazione della statistica di test con un valore assoluto maggiore di quello che si è realizzato nel nostro
  campione, per cui vi è evidenza che l'ipotesi nulla è falsa, e pertanto si accetta l'ipotesi alternativa.

Se invece $H_1'' : \mu < \mu_0$, con ragionamenti analoghi si ha che ($\alpha = 0.05$):
- $p-\text{value} = 0.4 \implies P(t < t^*) = 0.4 \implies \text{non rifiuto } H_0$;
- $p-\text{value} = 0.0001 \implies P(t < t^*) = 0.0001 \implies \text{rifiuto } H_0$;
Analogamente, se invece $H_1'' : \mu > \mu_0$, con ragionamenti analoghi si ha che ($\alpha = 0.05$):
- $p-\text{value} = 0.4 \implies P(t > t^*) = 0.4 \implies \text{non rifiuto } H_0$;
- $p-\text{value} = 0.0001 \implies P(t > t^*) = 0.0001 \implies \text{rifiuto } H_0$;
Relativamente all'/ipotesi di incorrelazione/ tra i residui, il metodo più utilizzato è il /Test di Durbin-Watson/. In esso viene
considerata la statistica
$$D = \frac{\sum_{i=2}^n (E_i - E_{i-1})^2}{\sum_{i=1}^n E_i^2}$$
dove $E_i$ è l'\(i\)-esimo residuo.

Sotto l'ipotesi di incorrelazione, tale statistica ha una particolare distribuzione, le cui tavole si trovano in numerosi testi
introduttivi all'analisi di regressione.

È possibile quindi determinare delle regioni critiche per $D$ e rifiutare l'ipotesi di incorrelazione se la realizzazione di $D$
dovesse cadere in tali regioni.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-09 15:11:12
[[file:Lezioni/screenshot_2018-06-09_15-11-12.png]]

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-09 15:11:28
[[file:Lezioni/screenshot_2018-06-09_15-11-28.png]]

Mostriamo come sia possibile controllare l'/ipotesi di normalità/. In proposito si possono adottare diverse tecniche più o meno
valide. A nostro giudizio la tecnica numerica più valida è quella che fa ricorso ai /test per la bontà dell'adattamento/
denominati test:
- Di /Kolmogorov-Smirnov/:
  - È basato sulla differenza tra ripartizione empirica e ripartizione della normale: $K_n = \text{sup}_x |F_e(x) - F_n(x)|\sqrt{n}$.
  - Si rifiuta l'ipotesi di normalità se $K_n$ è grande;
  - Richiede campioni di elevata numerosità.
- Del /Chi-Quadro/;
- Di /Shapiro Wilk/:
  - Serve per campioni di bassa numerosità;
  - La statistica $W$ misura la rettilineità del normal plot: $W = \frac{\left[\sum_1^n w_i e_i'\right]^2}{\sum_i^n (e_i - \bar{e})^2}$, dove
    i $w_i$ sono quantità tabulate, $e_i$ i valori campionari ed $e_i'$ i valori ordinati;
  - Si rifiuta se $W$ è piccolo.
In tutti i casi si controlla l'ipotesi che i /residui campionari/ $e_1, \dots, e_n$ ottenuti dalle osservazioni del campione
$\{(X_1, Y_1), \dots, (X_n, Y_n)\}$ siano /distribuiti normalmente con media nulla e varianza/ $s_{\text{RES}}^2$, ricordando:
$S_{\text{res}}^2 = \frac{1}{n-2} \cdot \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$.

Un'alternativa è il /Normal Quantile plot/. In ascissa la cumulata di una normale. In ordinata la cumulata dei dati. I punti
del grafico devono essere allineati se provengono da una distribuzione normale (o comunque cadere entro le curve in rosso,
regione di confidenza a $(1-\alpha)$).

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-09 15:27:30
[[file:Lezioni/screenshot_2018-06-09_15-27-30.png]]

Normal Quantile plot - Non-normalità:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-09 15:27:57
[[file:Lezioni/screenshot_2018-06-09_15-27-57.png]]

Le maggiori difficoltà si incontrano nella /verifica di omoschedasticità/, ovvero di identica varianza per ogni residuo. Test
veramente significativi per tale ipotesi possono essere realizzati se il numero di valori assumibili dal carattere $X$
è limitato e se per ognuno di tali valori esistono numerose osservazioni. Questa situazione purtroppo si presenta raramente
nelle applicazioni e si è costretti a ricorrere a metodi grafici.

Tra questi, comunque non sempre disprezzabili, segnaliamo quello basato sulla rappresentazione dei residui in funzione dei valori
assunti dalla variabile $X$ o $Y$. Per ogni elemento del campione si individui il corrispondente punto su un piano cartesiano
avente i valori del carattere $X$ (o $Y$) su un asse, ed i valori del residuo sull'altro.

/Accettiamo l'ipotesi di omoschedasticità/ se i punti così individuati si presentano a forma di una nuvola, come riportato sotto:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-09 15:31:54
[[file:Lezioni/screenshot_2018-06-09_15-31-54.png]]

Rifiutiamo invece l'ipotesi se i punti assumono un andamento più regolare come mostrato sotto:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-09 15:32:23
[[file:Lezioni/screenshot_2018-06-09_15-32-23.png]]

In questo caso è lecito pensare che la varianza del residuo aumenti all'aumentare del valore assunto dal carattere $X$.

Un altro caso di varianza non costante (dipendente da $y$):

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-09 15:33:14
[[file:Lezioni/screenshot_2018-06-09_15-33-14.png]]

Uno dei problemi che si presentano nella pratica è la presenza di /osservazioni anomale/ (dovute, ad esempio, a errori di misura).
Questi valori anomali, detti /outliers/, possono /provocare un peggioramento nella qualità della regressione/, in quanto i valori
dei /parametri stimati vengono distorti per tenere conto anche di questi valori errati/.

Per riconoscere i possibili outliers ed eliminarli dal calcolo, un modo è quello di calcolare i /Residui Studentizzati/:
$$r_i = \frac{e_i}{\sqrt{\text{MSE} \left(1 - \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum (x_i - \bar{x})^2}\right)}}$$
con $e_i$ l'\(i\)-esimo residuo, $\text{SSE}$ somma quadratica dei residui e $\text{MSE}$ scarto quadratico medio dei residui
$\text{MSE} =\frac{\sum e_i^2}{n-2}$.

Si dimostra che essi sono distribuiti secondo una t di Student. Se il residuo studentizzato di un'osservazione supera un valore di
4-5, allora l'osservazione è un candidato outlier.

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-09 15:40:04
[[file:Lezioni/screenshot_2018-06-09_15-40-04.png]]

Non tutti gli outliers hanno però un ugual effetto di modifica dei parametri della regressione: per riconoscere quelli che la
influenzano effettivamente, si usa una misura detta /D-Cook/, che indica il cambio di pendenza se si omette un'osservazione.

Un sospetto outlier è influente se $D > 1$.

Concludiamo osservando che in questo paragrafo e nel paragrafo precedente abbiamo visto come controllare le ipotesi sui residui e
l'attendibilità del modello di regressione lineare.

Ma cosa possiamo fare se le ipotesi vengono rifiutate o se il modello risulta inadeguato?

Tra i diversi tentativi che possono essere fatti suggeriamo di passare a considerare un modello di regressione multipla o non
lineare, oppure di cercare di trasformare opportunamente le variabili del modello in modo da rendere lineari enventuali
relazioni non lineari così come mostrato al termine del capitolo 1.
*** Regressione lineare multipla
Vediamo ora come le stime ed i test descritti per la regressione lineare semplice si generalizzano al caso di /regressione
lineare multipla/. Torniamo quindi a considerare una popolazione $(m+1)$ dimensionale $(X_1, \dots, X_m, Y)$ ed un'/equazione di
regressione/ del tipo $Y_i = \alpha_0 + \alpha_i \cdot X_{1, i} + \dots + \alpha_m \cdot X_{m, i} + E_i$, dove l'indice $i$ varia tra 1 ed $N$, numero totale degli
individui della popolazione. Come vedremo, dal punto di vista teorico non vi sono grandi differenze rispetto al modello
di regressione semplice, eccetto per alcuni indici di correlazione e per il /problema di multicollinearità/.
**** Stima dei Parametri del Modello
Come nel caso della regressione lineare semplice, la prima cosa che potremmo essere intenzionati a determinare è una
/stima delle costanti e della varianza dei residui/. Occorre sottolineare che anche per la regressione lineare multipla,
la determinazione di tale /stima/ è /realizzabile solo condizionatamente alla validità delle ipotesi/:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro.
Supponiamo pertanto di poter affermare che tali ipotesi sono soddisfatte e supponiamo di poter disporre di un campione
$(X_{1,1}, \dots, X_{m,1}, Y_i), \dots, (X_{1,n}, \dots, X_{m,n}, Y_n)$ di numerosità $n > m$  estratto dalla popolazione $(X_1, \dots, X_m, Y)$.
Qui $X_{k,i}$ denota il carattere \(k\)-mo dell'individuo \(i\)-mo con $k = 1, \dots, m$ e $i = 1, \dots, n$

Si può pervenire alla determinazione delle /stime delle costanti del modello/ ragionando esattamente come nel caso semplice,
ovvero /rendendo minima la somma dei quadrati o dei residui/
$$\sum_{i=1}^n E_i^2 = \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^n (Y_i -(\alpha_0 + \alpha_1 \cdot X_{1,i} + \dots + \alpha_m \cdot X_{m,i}))^2$$
Si ottengono così le stime delle costanti per la cui descrizione conviene introdurre una notazione matriciale. A tal fine sia:
\begin{equation*}
X =
\begin{bmatrix}
&1 &X_{1,1} &\dots &X_{m,1}\\
&\vdots &\vdots &\ddots &\vdots\\
&1 &X_{1,n} &\dots &X_{m,n}
\end{bmatrix}
\end{equation*}
la matrice contenente le $m$ /variabili esplicative/ degli $n$ individui del campione (aggiunta della prima colonna contenente
solo l'unità).

Siano poi $Y = (Y_1, \dots, Y_n)^T$ il vettore con i caratteri $Y$ degli individui del campione e $A = (A_0, \dots, A_m)^T$ il vettore colonna
contenente gli stimatori di $\alpha_0, \dots, \alpha_m$.

Denotiamo infine con $M^T$ la trasposta di una matrice o un vettore $M$ e con $M^{-1}$ la matrice inversa di una matrice $M$
/quadrata e non singolare/.

Il /modello regressivo/ può allora essere riscritto in forma vettoriale come $Y = XA + E$.

Si dimostra che se sono valide le ipotesi:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
relative ai residui, allora gli /stimatori corretti di varianza minima di/ $\alpha_0, \dots, \alpha_m$ /sono/ dati da $A = (X^T \cdot X)^{-1} \cdot X^T \cdot Y$.

Si dimostra poi che uno /stimatore corretto della varianza dei residui è/
$S_{\text{RES}}^2 = \frac{q}{n-m-1}\cdot \sum_{i=1}^n E_i^2 = \frac{1}{n-2} \cdot \sum_{i=1}^n (Y_i - \hat{Y}_i)^2$  

In conclusione possiamo dire che se $(x_{1, 1}, \dots, x_{m, 1}, y_1), \dots, (x_{1, n}, \dots, x_{m, n}, y_n)$ è una realizzazione del campione
$(X_{1, 1}, \dots, X_{m, 1}, Y_1), \dots, (X_{1, n}, \dots, X_{m, n}, Y_n)$, allora una stima del vettore dei coefficienti $(\alpha_0, \dots, \alpha_m)^T$ è data dal vettore
$a = (a_0, \dots, a_m)^T$ ottenuto come $a = (x^T \cdot x)^{-1} \cdot x^T \cdot y$.

Una /stima della varianza/ è invece data da: $s_{\text{RES}}^2 = \frac{1}{n-m-1} \cdot \sum_{i=1}^n e_i^2$, dove gli $e_i$ sono le realizzazioni
dei residui $E_i$, ovvero: $e_i = y_i - \hat{y}_i = y_i - (a_0 + a_1 \cdot x_{1, i} + \dots, a_m \cdot x_{m, i})$

Nel caso in cui risulti valida anche l'ipotesi di normalità, sarà possibile computare anche gli intervalli di confidenza per le
stime dei parametri in modo del tutto analogo a quanto fatto nel caso della regressione lineare semplice.

Si denoti con $C_{k, k}$ la \(k\)-ma componente della diagonale principale della matrice $(X^T \cdot X)^{-1}$.

Gli intervalli di confidenza per le costanti si ottengono tenendo conto del fatto che, quando sono soddisfatte le ipotesi sui
residui, allora le variabili $T_k = \frac{A_k - \alpha_k}{\sqrt{S_{\text{RES}} \cdot C_{k, k}}}$ sono distribuite secondo delle t di Student con
$(n-m-1)$ gradi di libertà.

I consueti ragionamenti portano a questo punto a determinare gli intervalli di confidenza per qualsiasi livello di fiducia.
**** Attendibilità del Modello
Abbiamo già accennato al fatto che passando a considerare la regressione lineare multipla, alcune quantità utilizzabili per valutare
la bontà dell'adattamento di un modello di regressione lineare semplice perdono di significato.

È questo il caso, ad esempio, del /coefficiente di correlazione lineare/ $r_{XY}$.

Altre quantità continuano però ad essere significative. Tra di esse, in particolare ricordiamo il /coefficiente di correlazione
generalizzato/ definito come $R^2 = 1 - \frac{D_{\text{RES}}}{D_{\text{TOT}}}$.

Il modello di regressione $Y_i = \alpha_0 + \alpha_1 \cdot X_{i,i} + \dots + \alpha_m \cdot X_{m,i} + E_i$, le cui costanti vengono stimate come mostrato in precedenza,
dovrà essere considerato tanto più attendibile quanto più $R^2$ si avvicina ad 1, ed essere considerato inattendibile quando esso
è prossimo allo 0.

Un vero e proprio test si può effettuare quando siano verificate tutte le ipotesi sui residui:
- $E[E_i] = 0 \quad \forall i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
- Le variabili $E_i$ sono normalmente distribuite.
In questo caso, si può far ricorso alla statistica $\tilde{F} = \frac{D_{\text{SP}}}{S_{\text{RES}}^2} = (n-m-1) \cdot \frac{D_{\text{SP}}}{D_{\text{RES}}}$,
che risulta essere distribuita secondo una F con $(m, n-m-1)$ gradi di libertà quando non esiste dipendenza tra il carattere
risposta $Y$ ed i caratteri esplicativi $X_k$, ovvero quando $\alpha_1 = \dots = \alpha_m = 0$.

Rifiuteremo allora l'ipotesi nulla $H_0 : \alpha_1 = \dots = \alpha_m = 0$ quando la realizzazione $\tilde{f}$ della statistica $\tilde{F}$ assume valori
grandi.

In particolare, rifiuteremo l'ipotesi nulla (per un test di ampiezza $\alpha$), quando $\tilde{f} > F_{1-\alpha}$ rappresenta il quantile di ordine
$(1-\alpha)$ della $F(m, n-m-1)$. Per quanto riguarda le ipotesi sui residui, continuano a valere le considerazioni espresse
relativamente alla regressione lineare semplice.
**** Sull'importanza delle Singole Variabili Esplicative
Non tutte le variabili $X_k$ che compaiono nel modello di regressione lineare multipla $Y_i = \alpha_0 + \alpha_1 \cdot X_{1,i} + \dots + \alpha_m \cdot X_{m,i} + E_i$
possono essere realmente esplicative, nel senso che non necessariamente tutte contribuiscono a "spiegare" i valori assunti dal
carattere dipendente $Y$.

Potrebbe infatti essere $\alpha_k = 0$ per qualche $k = 1, \dots, m$.

Può essere utile venire a conoscenza di tale fatto, onde evitare in futuro di rilevare i valori assunti da $X_k$ per determinare
delle stime di $Y$.

Occorre allora poter effettuare dei test specifici per accertare l'eventuale indipendenza tra una singola variabile esplicativa
$X_k$ e la variabile dipendente $Y$.

Per questo si ricorre al fatto già citato che le variabili $T_k = \frac{A_k - \alpha_k}{\sqrt{S_{\text{RES}}^2 \cdot C_{k,k}}}$ hanno distribuzione
nota quando valgono le ipotesi:
- $E[E_i] = 0 \forall \quad i = 1, \dots, N$;
- $V[E_i] = \sigma^2 \quad \forall i = 1, \dots, N$;
- Le variabili $E_i$ sono incorrelate tra loro;
- Le variabili $E_i$ sono normalmente distribuite.

In particolare, se vale anche l'ipotesi nulla $H_0 : \alpha_k = 0$, avremo che la statistica $\tilde{T}_k = \frac{A_k}{\sqrt{S_{\text{RES}}^2 \cdot C_{k,k}}}$
sarà distribuita come una t di Student con $n-m-1$ gradi di libertà.

Rifiuteremo pertanto l'ipotesi nulla $H_0 : \alpha_k = 0$ qualora la realizzazione $\tilde{t}_k$ di $\tilde{T}_k$ assuma valori troppo distanti
dallo 0 per poter pensare che essa abbia distribuzione t di Student con $n-m-1$ gradi di libertà.

Per esempio, per un test di ampiezza $\alpha$, rifiutiamo $H_0$ quando $|\tilde{t}_k| > t_{1-\frac{\alpha}{2}}$, dove $t_{1-\frac{\alpha}{2}}$ rappresenta il
quantile di ordine $1-\frac{\alpha}{2}$ per una t di Student con $n-m-1$ gradi di libertà.
**** Il Problema della Multicollinearità
Il problema della /multicollinearità/ non esiste nella regressione lineare semplice. Notiamo che per determinare il vettore delle
stime $a = (a_0, \dots, a_m)^T$ è necessario calcolare la matrice inversa di $(x^T \cdot x)$ e per questo occorre che tale prodotto matriciale
sia non singolare, ovvero con determinante non nullo. Ciò avviene se nessuna delle variabili $X_k$ è combinazione lineare delle altre.

Nella scelta delle variabili esplicative occorre quindi essere certi che non si verifichi tale fenomeno detto di multicollinearità.

In realtà si presentano dei problemi non solo se una delle $X_k$ è combinazione lineare delle altre, ma anche se ci si trova di fronte
a forti dipendenze lineari tra le variabili esplicative.

In questi casi occorre quindi eliminare dal modello una o più di esse.

Come valutare se esiste un problema di multicollinearità?

Diverse misure sono state proposte in letteratura a tale scopo, ma non le descriviamo qui.

Una cosa che comunque si può fare senza troppe difficoltà è considerare la matrice di correlazione delle variabili
esplicative (ovvero la matrice le cui componenti sono i coefficienti di correlazione lineare di tutte le coppie di variabili
esplicative).

Una forte correlazione tra due variabili esplicative è condizione sufficiente per affermare che esiste un fenomeno di
multicollinearità. Conviene in questo caso eliminare una delle due variabili dal modello.

* Laboratorio
** Libri
Help:
#+BEGIN_SRC R
help(solve)
#+END_SRC
#+BEGIN_SRC R
?solve
#+END_SRC
#+BEGIN_SRC R
help.start()
#+END_SRC
#+BEGIN_SRC R
??solve
#+END_SRC

Source:
#+BEGIN_SRC R
source("commands.R")
#+END_SRC

Basics:
#+BEGIN_SRC R
ls()

rm(x, y)

rm(list = ls())
#+END_SRC

Vectors
#+BEGIN_SRC R :results output :exports both
x <- c(2, 5, 4)
y <- c(x, 3)

y
#+END_SRC

#+RESULTS:
| 2 |
| 5 |
| 4 |
| 3 |

Vector arithmetic
#+BEGIN_SRC R :results output :exports both
x <- c(1, 2, 3, 4)
y <- 4

v <- 2*x + y + 1

v
#+END_SRC

#+RESULTS:
|  7 |
|  9 |
| 11 |
| 13 |

#+BEGIN_SRC R :results output :exports both
x <- c(1, 4, 6, 6)

max <- c("max:", max(x))
min <- c("min:", min(x))
length <- c("length:", length(x))
mean <- c("mean:", mean(x))
variance <- c("variance:", var(x))
sum <- c("sum:", sum(x))
product <- c("product:", prod(x))

results <- c(max, min, length, mean, variance, sum, product)
sorted_results <- sort(results)

visual <- c("Results:", results, "Sorted results:", sorted_results)
visual
#+END_SRC

#+RESULTS:
| Results:         |
| max:             |
| 6                |
| min:             |
| 1                |
| length:          |
| 4                |
| mean:            |
| 4.25             |
| variance:        |
| 5.58333333333333 |
| sum:             |
| 17               |
| product:         |
| 144              |
| Sorted results:  |
| 1                |
| 144              |
| 17               |
| 4                |
| 4.25             |
| 5.58333333333333 |
| 6                |
| length:          |
| max:             |
| mean:            |
| min:             |
| product:         |
| sum:             |
| variance:        |

#+BEGIN_SRC R :results output :exports both
x <- c(3, 6, -4, 5)
y <- c(1, 4, 5, 0)

pmax <- pmax(x, y)
pmin <- pmin(x, y)

res <- c("pmax", pmax, "pmin", pmin)
res
#+END_SRC

#+RESULTS:
| pmax |
|    3 |
|    6 |
|    5 |
|    5 |
| pmin |
|    1 |
|    4 |
|   -4 |
|    0 |

#+BEGIN_SRC R :results output :exports both
sqrt(-17+0i)
#+END_SRC

#+RESULTS:
: 0+4.12310562561766i

Generating regular sequences:
#+BEGIN_SRC R :results output :exports both
seq(1, 5)
seq(from=1, to=5)
seq(to=5, from=1)
#+END_SRC

#+RESULTS:
| 1 |
| 2 |
| 3 |
| 4 |
| 5 |

#+BEGIN_SRC R :results output :exports both
seq(-5, 5, by=.9)
#+END_SRC

#+RESULTS:
|   -5 |
| -4.1 |
| -3.2 |
| -2.3 |
| -1.4 |
| -0.5 |
|  0.4 |
|  1.3 |
|  2.2 |
|  3.1 |
|    4 |
|  4.9 |

#+BEGIN_SRC R :results output :exports both
seq(length=10, from=-5, by=0.2)
#+END_SRC

#+RESULTS:
|   -5 |
| -4.8 |
| -4.6 |
| -4.4 |
| -4.2 |
|   -4 |
| -3.8 |
| -3.6 |
| -3.4 |
| -3.2 |

#+BEGIN_SRC R :results output :exports both
x <- c(0, 1)
rep(x, times=5)
#+END_SRC

#+RESULTS:
| 0 |
| 1 |
| 0 |
| 1 |
| 0 |
| 1 |
| 0 |
| 1 |
| 0 |
| 1 |

#+BEGIN_SRC R :results output :exports both
x <- c(0, 1)
rep(x, each=3)
#+END_SRC

#+RESULTS:
| 0 |
| 0 |
| 0 |
| 1 |
| 1 |
| 1 |

Logical vectors:
#+BEGIN_SRC R :results output :exports both
x <- 0:5
t <- x > 3
#+END_SRC

#+RESULTS:
| FALSE |
| FALSE |
| FALSE |
| FALSE |
| TRUE  |
| TRUE  |

#+BEGIN_SRC R :results output :exports both
x <- c(0, 1, NA)
ind <- is.na(x)
#+END_SRC

#+RESULTS:
| FALSE |
| FALSE |
| TRUE  |

#+BEGIN_SRC R :results output :exports both
0/0

Inf - Inf

#NaN
#+END_SRC

#+RESULTS:
: nil

#+BEGIN_SRC R :results output :exports both
x <- c(0, NA, NaN)

is.nan(x)
#+END_SRC

#+RESULTS:
| FALSE |
| FALSE |
| TRUE  |

Character vectors:
#+BEGIN_SRC R :results output :exports both
labs <- paste(c("X", "Y"), 1:5, sep="")
#+END_SRC

#+RESULTS:
| X1 |
| Y2 |
| X3 |
| Y4 |
| X5 |

Index cectors; selecting and modifying subsets of a data  set
#+BEGIN_SRC R :results output :exports both
x <- c(0, 1, NA, 4)
y <- x[!is.na(x)]
#+END_SRC

#+RESULTS:
| 0 |
| 1 |
| 4 |

#+BEGIN_SRC R :results output :exports both
x <- c(0, 1, 2, 3, 4, 5)
x[1:3]
#+END_SRC

#+RESULTS:
| 0 |
| 1 |
| 2 |

#+BEGIN_SRC R :results output :exports both
x <- c(0, 1, 2, 3, 4, 5)
x[-(1:3)]
#+END_SRC

#+RESULTS:
| 3 |
| 4 |
| 5 |

#+BEGIN_SRC R :results output :exports both
c("x", "y")[rep(c(1, 2, 2, 1), times=2)]
#+END_SRC

#+RESULTS:
| x |
| y |
| y |
| x |
| x |
| y |
| y |
| x |

#+BEGIN_SRC R :results output :exports both
fruit <- c(5, 10, 1, 20)
names(fruit) <- c("orange", "banana", "apple", "peach")
lunch <- fruit[c("apple", "orange")]
#+END_SRC

#+RESULTS:
| 1 |
| 5 |

#+BEGIN_SRC R :results output :exports both
x <- c(NA, 1, NA, 2)
x[is.na(x)] <- 0

x
#+END_SRC

#+RESULTS:
| 0 |
| 1 |
| 0 |
| 2 |

#+BEGIN_SRC R :results output :exports both
y <- c(-1, -2, 0, 4, 6)
y[y < 0] <- -y[y < 0]

# Same effect as
y <- abs(y)
#+END_SRC

#+RESULTS:
| 1 |
| 2 |
| 0 |
| 4 |
| 6 |

Intrinsic attributes: mode and length
#+BEGIN_SRC R :results output :exports both
x <- c(0, 1)

mode(x)
#+END_SRC

#+RESULTS:
: numeric

#+BEGIN_SRC R :results output :exports both
x <- (3 + 5i)

attributes(x)
mode(x)
#+END_SRC

#+RESULTS:
: complex

#+BEGIN_SRC R :results output :exports both
z <- 0:5

digits <- as.character(z)

d <- as.integer(digits)
#+END_SRC

#+RESULTS:
| 0 |
| 1 |
| 2 |
| 3 |
| 4 |
| 5 |

#+BEGIN_SRC R :results output :exports both
e <- numeric()

e[3] <- 4

e
#+END_SRC

#+RESULTS:
| nil |
| nil |
| 4   |

#+BEGIN_SRC R :results output :exports both
alpha <- 0:9

alpha <- alpha[2 * 1:5]
#+END_SRC

#+RESULTS:
| 1 |
| 3 |
| 5 |
| 7 |
| 9 |

#+BEGIN_SRC R :results output :exports both
alpha <- 0:9

alpha <- alpha[2 * 1:5]

length(alpha) <- 3

alpha
#+END_SRC

#+RESULTS:
| 1 |
| 3 |
| 5 |

#+BEGIN_SRC R
#Allows R to treat z as if it were a 10-by-10 matrix
attr(z, "dim") <- c(3, 3)
#+END_SRC

The class of an object:
- numeric;
- logical;
- character;
- list;
- matrix;
- factor;
- date.frame

#+BEGIN_SRC R
#Temporarily remove the effects of class
unclass(winter)
#+END_SRC

Ordered and unordered factors:
#+BEGIN_SRC R :results output :exports both
state <- c("tas", "sa", "qld", "nsw", "nsw", "nt", "wa", "wa", "qld",
           "vic", "nsw", "vic", "qld", "qld", "sa", "tas", "sa",
           "nt", "wa", "vic", "qld", "nsw", "nsw", "wa", "sa", "act",
           "nsw", "vic", "vic", "act")

statef <- factor(state)

levels(statef)
#+END_SRC

#+RESULTS:
| act |
| nsw |
| nt  |
| qld |
| sa  |
| tas |
| vic |
| wa  |

#+BEGIN_SRC R :results output :exports both
state <- c("tas", "sa", "qld", "nsw", "nsw", "nt", "wa", "wa", "qld",
           "vic", "nsw", "vic", "qld", "qld", "sa", "tas", "sa",
           "nt", "wa", "vic", "qld", "nsw", "nsw", "wa", "sa", "act",
           "nsw", "vic", "vic", "act")

incomes <- c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56,
             61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46,
             59, 46, 58, 43)

statef <- factor(state)

incmeans <- tapply(incomes, statef, mean)
#+END_SRC

#+RESULTS:
|             44.5 |
| 57.3333333333333 |
|             55.5 |
|             53.6 |
|               55 |
|             60.5 |
|               56 |
|            52.25 |

#+BEGIN_SRC R :results output :exports both
state <- c("tas", "sa", "qld", "nsw", "nsw", "nt", "wa", "wa", "qld",
           "vic", "nsw", "vic", "qld", "qld", "sa", "tas", "sa",
           "nt", "wa", "vic", "qld", "nsw", "nsw", "wa", "sa", "act",
           "nsw", "vic", "vic", "act")

incomes <- c(60, 49, 40, 61, 64, 60, 59, 54, 62, 69, 70, 42, 56,
             61, 61, 61, 58, 51, 48, 65, 49, 49, 41, 48, 52, 46,
             59, 46, 58, 43)

# ordered() creates ordered factors
statef <- factor(state)

incmeans <- tapply(incomes, statef, mean)

stdError <- function(x) sqrt(var(x)/length(x))

incster <- tapply(incomes, statef, stdError)
#+END_SRC

#+RESULTS:
|              1.5 |
| 4.31019463339856 |
|              4.5 |
| 4.10609303352956 |
| 2.73861278752583 |
|              0.5 |
| 5.24404424085076 |
| 2.65753645318366 |

Arrays:
#+BEGIN_SRC R :results output :exports both
x <- array(1:20, dim=c(4, 5)) # Generate a 4 by 5 array
#+END_SRC

#+RESULTS:
| 1 | 5 |  9 | 13 | 17 |
| 2 | 6 | 10 | 14 | 18 |
| 3 | 7 | 11 | 15 | 19 |
| 4 | 8 | 12 | 16 | 20 |

#+BEGIN_SRC R :results output :exports both
x <- array(1:20, dim=c(4, 5)) # Generate a 4 by 5 array

i <- array(c(1:3, 3:1), dim=c(3, 2)) # i is a 3 by 2 index array
#+END_SRC

#+RESULTS:
| 1 | 3 |
| 2 | 2 |
| 3 | 1 |

#+BEGIN_SRC R :results output :exports both
x <- array(1:20, dim=c(4, 5)) # Generate a 4 by 5 array

i <- array(c(1:3, 3:1), dim=c(3, 2)) # i is a 3 by 2 index array

x[i] # Extract those elements
#+END_SRC

#+RESULTS:
| 9 |
| 6 |
| 3 |

#+BEGIN_SRC R
x <- array(1:20, dim=c(4, 5)) # Generate a 4 by 5 array

i <- array(c(1:3, 3:1), dim=c(3, 2)) # i is a 3 by 2 index array

x[i] # Extract those elements

x[i] <- 0 #Replace those elements by zeros
#+END_SRC

#+BEGIN_SRC R
Xb <- matrix(0, n, b) # blocks
Xv <- matrix(0, n, v) # varieties
ib <- cbind(1:n, blocks)
iv <- cbind(1:n, varieties)
Xb[ib] <- 1
Xv[iv] <- 1
X <- cbind(Xb, Xv)

N <- table(blocks, varieties)
#+END_SRC

#+BEGIN_SRC R :results output :exports both
x <- matrix(1:10, ncol=5)
dimnames(x) <- list(c("nome1", "nome2"), NULL) #nomina solo le righe
x
#+END_SRC

#+RESULTS:
:       [,1] [,2] [,3] [,4] [,5]
: nome1    1    3    5    7    9
: nome2    2    4    6    8   10

#+BEGIN_SRC R :results output :exports both
x <- matrix(1:10, ncol=5)
dimnames(x) <- list(c("nome2", "nome2"), NULL) #nomina solo le righe
dimnames(x)[[2]] <- c("g", "h", "j", "j", "k") #nomina le colonne
x
#Utilizzabili anche le funzioni rownames() e colnames()
#+END_SRC 

#+RESULTS:
:       g h j j  k
: nome2 1 3 5 7  9
: nome2 2 4 6 8 10

#+BEGIN_SRC R :results output :exports both
h <- 0:23

Z <- array(h, dim=c(3, 4, 2))

#Equivalent if h has 24 elements and not less
Z <- h ; dim(Z) <- c(3, 4, 2)
Z
#+END_SRC

#+RESULTS:
| 0 | 3 | 6 |  9 | 12 | 15 | 18 | 21 |
| 1 | 4 | 7 | 10 | 13 | 16 | 19 | 22 |
| 2 | 5 | 8 | 11 | 14 | 17 | 20 | 23 |

#+BEGIN_SRC R :results output :exports both
h <- 0:11

Z <- array(h, dim=c(3, 4, 2))
#+END_SRC

#+RESULTS:
| 0 | 3 | 6 |  9 | 0 | 3 | 6 |  9 |
| 1 | 4 | 7 | 10 | 1 | 4 | 7 | 10 |
| 2 | 5 | 8 | 11 | 2 | 5 | 8 | 11 |

#+BEGIN_SRC R :results output :exports both
h <- 0:5

Z <- array(h, dim=c(3, 2, 2))

Z[1:6]
#+END_SRC

#+RESULTS:
| 0 |
| 1 |
| 2 |
| 3 |
| 4 |
| 5 |

#+BEGIN_SRC R :results output :exports both
h <- 0:5

Z <- array(h, dim=c(3, 2, 2))

# Equivalent to Z
Z[]
#+END_SRC

#+RESULTS:
| 0 | 3 | 0 | 3 |
| 1 | 4 | 1 | 4 |
| 2 | 5 | 2 | 5 |

The outer product of two arrays:
#+BEGIN_SRC R :results output :exports both
a <- array(1:5)
b <- array (6: 10)

# Equivalent: ab <- outer(a, b, "*")
ab <- a %o% b
#+END_SRC

#+RESULTS:
|  6 |  7 |  8 |  9 | 10 |
| 12 | 14 | 16 | 18 | 20 |
| 18 | 21 | 24 | 27 | 30 |
| 24 | 28 | 32 | 36 | 40 |
| 30 | 35 | 40 | 45 | 50 |

#+BEGIN_SRC R :results output :exports both
x <- 0:5
y <- 2:4

f <- function(x, y) cos(y)/(1 + x^2)
z <- outer(x, y, f)
#+END_SRC

#+RESULTS:
|  -0.416146836547142 |  -0.989992496600445 |  -0.653643620863612 |
|  -0.208073418273571 |  -0.494996248300223 |  -0.326821810431806 |
| -0.0832293673094285 |  -0.197998499320089 |  -0.130728724172722 |
| -0.0416146836547142 | -0.0989992496600445 | -0.0653643620863612 |
| -0.0244792256792437 | -0.0582348527412027 | -0.0384496247566831 |
| -0.0160056475595055 | -0.0380766344846325 | -0.0251401392639851 |

#+BEGIN_SRC R :results output :exports both
# Determinant of 2 by 2 matrices

d <- outer(0:9, 0:9)
#+END_SRC

#+RESULTS:
| 0 | 0 |  0 |  0 |  0 |  0 |  0 |  0 |  0 |  0 |
| 0 | 1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |  9 |
| 0 | 2 |  4 |  6 |  8 | 10 | 12 | 14 | 16 | 18 |
| 0 | 3 |  6 |  9 | 12 | 15 | 18 | 21 | 24 | 27 |
| 0 | 4 |  8 | 12 | 16 | 20 | 24 | 28 | 32 | 36 |
| 0 | 5 | 10 | 15 | 20 | 25 | 30 | 35 | 40 | 45 |
| 0 | 6 | 12 | 18 | 24 | 30 | 36 | 42 | 48 | 54 |
| 0 | 7 | 14 | 21 | 28 | 35 | 42 | 49 | 56 | 63 |
| 0 | 8 | 16 | 24 | 32 | 40 | 48 | 56 | 64 | 72 |
| 0 | 9 | 18 | 27 | 36 | 45 | 54 | 63 | 72 | 81 |

#+BEGIN_SRC R :results output :exports both
# Determinant of 2 by 2 matrices

d <- outer(0:9, 0:9)
fr <- table(outer(d, d, "-"))
#+END_SRC

#+RESULTS:
| -81 |  19 |
| -80 |   1 |
| -79 |   2 |
| -78 |   2 |
| -77 |   3 |
| -76 |   2 |
| -75 |   4 |
| -74 |   2 |
| -73 |   4 |
| -72 |  41 |
| -71 |   4 |
| -70 |   4 |
| -69 |   8 |
| -68 |   6 |
| -67 |   6 |
| -66 |  10 |
| -65 |   7 |
| -64 |  27 |
| -63 |  49 |
| -62 |   8 |
| -61 |   8 |
| -60 |  17 |
| -59 |   8 |
| -58 |  12 |
| -57 |  18 |
| -56 |  53 |
| -55 |  13 |
| -54 |  60 |
| -53 |  12 |
| -52 |  18 |
| -51 |  22 |
| -50 |  16 |
| -49 |  35 |
| -48 |  70 |
| -47 |  22 |
| -46 |  24 |
| -45 |  66 |
| -44 |  28 |
| -43 |  18 |
| -42 |  72 |
| -41 |  22 |
| -40 |  75 |
| -39 |  37 |
| -38 |  34 |
| -37 |  26 |
| -36 | 111 |
| -35 |  63 |
| -34 |  36 |
| -33 |  45 |
| -32 |  84 |
| -31 |  34 |
| -30 |  94 |
| -29 |  36 |
| -28 |  93 |
| -27 |  97 |
| -26 |  50 |
| -25 |  53 |
| -24 | 156 |
| -23 |  42 |
| -22 |  60 |
| -21 | 103 |
| -20 | 107 |
| -19 |  50 |
| -18 | 168 |
| -17 |  51 |
| -16 | 140 |
| -15 | 112 |
| -14 | 116 |
| -13 |  59 |
| -12 | 191 |
| -11 |  65 |
| -10 | 126 |
|  -9 | 156 |
|  -8 | 185 |
|  -7 | 115 |
|  -6 | 206 |
|  -5 | 117 |
|  -4 | 179 |
|  -3 | 153 |
|  -2 | 156 |
|  -1 | 111 |
|   0 | 570 |
|   1 | 111 |
|   2 | 156 |
|   3 | 153 |
|   4 | 179 |
|   5 | 117 |
|   6 | 206 |
|   7 | 115 |
|   8 | 185 |
|   9 | 156 |
|  10 | 126 |
|  11 |  65 |
|  12 | 191 |
|  13 |  59 |
|  14 | 116 |
|  15 | 112 |
|  16 | 140 |
|  17 |  51 |
|  18 | 168 |
|  19 |  50 |
|  20 | 107 |
|  21 | 103 |
|  22 |  60 |
|  23 |  42 |
|  24 | 156 |
|  25 |  53 |
|  26 |  50 |
|  27 |  97 |
|  28 |  93 |
|  29 |  36 |
|  30 |  94 |
|  31 |  34 |
|  32 |  84 |
|  33 |  45 |
|  34 |  36 |
|  35 |  63 |
|  36 | 111 |
|  37 |  26 |
|  38 |  34 |
|  39 |  37 |
|  40 |  75 |
|  41 |  22 |
|  42 |  72 |
|  43 |  18 |
|  44 |  28 |
|  45 |  66 |
|  46 |  24 |
|  47 |  22 |
|  48 |  70 |
|  49 |  35 |
|  50 |  16 |
|  51 |  22 |
|  52 |  18 |
|  53 |  12 |
|  54 |  60 |
|  55 |  13 |
|  56 |  53 |
|  57 |  18 |
|  58 |  12 |
|  59 |   8 |
|  60 |  17 |
|  61 |   8 |
|  62 |   8 |
|  63 |  49 |
|  64 |  27 |
|  65 |   7 |
|  66 |  10 |
|  67 |   6 |
|  68 |   6 |
|  69 |   8 |
|  70 |   4 |
|  71 |   4 |
|  72 |  41 |
|  73 |   4 |
|  74 |   2 |
|  75 |   4 |
|  76 |   2 |
|  77 |   3 |
|  78 |   2 |
|  79 |   2 |
|  80 |   1 |
|  81 |  19 |

#+BEGIN_SRC R :file graph.png :results graphics :exports both
# Determinant of 2 by 2 matrices

d <- outer(0:9, 0:9)
fr <- table(outer(d, d, "-"))
plot(fr, xlab="Determinant", ylab="Frequency")
#+END_SRC

#+RESULTS:
[[file:graph.png]]

Generalized transpose of an array:
#+BEGIN_SRC R :results output :exports both
A <- array(c(0:3, 5:6), dim=c(3, 2))

#Transpose (also t(A)):
B <- aperm(A, c(2, 1))
#+END_SRC

#+RESULTS:
| 0 | 1 | 2 |
| 3 | 5 | 6 |

Matrix facilities:
#+BEGIN_SRC R :results output :exports both
A <- array(0:9, c(5, 2))

row <- nrow(A)
col <- ncol(A)

c(row, col)
#+END_SRC

#+RESULTS:
| 5 |
| 2 |

Matrix multiplication:
#+BEGIN_SRC R :results output :exports both
A <- array(0:5, c(2, 3))
B <- array(1:6, c(2, 3))

A * B
#+END_SRC

#+RESULTS:
| 0 |  6 | 20 |
| 2 | 12 | 30 |

#+BEGIN_SRC R :results output :exports both
A <- array(0:5, c(2, 3))
B <- array(1:6, c(3, 2))

A %*% B
#+END_SRC

#+RESULTS:
| 16 | 34 |
| 22 | 49 |

#+BEGIN_SRC R
#The same, but more efficient, of: t(X) %*% y
crossprod(X, y)
#+END_SRC

#+BEGIN_SRC R :results output :exports both
x <- 0:4
y <- 5:9
A <- array(0:24, c(5, 5))

#Quadratic form
x %*% A %*% x
#+END_SRC

#+RESULTS:
: 1800

#+BEGIN_SRC R :results output :exports both
v <- 0:5

diag(v)
#+END_SRC

#+RESULTS:
| 0 | 0 | 0 | 0 | 0 | 0 |
| 0 | 1 | 0 | 0 | 0 | 0 |
| 0 | 0 | 2 | 0 | 0 | 0 |
| 0 | 0 | 0 | 3 | 0 | 0 |
| 0 | 0 | 0 | 0 | 4 | 0 |
| 0 | 0 | 0 | 0 | 0 | 5 |

#+BEGIN_SRC R :results output :exports both
k <- 4

diag(k)
#+END_SRC

#+RESULTS:
| 1 | 0 | 0 | 0 |
| 0 | 1 | 0 | 0 |
| 0 | 0 | 1 | 0 |
| 0 | 0 | 0 | 1 |

#+BEGIN_SRC R :results output :exports both
M <- array(0:8, c(3, 3))

diag(M)
#+END_SRC

#+RESULTS:
| 0 |
| 4 |
| 8 |

Linear equations and inversion:
#+BEGIN_SRC R :results output :exports both
A <- array(0:3, c(2, 2))
b <- 0:1

# b = A x
# x = A^{-1} b
x <- solve(A, b)
#+END_SRC

#+RESULTS:
| 1 |
| 0 |

Eigenvalues and eigenvectors
#+BEGIN_SRC R
# Assign to ev the list of this 2 components: values and vectors
Sm <- array(c(1, 2, 2, 1), c(2, 2))

ev <- eigen(Sm)

# For only the eigenvalues
evals <- eigen(Sm)$values

# If the eigenvectors are not needed
evals <- eigen(Sm, only.values = TRUE)$values
#+END_SRC

Least squares fitting and the QR decomposition
#+BEGIN_SRC R
# A list squares fit where y is the vector of observations and X is the desing matrix
# A grand mean term is automatically included and need not be included explicitly as a column of X
ans <- lsfit(X, y)
#+END_SRC

Forming partitioned matrices, ~cbind()~ and ~rbind()~
#+BEGIN_SRC R
# The arguments to cbind() must be either vectors of any length,
# or matrices with the same colums size, that is the same number of rows
X <- cbind(arg_1, arg_2, ...)

# rbind → row bind
#+END_SRC

#+BEGIN_SRC R :results output :exports both
X1 <- 0:4
X2 <- 5:9

# The 1 is shorter than the vectors, so it is cyclically extended to match the matrix column size
X <- cbind(1, X1, X2)
#+END_SRC

#+RESULTS:
| 1 | 0 | 5 |
| 1 | 1 | 6 |
| 1 | 2 | 7 |
| 1 | 3 | 8 |
| 1 | 4 | 9 |

The concatenation function, ~c()~, with arrays
#+BEGIN_SRC R
# To coerce the array vec batk to a simple vector object
vec <- as.vector(X)
# Equivalent to
vec <- c(X)
#+END_SRC

Lists and data frames:
#+BEGIN_SRC R
Lst <- list(name="Fred", wife="Mary", no.children=3, child.ages=c(4, 7, 9))

# Lst[[1]] is the first component, etc.
# Lst$name or Lst[["name"]] is also the first component
# Lst[1] is the sublist consisting of the first entry only. The correct operator is '[[...]]'
# length(Lst) is the lenght of the list
#+END_SRC

Constructing and modifying lists
#+BEGIN_SRC R
# To extend a list by specifying additional components
Lst[5] <- list(matrix=Mat)
#+END_SRC

#+BEGIN_SRC R
# Concatenating lists
list.ABC <- c(list.A, list.B, list.C)
#+END_SRC

Data frames
#+BEGIN_SRC R
accountants <- data.frame(home=statef, loot=incomes, shot=incomef)

# List to data frame: use the function as.data.frame()

# To eliminate a component
accountants$home <- NULL
#+END_SRC

#+BEGIN_SRC R
#crea un dataframe con una variabile 'quantitativa' ed una
#'qualitativa'
X <- data.frame(a=1:4, sesso=c("M", "F", "F", "M"))
X$eta <- c(2.5, 3, 5, 6.2) #aggiungi una variabile di nome eta
#+END_SRC

#+BEGIN_SRC R :results output :exports both
X <- data.frame(a=1:4, sesso=c("M", "F", "F", "M"))
X$eta <- c(2.5, 3, 5, 6.2) #aggiungi una variabile di nome eta

#seleziona i valori di "a" se eta <=5 E eta >3
X$a[X$eta <=5 & X$eta>3]
#+END_SRC

#+RESULTS:
: 3

#+BEGIN_SRC R :results output :exports both
X <- data.frame(a=1:4, sesso=c("M", "F", "F", "M"))
X$eta <- c(2.5, 3, 5, 6.2) #aggiungi una variabile di nome eta

subset(X, subset=(eta<3 | eta>5), select=c(a, sesso))
#+END_SRC

#+RESULTS:
| 1 | M |
| 4 | M |

#+BEGIN_SRC R :results output :exports both
X <- data.frame(a=1:4, sesso=c("M", "F", "F", "M"))
X$eta <- c(2.5, 3, 5, 6.2) #aggiungi una variabile di nome eta

subset(X, subset=(eta<3|eta>5), select=-eta)
#+END_SRC

#+RESULTS:
| 1 | M |
| 4 | M |

#+BEGIN_SRC R
# lentils is a data frame: lentils$u, lentils$v and lentils$w
attach(lentils)

# Do not modify lentils$u
u <- v + w

#Modify lentils$u
lentils$u <- v + w

detach(lentils)
# or detach("lentils")
#+END_SRC

Reading data from files
#+BEGIN_SRC R
HousePrice <- read.table("houses.data", header=TRUE)
#+END_SRC

#+BEGIN_SRC R
prova <- read.table("mio.txt", row.names=1)
#+END_SRC

#+BEGIN_SRC R
X <- read.table(file="c:/documenti/dati.txt",
                header=TRUE,
                sep="\t"
                na.strings="NA",
                dec=".")
# nrows → numero massimo di righe da leggere
# skip → numero di righe iniziali da saltare prima dell'importazione
#+END_SRC

#+BEGIN_SRC R
X$eta[1] <- X$sesso[4] <- NA
X2 <- X[!is.na(X$sesso)&!is.na(X$eta),]
# Si può semplicemente usare na.omit(X)
#+END_SRC

#+BEGIN_SRC R :results output :exports both
# Crea una variabile categoriale e nomina le etichette:
x <- factor(c(1, 1, 2, 3, 1), labels=c("gruppo1", "gruppo2", "gruppo3"))
x
#+END_SRC

#+RESULTS:
| gruppo1 |
| gruppo2 |
| gruppo2 |
| gruppo3 |
| gruppo1 |

#+BEGIN_SRC R :results output :exports both
x <- factor(c(1, 1, 2, 3, 1), labels=c("gruppo1", "gruppo2", "gruppo3"))

factor(x, levels=c("gruppo1", "gruppo3")) #escludi la categoria 3
#+END_SRC

#+RESULTS:
| gruppo1 |
| gruppo1 |
| nil     |
| gruppo3 |
| gruppo1 |

#+BEGIN_SRC R :results org :exports both
eta <- c(2, 4, .3, 5, .2, 6, 8, 9, .8, 4, 10, 9.5)
eta.cat <- cut(eta, breaks=c(0, 3, 5, 10),
               labels=c("basso", "medio", "alto"))
eta.cat
#+END_SRC

#+RESULTS:
| basso |
| medio |
| basso |
| medio |
| basso |
| alto  |
| alto  |
| alto  |
| basso |
| medio |
| alto  |
| alto  |

#+BEGIN_SRC R
inp <- scan("input.dat", list("", 0, 0))

label <- inp[[1]]; x <- inp[[2]]; y <- inp[[3]]
#+END_SRC

Similarly:
#+BEGIN_SRC R
inp <- scan("input.dat", list(id="", x=0, y=0))

lable «- inp$id; x <- inp$x; y <- inpt$y
#+END_SRC

#+BEGIN_SRC R
# The second arguemnt is a single value and not a list
X <- matrix(scan("light.dat", 0), ncol=5, byrow=TRUE)
#+END_SRC

#+BEGIN_SRC R
# List of datasets
data()
#+END_SRC

#+BEGIN_SRC R
# To access data from a particular package
data(package="rpart")
data(Puromycin, package="datasets")
#+END_SRC

#+BEGIN_SRC R
xnew <- edit(xold)

# fix(xold) is equivalent to
xold <- edit(xold)

# To enter new data via the spreadsheet interface
xnew <- edit(data.frame())
#+END_SRC

** Pratica
*** Statistica Descrittiva
Esempio 1.1
#+BEGIN_SRC R :file barplot.png :results graphics :exports both
# Caricare il file "esempio1.1.csv"
#f <- file.choose()
#ese11 <- scan(f, sep=";")
ese11 <- scan("./Files/esempio1.1.csv", sep=";")

# Frequenze assolute
table(ese11)

# Frequenze relative
prop.table(table(ese11))

# Frequenze cumulate assolute
cumsum(table(ese11))

# Frequenze cumulate relative
cumsum(prop.table(table(ese11)))

# Dati quantitativi - barplot
barplot(table(ese11), xlab="numero stanze", ylab="frequenza",
        main="")
#+END_SRC

#+RESULTS:
[[file:barplot.png]]

#+BEGIN_SRC R :file stripchart.png :results graphics :exports both
# Caricare il file "esempio1.1.csv"
#f <- file.choose()
#ese11 <- scan(f, sep=";")
ese11 <- scan("./Files/esempio1.1.csv",
sep=";")

# Frequenze assolute
table(ese11)

# Frequenze relative
prop.table(table(ese11))

# Frequenze cumulate assolute
cumsum(table(ese11))

# Frequenze cumulate relative
cumsum(prop.table(table(ese11)))

# Dati quantitativi - stripchart o dot plot
stripchart(ese11, method="stack", xlab="stanze") # stack
#+END_SRC

#+RESULTS:
[[file:stripchart.png]]

Esempio 1.2
#+BEGIN_SRC R :file hist1.png :results graphics :exports both
# Caricare il file "esempio1.2.csv"
ese12 <- scan("./Files/esempio1.2.csv",
sep=";", dec=",")

# minimo e massimo
min(ese12)
max(ese12)

# Dati quantitativi - hist
interval <- c(0.4, 1.5, 2.3, 3, 4, 5.5)
hist(ese12, breaks=interval)
#+END_SRC

#+RESULTS:
[[file:hist1.png]]

#+BEGIN_SRC R :file hist2.png :results graphics :exports both

# Caricare il file "esempio1.2.csv"
ese12 <- scan("./Files/esempio1.2.csv",
sep=";", dec=",")

# minimo e massimo
min(ese12)
max(ese12)

# Dati quantitativi - hist
interval <- c(0.4, 1.5, 2.3, 3, 4, 5.5)
hist(ese12)
#+END_SRC

#+RESULTS:
[[file:hist2.png]]

#+BEGIN_SRC R :file plot.png :results graphics :exports both

# Caricare il file "esempio1.2.csv"
ese12 <- scan("./Files/esempio1.2.csv",
sep=";", dec=",")

# minimo e massimo
min(ese12)
max(ese12)

# Dati quantitativi - plot
plot(ese12, xlab="appartamenti", ylab="costo")
#+END_SRC

#+RESULTS:
[[file:plot.png]]

Esercizio 1:
- Si è interessati a studiare la variabile ~years~ relativa all'età di un certo numero di giocatori di calcio.
#+BEGIN_SRC R :results output :exports both :session giocatori
giocatori <- read.table("./Files/giocatori.txt",
            sep=";", header=TRUE)

attach(giocatori)

table(years)
#+END_SRC

#+RESULTS:
: years
:  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37 
:  17  36 107 170 192 209 203 173 198 209 197 209 149 159 139 115 128  96  85  56 
:  38  39  40  41  42  43  44  46 
:  40  30  15   9   5   2   3   1

#+BEGIN_SRC R :results output :exports both :session giocatori
prop.table(table(years))
#+END_SRC

#+RESULTS:
#+begin_example
years
          18           19           20           21           22           23 
0.0057588076 0.0121951220 0.0362466125 0.0575880759 0.0650406504 0.0707994580 
          24           25           26           27           28           29 
0.0687669377 0.0586043360 0.0670731707 0.0707994580 0.0667344173 0.0707994580 
          30           31           32           33           34           35 
0.0504742547 0.0538617886 0.0470867209 0.0389566396 0.0433604336 0.0325203252 
          36           37           38           39           40           41 
0.0287940379 0.0189701897 0.0135501355 0.0101626016 0.0050813008 0.0030487805 
          42           43           44           46 
0.0016937669 0.0006775068 0.0010162602 0.0003387534
#+end_example

#+BEGIN_SRC R :results output :exports both :session giocatori
cumsum(table(years))
#+END_SRC

#+RESULTS:
:   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32   33 
:   17   53  160  330  522  731  934 1107 1305 1514 1711 1920 2069 2228 2367 2482 
:   34   35   36   37   38   39   40   41   42   43   44   46 
: 2610 2706 2791 2847 2887 2917 2932 2941 2946 2948 2951 2952

#+BEGIN_SRC R :results org :exports both :session giocatori
cumsum(prop.table(table(year)))
#+END_SRC

#+RESULTS:
#+begin_example
         18          19          20          21          22          23 
0.005758808 0.017953930 0.054200542 0.111788618 0.176829268 0.247628726 
         24          25          26          27          28          29 
0.316395664 0.375000000 0.442073171 0.512872629 0.579607046 0.650406504 
         30          31          32          33          34          35 
0.700880759 0.754742547 0.801829268 0.840785908 0.884146341 0.916666667 
         36          37          38          39          40          41 
0.945460705 0.964430894 0.977981030 0.988143631 0.993224932 0.996273713 
         42          43          44          46 
0.997967480 0.998644986 0.999661247 1.000000000
#+end_example

#+BEGIN_SRC R :results output :exports both :session giocatori
min <- min(years)
max <- max(years)

c(min, max)
#+END_SRC

#+RESULTS:
: [1] 18 46

#+BEGIN_SRC R :file istogramma.png :results graphics :exports both :session giocatori
hist(years, main="Istogramma età")
#+END_SRC

#+RESULTS:
[[file:istogramma.png]]

Esempio 1.8:
#+BEGIN_SRC R :results org :exports both
ese11 <- scan("./Files/esempio1.1.csv",
sep=";")

# Ordinamento crescente
cresc <- sort(ese11)
# Ordinamento descrescente
decr <- sort(ese11, dec=T)

# Media
media <- mean(ese11)

# Mediana
mediana <- median(ese11)

# Moda
m <- table(ese11)
moda <- m[m==max(m)]

list(cresc, decr, media, mediana, moda)
#+END_SRC

#+RESULTS:
#+begin_example
[[1]]
 [1] 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4
[39] 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 7 7 7 7
[77] 7 8 8 8

[[2]]
 [1] 8 8 8 7 7 7 7 7 6 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 4 4 4 4 4 4
[39] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2
[77] 2 2 2 2

[[3]]
[1] 4.3125

[[4]]
[1] 4

[[5]]
 4 
24 

#+end_example

#+BEGIN_SRC R :results output :exports both :session es12
# Caricare il file "esempio1.2.csv"
ese12 <- scan("./Files/esempio1.2.csv",
sep=";", dec=",")

# Varianza campionaria
var <- var(ese12)

# Deviazione standard
sd <- sd(ese12)
sqrt(var(ese12))

# Quantili
quant <- quantile(ese12, c(0.25, 0.5, 0.75))

# Range interquantile
iqr <- IQR(ese12)

# Range
range(ese12)

list(var, sd, quant, iqr, range)
#+END_SRC

#+RESULTS:
#+begin_example
Read 80 items
[1] 1.2132
[1] 0.44 5.42
[[1]]
[1] 1.471853

[[2]]
[1] 1.2132

[[3]]
  25%   50%   75% 
1.895 2.355 3.675 

[[4]]
[1] 1.78

[[5]]
function (..., na.rm = FALSE)  .Primitive("range")
#+end_example

#+BEGIN_SRC R :results output :exports both :session es12
#install.packages("e1071")
#library("e1071")
require("e1071")

# Asimmetria
skewness <- skewness(ese12)
#2 * sqrt(6/length(ese12))

# Curtosi
kurtosis <- kurtosis(ese12)
#4 * sqrt(6/length(ese12))

list(skewness, kurtosis)
#+END_SRC

#+RESULTS:
: [[1]]
: [1] 0.3226639
: 
: [[2]]
: [1] -0.8020067

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-20 19:50:29
[[file:Laboratorio/screenshot_2018-03-20_19-50-29.png]]

#+DOWNLOADED: /tmp/screenshot.png @ 2018-03-20 19:51:03
[[file:Laboratorio/screenshot_2018-03-20_19-51-03.png]]

Esercizio 2:
- Si è interessati a studiare la variabile years relativa all'età di un certo numero di giocatori di calcio
#+BEGIN_SRC R :results output :exports both
require("e1071")

giocatori <- read.table("./Files/giocatori.txt",
            sep=";", header=TRUE)

attach(giocatori)

media <- mean(years)

mediana <- median(years)

max <- max(years)

m <- table(years)
moda <- m[m==max(m)]

iqr <- IQR(years)

varianza <- var(years)

devst <- sd(years)

skewness <- skewness(years)

kurtosis <- kurtosis(years)

list(media, mediana, max, moda, iqr, varianza, devst, skewness, kurtosis)
#+END_SRC

#+RESULTS:
#+begin_example
[[1]]
[1] 27.74898

[[2]]
[1] 27

[[3]]
[1] 46

[[4]]
years
 23  27  29 
209 209 209 

[[5]]
[1] 7

[[6]]
[1] 26.43409

[[7]]
[1] 5.141409

[[8]]
[1] 0.3825924

[[9]]
[1] -0.5454063

#+end_example

Esempio 1.12
#+BEGIN_SRC R :results output :exports both :session es112
# Caricare il file "esempio 1.12.csv"
ese112 <- read.csv("./Files/esempio1.12.csv",
                   sep=";", header=TRUE)

# Frequenze assolute
tc <- table(ese112)
tc
#+END_SRC

#+RESULTS:
:       Occupanti
: Stanze 2 3 4 5 7
:      2 3 1 0 0 0
:      3 5 2 2 0 0
:      4 1 5 4 2 0
:      5 1 3 2 0 1
:      6 0 0 2 0 1

#+BEGIN_SRC R :results output :exports both :session es112
# Tabella di contingenza con distribuzioni assolute marginali
tcc <- cbind(tc, margin.table(tc, 1)) # marginale stanze
tcc
#+END_SRC

#+RESULTS:
:   2 3 4 5 7   
: 2 3 1 0 0 0  4
: 3 5 2 2 0 0  9
: 4 1 5 4 2 0 12
: 5 1 3 2 0 1  7
: 6 0 0 2 0 1  3

#+BEGIN_SRC R :results output :exports both :session es112
rbind(tcc, margin.table(tcc, 2)) # marginale occupanti
#+END_SRC

#+RESULTS:
:    2  3  4 5 7   
: 2  3  1  0 0 0  4
: 3  5  2  2 0 0  9
: 4  1  5  4 2 0 12
: 5  1  3  2 0 1  7
: 6  0  0  2 0 1  3
:   10 11 10 2 2 35

#+BEGIN_SRC R :results output :exports both :session es112
# Frequenze relative
tcr <- prop.table(table(ese112))
tcr
#+END_SRC

#+RESULTS:
:       Occupanti
: Stanze          2          3          4          5          7
:      2 0.08571429 0.02857143 0.00000000 0.00000000 0.00000000
:      3 0.14285714 0.05714286 0.05714286 0.00000000 0.00000000
:      4 0.02857143 0.14285714 0.11428571 0.05714286 0.00000000
:      5 0.02857143 0.08571429 0.05714286 0.00000000 0.02857143
:      6 0.00000000 0.00000000 0.05714286 0.00000000 0.02857143

#+BEGIN_SRC R :results output :exports both :session es112
# Tabella di contingenza con distribuzioni relative marginali
tccr <- cbind(tcr, margin.table(tcr, 1)) # marginale stanze
rbind(tccr, margin.table(tccr, 2)) # marginale occupanti
#+END_SRC

#+RESULTS:
:            2          3          4          5          7           
: 2 0.08571429 0.02857143 0.00000000 0.00000000 0.00000000 0.11428571
: 3 0.14285714 0.05714286 0.05714286 0.00000000 0.00000000 0.25714286
: 4 0.02857143 0.14285714 0.11428571 0.05714286 0.00000000 0.34285714
: 5 0.02857143 0.08571429 0.05714286 0.00000000 0.02857143 0.20000000
: 6 0.00000000 0.00000000 0.05714286 0.00000000 0.02857143 0.08571429
:   0.28571429 0.31428571 0.28571429 0.05714286 0.05714286 1.00000000

#+BEGIN_SRC R :file bubbleplot.png :results graphics :exports both :session es112
#install.packages("labstatR")
require("labstatR")
bubbleplot(tc)
#+END_SRC

#+RESULTS:
[[file:bubbleplot.png]]

#+BEGIN_SRC R :file scatterplot.png :results graphics :exports both :session es112
plot(ese112$Occupanti, ese112$Stanze, xlab="Occupanti", ylab="Stanze", main="Scatterplot")
#+END_SRC

#+RESULTS:
[[file:scatterplot.png]]

#+BEGIN_SRC R :file barplot.png :results graphics :exports both :session es112
barplot(table(ese112), legend=T, col=c("lightblue", "mistyrose", "lightcyan",
                                       "lavender", "cornsilk"), xlab="Occupanti",
        ylab="Frequenze", args.legend=list(x="topright", title="Stanze"))
#+END_SRC

#+RESULTS:
[[file:barplot.png]]

#+BEGIN_SRC R :results output :exports both :session es112
# Covarianza
cov(ese112$Stanze, ese112$Occupanti)

# Correlazione
cor(ese112$Stanze, ese112$Occupanti)
#+END_SRC

#+RESULTS:
: [1] 0.805042
: [1] 0.5548666

Esercizio 3:
#+BEGIN_SRC R barplot.png :results output :exports both :session giocatori
giocatori <- read.table("./Files/giocatori.txt",
            sep=";", header=TRUE)

attach(giocatori)

tc <- table(role, years)
tc

tcc <- cbind(tc, margin.table(tc, 1))
tcc

rbind(tcc, margin.table(tcc, 2))
#+END_SRC

#+RESULTS:
#+begin_example
The following objects are masked from giocatori (pos = 3):

    birth_date, name, player_id, role, year_date, years

The following objects are masked from giocatori (pos = 4):

    birth_date, name, player_id, role, year_date, years

The following objects are masked from giocatori (pos = 5):

    birth_date, name, player_id, role, year_date, years
            years
role          18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33
  defence      3   6  34  47  47  58  51  50  75  69  59  65  60  52  47  40
  goalkeeper   2   6  15  22  29  23  20  18  18  21  13  15  11  15  16  16
  midfield    10  19  44  77  98  95 112  93  93 103 110 109  71  80  63  53
  striker      1   4   8  19  18  30  18  12  12  16  15  20   7  12  13   6
  unknown      1   1   6   5   0   3   2   0   0   0   0   0   0   0   0   0
            years
role          34  35  36  37  38  39  40  41  42  43  44  46
  defence     46  41  33  24  16   9   2   1   0   0   0   0
  goalkeeper  14   8  13  10   5   7   5   7   2   2   3   1
  midfield    55  40  33  19  17  12   7   0   3   0   0   0
  striker     13   7   6   2   2   2   1   1   0   0   0   0
  unknown      0   0   0   1   0   0   0   0   0   0   0   0
           18 19 20 21 22 23  24 25 26  27  28  29 30 31 32 33 34 35 36 37 38
defence     3  6 34 47 47 58  51 50 75  69  59  65 60 52 47 40 46 41 33 24 16
goalkeeper  2  6 15 22 29 23  20 18 18  21  13  15 11 15 16 16 14  8 13 10  5
midfield   10 19 44 77 98 95 112 93 93 103 110 109 71 80 63 53 55 40 33 19 17
striker     1  4  8 19 18 30  18 12 12  16  15  20  7 12 13  6 13  7  6  2  2
unknown     1  1  6  5  0  3   2  0  0   0   0   0  0  0  0  0  0  0  0  1  0
           39 40 41 42 43 44 46     
defence     9  2  1  0  0  0  0  935
goalkeeper  7  5  7  2  2  3  1  337
midfield   12  7  0  3  0  0  0 1416
striker     2  1  1  0  0  0  0  245
unknown     0  0  0  0  0  0  0   19
           18 19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34 35
defence     3  6  34  47  47  58  51  50  75  69  59  65  60  52  47  40  46 41
goalkeeper  2  6  15  22  29  23  20  18  18  21  13  15  11  15  16  16  14  8
midfield   10 19  44  77  98  95 112  93  93 103 110 109  71  80  63  53  55 40
striker     1  4   8  19  18  30  18  12  12  16  15  20   7  12  13   6  13  7
unknown     1  1   6   5   0   3   2   0   0   0   0   0   0   0   0   0   0  0
           17 36 107 170 192 209 203 173 198 209 197 209 149 159 139 115 128 96
           36 37 38 39 40 41 42 43 44 46     
defence    33 24 16  9  2  1  0  0  0  0  935
goalkeeper 13 10  5  7  5  7  2  2  3  1  337
midfield   33 19 17 12  7  0  3  0  0  0 1416
striker     6  2  2  2  1  1  0  0  0  0  245
unknown     0  1  0  0  0  0  0  0  0  0   19
           85 56 40 30 15  9  5  2  3  1 2952
#+end_example

#+BEGIN_SRC R :file barplotgiocatori.png :results graphics :exports both :session giocatori
barplot(tc, legend=T, col=c("lightblue", "mistyrose", "lightcyan",
                                       "lavender", "cornsilk"), xlab="Età",
        ylab="Frequenze", args.legend=list(x="topright", title="Ruoli"))
#+END_SRC

#+RESULTS:
[[file:barplotgiocatori.png]]

Esempio 1.16
#+BEGIN_SRC R :results output :exports both :session es116
ese116 <- read.csv("./Files/esempio1.16.csv", sep=";", header=TRUE)
ese116
#+END_SRC

#+RESULTS:
#+begin_example
   CaricoPrimaLesione.x. CaricoRottura.y.
1                   2550             4650
2                   2900             4650
3                   3000             4700
4                   3000             4750
5                   3000             4775
6                   3000             4775
7                   3250             4800
8                   3250             4950
9                   3250             5050
10                  3600             5100
11                  4225             5100
12                  4650             5150
13                  4750             5175
14                  5175             5250
15                  5300             5300
#+end_example

#+BEGIN_SRC R :results output :exports both :session es116
# Scarto quadratico medio x
sqrt(mean((ese116$CaricoPrimaLesione.x. - mean(ese116$CaricoPrimaLesione.x.))^2))

# Scarto quadratico medio y
sqrt(mean((ese116$CaricoRottura.y. - mean(ese116$CaricoRottura.y.))^2))

# Covarianza
mean((ese116$CaricoPrimaLesione.x. - mean(ese116$CaricoPrimaLesione.x.)) *
     (ese116$CaricoRottura.y - mean(ese116$CaricoRottura.y)))

# Correlazione
cov(ese116$CaricoPrimaLesione.x., ese116$CaricoRottura.y.)/
    (sd(ese116$CaricoPrimaLesione.x.) * sd(ese116$CaricoRottura.y.))
#+END_SRC

#+RESULTS:
: [1] 876.8219
: [1] 219.6968
: [1] 177133.3
: [1] 0.9195286

#+BEGIN_SRC R :results output :exports both :session es116
# Retta di regressione
rr <- lm(ese116$CaricoRottura.y.~ ese116$CaricoPrimaLesione.x.)
rr
#+END_SRC

#+RESULTS:
: 
: Call:
: lm(formula = ese116$CaricoRottura.y. ~ ese116$CaricoPrimaLesione.x.)
: 
: Coefficients:
:                  (Intercept)  ese116$CaricoPrimaLesione.x.  
:                    4101.7456                        0.2304

#+BEGIN_SRC R :file regressionelineare.png :results graphics :exports both :session es116
plot(ese116$CaricoPrimaLesione.x., ese116$CaricoRottura.y., xlab="Carico prima lesione", ylab="Carico rottura")
abline(rr, col="red", lwd=2)
#+END_SRC

#+RESULTS:
[[file:regressionelineare.png]]

#+BEGIN_SRC R :results output :exports both :session es116
# Retta di regressione (non lineare)
#rrnl <- lm(ese116$CaricoRottura.y.~1 + ese116$CaricoPrimaLesione.x. +
#           l(ese116$CaricoPrimaLesione.x.^2))
rrnl <- lm(ese116$CaricoRottura.y~ poly(ese116$CaricoPrimaLesione.x., 2, raw=T))
rrnl
#+END_SRC

#+RESULTS:
#+begin_example

Call:
lm(formula = ese116$CaricoRottura.y ~ poly(ese116$CaricoPrimaLesione.x., 
    2, raw = T))

Coefficients:
                                    (Intercept)  
                                      2.893e+03  
poly(ese116$CaricoPrimaLesione.x., 2, raw = T)1  
                                      8.743e-01  
poly(ese116$CaricoPrimaLesione.x., 2, raw = T)2  
                                     -8.108e-05
#+end_example

#+BEGIN_SRC R :file regressionenonlineare.png :results graphics :exports both :session es116
plot(ese116$CaricoPrimaLesione.x., ese116$CaricoRottura.y., xlab="Carico prima lesione", ylab="Carico rottura")
lines(ese116$CaricoPrimaLesione.x., predict(rrnl), col="green", lwd=2)
#+END_SRC

#+RESULTS:
[[file:regressionenonlineare.png]]

Esercizio 4:
#+BEGIN_SRC R :results output :exports both :session es4
x <- c(4, 8, 9, 12, 7)
y <- c(17, 24, 25, 28, 26)

rr <- lm(y~ x)
rr
#+END_SRC

#+RESULTS:
: 
: Call:
: lm(formula = y ~ x)
: 
: Coefficients:
: (Intercept)            x  
:      13.882        1.265

#+BEGIN_SRC R :file esregressionelineare.png :results graphics :exports both :session es4
plot(x, y, xlab="x", ylab="y")
abline(rr, col="red", lwd=2)
#+END_SRC

#+RESULTS:
[[file:esregressionelineare.png]]
*** Calcolo delle Probabilità
Lo spazio campione in R è di solito rappresentato da un data frame:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Per scaricare un pacchetto archiviato
# require(devtools)
# devtools::install_url('https://cran.r-project.org/src/contrib/Archive/prob/prob_1.0-0.tar.gz')

# Caricare il package prob
library(prob)
# Creare spazio campione
tosscoin
cards
rolldie
urnsamples

t <- tosscoin(2)
t
#+END_SRC

#+RESULTS:
#+begin_example
Carico il pacchetto richiesto: combinat

Attaching package: ‘combinat’

The following object is masked from ‘package:utils’:

    combn

Carico il pacchetto richiesto: fAsianOptions
Carico il pacchetto richiesto: timeDate
Carico il pacchetto richiesto: timeSeries
Carico il pacchetto richiesto: fBasics
Carico il pacchetto richiesto: fOptions

Attaching package: ‘prob’

The following objects are masked from ‘package:base’:

    intersect, setdiff, union
function (times, makespace = FALSE) 
{
    temp <- list()
    for (i in 1:times) {
        temp[[i]] <- c("H", "T")
    }
    res <- expand.grid(temp, KEEP.OUT.ATTRS = FALSE)
    names(res) <- c(paste(rep("toss", times), 1:times, sep = ""))
    if (makespace) 
        res$probs <- rep(1, 2^times)/2^times
    return(res)
}
<environment: namespace:prob>
function (jokers = FALSE, makespace = FALSE) 
{
    x <- c(2:10, "J", "Q", "K", "A")
    y <- c("Club", "Diamond", "Heart", "Spade")
    res <- expand.grid(rank = x, suit = y)
    if (jokers) {
        levels(res$rank) <- c(levels(res$rank), "Joker")
        res <- rbind(res, data.frame(rank = c("Joker", "Joker"), 
            suit = c(NA, NA)))
    }
    if (makespace) {
        res$probs <- rep(1, dim(res)[1])/dim(res)[1]
    }
    return(res)
}
<environment: namespace:prob>
function (times, nsides = 6, makespace = FALSE) 
{
    temp = list()
    for (i in 1:times) {
        temp[[i]] <- 1:nsides
    }
    res <- expand.grid(temp, KEEP.OUT.ATTRS = FALSE)
    names(res) <- c(paste(rep("X", times), 1:times, sep = ""))
    if (makespace) 
        res$probs <- rep(1, nsides^times)/nsides^times
    return(res)
}
<environment: namespace:prob>
function (x, ...) 
UseMethod("urnsamples")
<environment: namespace:prob>
  toss1 toss2
1     H     H
2     T     H
3     H     T
4     T     T
#+end_example

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Spazio campione del lancio di un dado bilanciato a 6 facce
r <- rolldie(1)
r
#+END_SRC

#+RESULTS:
:   X1
: 1  1
: 2  2
: 3  3
: 4  4
: 5  5
: 6  6

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Spazio campione (primi 6 elementi) di un mazzo composto da 52 carte
c <- cards()
head(c)
#+END_SRC

#+RESULTS:
:   rank suit
: 1    2 Club
: 2    3 Club
: 3    4 Club
: 4    5 Club
: 5    6 Club
: 6    7 Club

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Spazio campione di un'urna di 3 palline numerate da 1 a 3 con estrazione 2 palline
u <- urnsamples(1:3, size=2, replace=TRUE, ordered=TRUE)
u
#+END_SRC

#+RESULTS:
#+begin_example
  X1 X2
1  1  1
2  2  1
3  3  1
4  1  2
5  2  2
6  3  2
7  1  3
8  2  3
9  3  3
#+end_example

#+BEGIN_SRC R :results output :exports both :session calcoloProb
urnsamples(1:3, size=2, replace=FALSE, ordered=TRUE)
#+END_SRC

#+RESULTS:
:   X1 X2
: 1  1  2
: 2  2  1
: 3  1  3
: 4  3  1
: 5  2  3
: 6  3  2

#+BEGIN_SRC R :results output :exports both :session calcoloProb
urnsamples(1:3, size=2, replace=FALSE, ordered=FALSE)
#+END_SRC

#+RESULTS:
:   X1 X2
: 1  1  2
: 2  1  3
: 3  2  3

#+BEGIN_SRC R :results output :exports both :session calcoloProb
urnsamples(1:3, size=2, replace=TRUE, ordered=FALSE)
#+END_SRC

#+RESULTS:
:   X1 X2
: 1  1  1
: 2  1  2
: 3  1  3
: 4  2  2
: 5  2  3
: 6  3  3

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Accedere ad alcuni elementi
u[c(2, 4),]
#+END_SRC

#+RESULTS:
:   X1 X2
: 2  2  1
: 4  1  2

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Accedere a sottoinsiemi con funzione subset
# Estrarre solo le carte di seme Spade
x <- subset(c, suit=="Spade")
x
#+END_SRC

#+RESULTS:
#+begin_example
   rank  suit
40    2 Spade
41    3 Spade
42    4 Spade
43    5 Spade
44    6 Spade
45    7 Spade
46    8 Spade
47    9 Spade
48   10 Spade
49    J Spade
50    Q Spade
51    K Spade
52    A Spade
#+end_example

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# %in%
# Estrarre solo le carte 5 e 6
subset(c, rank %in% 5:6)
# subset(c, rank==6 | rank==5)
#+END_SRC

#+RESULTS:
:    rank    suit
: 4     5    Club
: 5     6    Club
: 17    5 Diamond
: 18    6 Diamond
: 30    5   Heart
: 31    6   Heart
: 43    5   Spade
: 44    6   Spade

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# isin()
x1 <- 1:10
y1 <- 8:12
r1 <- isin(x1, y1)

x2 <- 1:10
y2=c(3, 3, 7)
r2 <- isin(x2, y2)

x3 <- 1:10
y3=c(3, 3, 7)
r3 <- all(y3 %in% x3)

c(r1, r2, r3)
#+END_SRC

#+RESULTS:
: [1] FALSE FALSE  TRUE

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Espressioni matematiche
# Somma delle facce dei 3 dadi maggiore di 14
subset(rolldie(3), X1+X2+X3>14)
#+END_SRC

#+RESULTS:
#+begin_example
    X1 X2 X3
108  6  6  3
138  6  5  4
143  5  6  4
144  6  6  4
168  6  4  5
173  5  5  5
174  6  5  5
178  4  6  5
179  5  6  5
180  6  6  5
198  6  3  6
203  5  4  6
204  6  4  6
208  4  5  6
209  5  5  6
210  6  5  6
213  3  6  6
214  4  6  6
215  5  6  6
216  6  6  6
#+end_example

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Somma delle due facce sia numero pari (%% modulo)
subset(rolldie(2), ((X1+X2)%%2)==0)
#+END_SRC

#+RESULTS:
#+begin_example
   X1 X2
1   1  1
3   3  1
5   5  1
8   2  2
10  4  2
12  6  2
13  1  3
15  3  3
17  5  3
20  2  4
22  4  4
24  6  4
25  1  5
27  3  5
29  5  5
32  2  6
34  4  6
36  6  6
#+end_example

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Faccia del primo dado maggiore di quella del secondo
subset(rolldie(2), (X1>X2))
#+END_SRC

#+RESULTS:
#+begin_example
   X1 X2
2   2  1
3   3  1
4   4  1
5   5  1
6   6  1
9   3  2
10  4  2
11  5  2
12  6  2
16  4  3
17  5  3
18  6  3
23  5  4
24  6  4
30  6  5
#+end_example

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Insiemistica
# Unione di due subset A, B
A <- 0:3
B <- 3:5
union(A, B)
#+END_SRC

#+RESULTS:
: [1] 0 1 2 3 4 5

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Intersezione tra A, B
intersect(A, B)
#+END_SRC

#+RESULTS:
: [1] 3

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Differenza tra A, B
setdiff(A, B)
#+END_SRC

#+RESULTS:
: [1] 0 1 2

#+BEGIN_SRC R
# isrep(oggetto, valore, ripetizione)
# verifica se in N compare 3 volte il valore red
isrep(N, vals="red", nrep=3)
#+END_SRC

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Spazio di probabilità
tosscoin(2, makespace=TRUE)
#+END_SRC

#+RESULTS:
:   toss1 toss2 probs
: 1     H     H  0.25
: 2     T     H  0.25
: 3     H     T  0.25
: 4     T     T  0.25

#+BEGIN_SRC R
# Hanno l'argomento makespace
cards()
rolldie()
#+END_SRC

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Spazio di probabilità
# probspace(spazio campione, probabilità)
# analogo a rolldie(1, makespace=TRUE)
outcome=rolldie(1)
p=rep(1/6, times=6)
probspace(outcome, probs=p)
#+END_SRC

#+RESULTS:
:   X1     probs
: 1  1 0.1666667
: 2  2 0.1666667
: 3  3 0.1666667
: 4  4 0.1666667
: 5  5 0.1666667
: 6  6 0.1666667

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Moneta sbilanciata
probspace(tosscoin(1), probs=c(0.3, 0.7))
# iidspace(c("H", "T"), ntrials=1, probs=c(0.3, 0.7))
#+END_SRC

#+RESULTS:
:   toss1 probs
: 1     H   0.3
: 2     T   0.7

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Calcolare la probabilità di un evento
# Prob(spazio di probabilità, evento)
S <- cards(makespace=TRUE)
A <- subset(S, suit=="Heart")
Prob(A)
# Prob(S, suit=="Heart")
#+END_SRC

#+RESULTS:
: [1] 0.25

#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Permutazioni
# factorial(n)

# Combinazioni semplici
# choose(n, k)
# Numero di combinazioni di 8 elementi presi a gruppi di 2
choose(8, 2)
#+END_SRC

#+RESULTS:
: [1] 28

Esercizio 1:
- Supponiamo che 10 carte numerate da 1 a 10 vengano introdotte in un cappello e che una carta venga estratta a caso.
  Vogliamo determinare:
  - Qual'è la probabilità che la carta estratta sia 10?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
carte <- urnsamples(1:10, size=1)
prob <- rep(1/10, times=10)
S <- probspace(carte, probs=prob)
Prob(S, out==10)
#+END_SRC

#+RESULTS:
: [1] 0.1

Esercizio 3:
- Si supponga che un'urna contenga 7 palline bianche e 5 nere. Supponiamo di estrarre 2 palline senza reimmissione.
  Assumendo che ogni pallina possa essere estratta con egual probabilità,
  - Qual'è la probabilità che entrambe le palline estratte siano bianche?

#+BEGIN_SRC R :results output :exports both :session calcoloProb
L=rep(c("white", "black"), times=c(7, 5))
urn <- urnsamples(L, size=2, replace=FALSE, ordered=FALSE)
space_urn <- probspace(urn)
Prob(space_urn, isrep(space_urn, "white", 2))
#+END_SRC

#+RESULTS:
: [1] 0.3181818

Esercizio 10:
- Un'urna contiene 3 biglie bianche e 2 biglie nere:
  1. Calcolare la probabilità che estraendo in successione (senza reimbussolamento) 3 biglie almeno una sia nera;
  2. Ripetere il punto precedente supponendo il reimbussolamento.

1:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urn <- urnsamples(c("B", "B", "B", "N", "N"), size=3, ordered=TRUE)
space_urn <- probspace(urn)
Prob(space_urn, X1=="N" | X2=="N" | X3=="N")
#+END_SRC

#+RESULTS:
: [1] 0.9

2:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urn <- urnsamples(c("B", "B", "B", "N", "N"), size=3, ordered=TRUE, replace=TRUE)
space_urn <- probspace(urn)
Prob(space_urn, X1=="N" | X2=="N" | X3=="N")
#+END_SRC

#+RESULTS:
: [1] 0.784

Esercizio 19:
- Da un mazzo regolare di 52 carte vengono estratte contemporaneamente 3 carte.
  - Qual'è la probabilità che le 3 carte estratte siano tutte di picche?

#+BEGIN_SRC R :results output :exports both :session calcoloProb
carte <- urnsamples(cards(), 3, replace=FALSE, ordered=FALSE)
carte_space <- probspace(carte)
Prob(carte_space, all(suit=="Club"))
#+END_SRC

Esercizio 4:
- Supponiamo che tre amici ad una festa gettino il proprio cappello sulla stessa sedia. Questi cappelli vengono
  mescolati tra loro e, successivamente, i tre amici scelgono un cappello a caso.
  - Qual'è la probabilità che nessuno di loro venga in possesso del proprio cappello?

#+BEGIN_SRC R :results output :exports both :session calcoloProb
cappelli <- urnsamples(c("Cappello 1", "Cappello 2", "Cappello 3"), 3, replace=FALSE, ordered=TRUE)
cappelli_space <- probspace(cappelli)
Prob(cappelli_space, X1!="Cappello 1" & X2!="Cappello 2" & X3!="Cappello 3")
#+END_SRC

#+RESULTS:
: [1] 0.3333333

Esercizio 5:
- Viene estratta una pallina da un'urna che contiene 4 palline numerate da 1 a 4. Siano $E = \{1, 2\}, F = \{1, 3\}, G = \{1, 4\}$.
  Calcolare:
  1. $P(E)$;
  2. $P(E \cap G)$;
  3. $P(F \cap G)$;
  4. $P(E \cap F \cap G)$.

1:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urna <- urnsamples(1:4, 1)
urna_space <- probspace(urna)
E <- c(1, 2)
F <- c(1, 3)
G <- c(1, 4)
Prob(urna_space, out %in% E)
#+END_SRC

#+RESULTS:
: [1] 0.5

2:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
Prob(urna_space, out %in% intersect(E, G))
#+END_SRC

#+RESULTS:
: [1] 0.25

3:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
Prob(urna_space, out %in% intersect(F, G))
#+END_SRC

#+RESULTS:
: [1] 0.25

4:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
Prob(urna_space, out %in% intersect(E, F, G))
#+END_SRC

#+RESULTS:
: [1] 0.25

Esercizio 10:
- Un'urna contiene 3 biglie bianche e 2 biglie nere.
  1. Calcolare la probabilità che estraendo in successione (senza reimbussolamento) 3 biglie almeno una sia nera;
  2. Ripetere il punto precedente supponendo il reimbussolamento.

#+BEGIN_SRC R
# Un'urna contiene 'b' (ex: b=3) biglie bianche e n (ex:n=2) biglie nere. 


# a) Calcolare la probabilità che estraendo in successione 
# (senza reimbussolamento) 3 biglie almeno una sia 'k' (ex: nera).




# b) Ripetere il punto precedente supponendo il reimbussolamento.



#---------------------------------------------------------


#importo la libreria prob
library(prob)



# PARAMETRI 
#b = num palline bianche nell'urna
#n = num palline nere nell'urna
#x = num di palline che estraiamo (ve volessimo aumentare il numero 
#    di palline estratte dovremmo modificare questo valore)
#k = colore di cui vogliamo calcolare la probabilità di estrarre almeno una pallina

b <- 3
n <- 2

x <- 3
k <- 'n'

#DATI

# inizializzazione dell'array relativo all'urna
y <- NULL


# Cicli che inseriscono elementi nell'urna

for (i in 1 : b){
  y <- append(y, "b");
}

for (i in 1 : n){
  y <- append(y, "n");
}




#------------------------------------------

#PROCEDIMENTO:

# a) Calcolatre la probabilità che estarendo in succesione (senza reimbussolamento)
# x biglie almeno una sia k

#CREAZIONE DELLO SPAZIO DELLE PROBABILITà

# a-1) Creazione dello "Spazio degli eventi relativo all'urna"

urn1 = urnsamples(y, x, replace = FALSE, ordered=TRUE)

numel = nrow(urn1)

# a-2) Generiamo un array indicante, in ogni posizione "i", la probabilità di 
# verificarsi dell'i-esimo evento dello "Spazio degli eventi" sopra definito.
# numel = numero degli elementi nell'urna

p <- rep(1/numel, times=numel)


# creazione dello spazio di probabilità

space_urn1 = probspace(urn1, p)



#SOLUZIONE: Probabilità che entrambe le palline estratte siano del tipo "k" 

#---> se volessimo estrarre un numero di palline maggiore di 3 dovremmo aggiungere
#nuove condizioni Xi == k come secondo argomento della funzione Prob.

#ad esempio se volessimo estrarre 4 palline la funzione prob diventerebbe:
#out <- Prob(space_urn1, X1==k | X2 == k | X3 == k | X4==k)

out <- Prob(space_urn1, X1==k | X2 == k | X3 == k)








# b) Ripetere il punto precedente supponendo il reimbussolamento

#CREAZIONE DELLO SPAZIO DELLE PROBABILITà

# b-1) Creazione dello "Spazio degli eventi relativo all'urna"

urn2 = urnsamples(y, x, replace = TRUE, ordered=TRUE)

numel2 = nrow(urn2)

# a-2) Generiamo un array indicante, in ogni posizione "i", la probabilità di 
# verificarsi dell'i-esimo evento dello "Spazio degli eventi" sopra definito.
# numel = numero degli elementi nell'urna

p2 <- rep(1/numel2, times=numel2)


# creazione dello spazio di probabilità

space_urn2 = probspace(urn2, p2)



#SOLUZIONE: Probabilità che entrambe le palline estratte siano del tipo "k" 
#---> se volessimo estrarre un numero di palline maggiore di 3 dovremmo aggiungere
#nuove condizioni Xi == k come secondo argomento della funzione Prob.

#ad esempio se volessimo estrarre 4 palline la funzione prob diventerebbe:
#out2 <- Prob(space_urn2, X1==k | X2 == k | X3 == k | X4== k)

out2 <- Prob(space_urn2, X1==k | X2 == k | X3 == k)
#+END_SRC
Esercizio 11:
- Una squadra di calcio chiera ad ogni partita 1 portiere, 5 difensori e 5 attaccanti. La sociatà "Aleas" sceglie
  in modo casuale ciascun gruppo di giocatori tra 2 portieri, 8 difensori e 12 attaccanti disponibili.
  1. Quante sono le formazioni possibili?
  2. Se Roberto e Ronaldo sono due attaccanti, quante sono le formazioni in cui giocano entrambi?
  3. Se Franco è un difensore, quante sono le formazioni in cui gioca con l'attaccante Roberto?

1:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
p <- 2
d <- 8
a <- 12

cp <- choose(p, 1)
cd <- choose(d, 5)
da <- choose(a, 5)

# Numero di formazioni possibili
cp*cd*da
#+END_SRC

#+RESULTS:
: [1] 88704

2:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
ca <- choose(a-2, 3)

# Numero di formazioni in cui giocano 2 attaccanti scelti
cp*cd*ca 
#+END_SRC

#+RESULTS:
: [1] 13440

3:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Ridondante: sono uguali
ca <- choose(a-1, 4)
cd <- choose(d-1, 4)

# Numero di formazioni in cui giocano 1 attaccante e un difensore scelto
cp*cd*ca
#+END_SRC

#+RESULTS:
: [1] 23100

Esercizio 17:
- Si supponga di estrarre contemporaneamente 13 carte da un mazzo di 52. Si calcoli la probabilità che le carte
  estratte contengano:
  1. Il 3 di cuori;
  2. Una sola carta di quadri;
  3. 3 carte di picche e 5 carte di quadri;
  4. Solo 3 figure.

1:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
combTot <- choose(52, 13)
comb3Cuori <- choose(51, 12)

# Probabilità di estrarre il 3 di cuori
comb3Cuori/combTot
#+END_SRC

#+RESULTS:
: [1] 0.25

2:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
comb1CartaQuadri <- 13*choose(39, 12)

# Probabilità di estrarre solo una carta di quadri
comb1CartaQuadri/combTot
#+END_SRC

#+RESULTS:
: [1] 0.08006186

3:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
comb3Picche5Quadri <- choose(13, 3)*choose(13, 5)*choose(26, 5)

# Probabilità di estrarre 3 carte di picche e 5 di quadri
comb3Picche5Quadri/combTot
#+END_SRC

#+RESULTS:
: [1] 0.038129

4:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
comb3Figure <- choose(12, 3)*choose(40, 10)

# Probabilità di estrarre solo 3 figure
comb3Figure/combTot
#+END_SRC

#+RESULTS:
: [1] 0.2936714
Esercizio 29:
- Un mazzo di carte napoletane costituito da 40 carte suddivise in 4 classi (detti "semi") ciascuna contenente 10 carte
  numerate. Supponiamo di estrarre "a caso" 8 carte da un tale mazzo.
  1. Calcolare la probabilità di estrarre i 4 "sette";
  2. Calcolare la probabilità di estrarre al più 2 "sei";
  3. Calcolare la probabilità di estrarre "4 sette" e al più 2 "sei".

1:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
combTot <- choose(40, 8)
comb4Sette <- choose(36, 4)

# Probabilità di estrarre i 4 "sette"
prob4Sette <- comb4Sette/combTot
prob4Sette
#+END_SRC

#+RESULTS:
: [1] 0.0007659481

2:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
#combMax2Sei <- choose(36, 8) + 4*choose(36, 7) + choose(4, 2)*choose(36, 6)
combMin3Sei <- choose(4, 3)*choose(36, 5) + choose(36, 4)

# Probabilità di estrarre al più 2 "sei"
#combMax2Sei/combTot
probMax2Sei <- 1 - combMin3Sei/combTot
probMax2Sei
#+END_SRC

#+RESULTS:
: [1] 0.9796258

3:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
probTot4Carte <- choose(36, 4)
combMin3Sei4Carte <- choose(4, 3)*choose(36, 1) + 1
probMax2Sei4Carte <- 1 - combMin3Sei4Carte/probTot4Carte

prob4SetteMax2Sei <- prob4Sette*probMax2Sei4Carte
prob4SetteMax2Sei
#+END_SRC

#+RESULTS:
: [1] 0.0007640627

#+BEGIN_SRC R
# Esercizio 29

# Un mazzo di carte napoletane costituito da 40 carte suddivise in 4 classi (detti "semi") 
# ciascuna contenente 10 carte numerate. Supponiamo di estrarre "a caso" n carte da un tale mazzo.

# a) Calcolare la probabilità di estrarre n7 (ex: n7=4) "sette".

# b) Calcolare la probabilità di estrarre al più n6 (ex: n6=2) "sei".

# c) Calcolare la probabilità di estrarre "n7 sette" e al più n6 "sei".


#---------------------------------------------------------

#importo la libreria prob
library(prob)


# PARAMETRI 

# n =  numero di carte estratte dal mazzo

# n7 = numero di sette nel punto a) e c)

# n6 = numero di sei nel punto b) e c)


n <- 8

n7 <- 4

n6 <- 2

# -----------------------------


#SOLUZIONE

# a) probabilità di estrarre n7 sette


# possibili combinazioni di n elementi estratti dal mazzo di 40 carte

CasiPoss <- choose(40,8)


# numero di passibili combinazioni in cui n7 sono carte di "sette"
# mentre (n - n7) carte non sono "sette"

CasiFavA <- choose(4, n7)*choose(40-4, n-n7) 

# soluzione:

out1 <- CasiFavA/CasiPoss 


#-------------------------------

# b) Calcolare la probabilità di estrarre al più n6 "sei"

# p conterrà la probabilità di estrarre "i" n6

p <- NULL

# per ogni i -> p[i] = probabilità di estrarre 'i' carte di "sei"
for (i in 0:n6){
  p[i+1] = (choose(4,i)*choose(40-4, n-i) ) / CasiPoss
}

# sommo le probabilità sopra generate
out2 <- sum(p)


#-------------------------------


# c) Calcolare la probabilità di estrarre n7 "sette" e n6 "sei"

# dobbiamo calcolare le probabilità congiunte:

# per ogni i -> pA[i] = probabilità di estrarre 'i' carte di "sei" e n7 carte di "sette"
pA <- NULL

# pA contiene, per ogni 'i', la probabilità di esatrarre n7 
# carte di "sette" ed i carte di "sei"

for (i in 0:n6){
  pA[i+1] = (choose(4,n7)*choose(4,i)*choose(40-4-4, n-i-n7) ) / CasiPoss
}

out3 <- sum(pA)
#+END_SRC
Esercizio 30:
- Consideriamo un mazzo di 40 carte suddivise in 4 classi (detti "semi") ciascuna contenente 10 carte numerate
  da 1 a 10. Ogni mano servita è formata da 5 carte estratte dal mazzo.
  1. Calcolare la probabilità di ricevere una mano che contiene i numeri 6, 7, 8, 9, 10;
  2. Calcolare la probabilità di ricevere una mano che contiene 5 numeri distinti.
 
#+BEGIN_SRC R :results output :exports both :session calcoloProb
combTot <- choose(40, 5)
comb5Numeri <- 4^5

#1
prob5Numeri <- comb5Numeri/combTot
prob5Numeri

#2
prob5NumeriDistinti <- prob5Numeri*choose(10, 5)
prob5NumeriDistinti
#+END_SRC

#+RESULTS:
: [1] 0.001556212
: [1] 0.3921654

#+BEGIN_SRC R
# Esercizio 29

# Consideriamo un mazzo di 40 carte suddivise in 4 classi (detti "semi") 
# ciascuna contenente 10 carte numerate da 1 a 10. Ogni mano servita è 
# formata da n (ex: n=5) carte estratte dal mazzo.

# a) Calcolare la probabilità di avere una mano formata dai numeri da 
#    1 a n (ex: con n=5 -> mano="1","2", "3", "4", "5")


# b) Calcolare la probabilità di ricevere una mano che contiene n carte con numero
#    diverso


#---------------------------------------------------------

#importo la libreria prob
library(prob)


# PARAMETRI 

# n = numero di carte con cui è formata una mano


n <- 5

# -----------------------------


#SOLUZIONE

# a) Calcolare la probabilità di avere una mano formata dai numeri da 
#    1 a n (ex: con n=5 -> mano="1","2", "3", "4", "5")

out1 <- 1;


for (i in n : 1){
  
  # probabilità di estrarre una carta di tipo diverso sapendo che 
  # sono già state estratte 'i' carte tra loro di tipo diverso 
  
  temp <- (4*i/(40-(5-i)))
  
  # calcolo del risultato finale del punto a) in forma ricorsiva
  
  out1 <- out1 * temp

}


# -------------------------------

# b) Calcolare la probabilità di ricevere una mano che contiene n carte con numero
#    diverso

# numero di possibili combinazioni di 10 carte con numeri diversi per una mano di n elementi
temp2 <- choose(10,n)

#soluzione
out2 <- temp2*out1
#+END_SRC

Esercizio 34:
- Scegliamo 3 carte a caso fra un mazzo di 52 carte da gioco.
  1. Qual'è la probabilità che 2 siano assi e una sia un 10?
  2. Qual'è la probabilità che almeno 2 siano figure?
  3. Qual'è la probabilità che almeno 2 siano figure e nessuna sia un asso?
     
#+BEGIN_SRC R :results output :exports both :session calcoloProb
combTot <- choose(52, 3)

#1
comb2Assi1Dieci <- choose(4, 2)*4
prob2Assi1Dieci <- comb2Assi1Dieci/combTot
prob2Assi1Dieci

#2
combMin2Figure <- choose(12, 2)*40 + choose(12, 3)
probMin2Figure <- combMin2Figure/combTot
probMin2Figure

#3

comb2Figure1Asso <- choose(12, 2)*4
probMin2Figure1Asso <-probMin2Figure - comb2Figure1Asso/combTot
probMin2Figure1Asso
#+END_SRC

#+RESULTS:
: [1] 0.001085973
: [1] 0.1294118
: [1] 0.1174661

#+BEGIN_SRC R
# Esercizio 34

# Scegliamo tre carte a caso fra un mazzo di 52 carte da gioco 
# (divise in quattro semi con ciascuno tre figure: fante, donna, re).

# 1) Qual'è la probabilità che nA siano assi e n10 siano dieci?

# 2) Qual'è la probabilità che almeno nF1 siano figure?

# 3) Qual'è la probabilità che almeno nF2 siano figure e nessuno sia asso



#---------------------------------------------------------


#importo la libreria prob
library(prob)



# PARAMETRI 


# nA = numero di assi che si desidera estrarre nel primo punto
# n10 = numero di 10 che si desidera estrarre nel primo punto

# totEstr = numero totale di carte che si vuole estrarre

# nF1 = numero minimo di figure che si vogliono estrarre nel punto 2

# nF2 = numero minimo di figure che si vogliono estrarre nel punto 3

nA <- 2
n10 <- 1
totEstr <- 3


nF1 <- 2

nF2 <- 2

#------------------------------------------

#SOLUZIONE

# 1) Qual'è la probabilità che nA siano assi e n10 siano dieci?

# combinazioni possibili di estrazione di nA assi
temp1 <- choose(4,nA)

# combinazioni possibili di estrazione di n10 dieci
temp2 <- choose(4,n10)

#casi possibili

den <- choose(52,totEstr)

# soluzione:

out <- (temp1*temp2)/den


#------------------------------

# 2) Qual'è la probabilità che almeno nF1 siano figure?

# numero di possibili combinazioni di estrazioni di almeno nF1 figure
num2 <- 0

for (i in totEstr : nF1){
  num2 <- num2 + choose(12,i)*choose(40, totEstr-i)
}

#casi possibili
den <- choose(52,totEstr)


# soluzione:

out2 <- num2/den

#------------------------------

# 3) Qual'è la probabilità che almeno nF2 siano figure e nessuno sia asso

# numero di possibili combinazioni di estrazioni di almeno nF2 figure
num3 <- 0

for (i in totEstr : nF2){
  num3 <- num3 + choose(12,i)*choose(36, totEstr-i)
}

#casi possibili
den <- choose(52,totEstr)


# soluzione:

out3 <- num3/den
#+END_SRC

Probabilità condizionata:
#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Prob(spazio di probabilità, evento, osservazione)
# Calcolare la probabilità che lanciando 2 dadi esca la stess faccia
# dato che la loro somma è maggiore o uguale a 8
S=rolldie(2, makespace=TRUE)
Prob(S, X1==X2, given=(X1+X2>=8))
#+END_SRC

#+RESULTS:
: [1] 0.2

Esercizio 1:
- Supponiamo che 10 carte numerate da 1 a 10 vengano introdotte in un cappello e che una carta venga estratta a caso.
  - Sapendo che la carta estratta è maggiore di 4, qual'è la probabilità che sia 10?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
carte <- urnsamples(1:10, size=1)
prob <- rep(1/10, times=10)
S <- probspace(carte, probs=prob)
Prob(S, out==10, given=out>4)
#+END_SRC

#+RESULTS:
: [1] 0.1666667

Esercizio 2:
- In una famiglia vi sono 2 figli. Qual'è la probabilità che entrambi siano maschi dato che almeno uno di loro è maschio?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
figli <- urnsamples(c("M", "F"), size=2, replace=T, ordered=T)
urn_figli <- probspace(figli)
Prob(urn_figli, X1=="M" & X2=="M", given=(X1=="M" | X2=="M"))
#+END_SRC

#+RESULTS:
: [1] 0.3333333

Formula del prodotto:
- Esempio 2.10:
  - Consideriamo un'urna contenente 2 palline Blu, 1 Rossa e 1 Verde e supponiamo di effettuare 3 estrazioni, lasciando fuori
    dall'urna le palline già estratte. Si vuole calcolare la probabilità che:
    1. La prima volta venga estratta una pallina blu;
    2. La seconda volta venga estratta la pallina rossa;
    3. La terza volta venga estratta la pallina verde.
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urn <- urnsamples(c("R", "B", "B", "V"), 3, ordered=T)
urn_sample <- probspace(urn)
Prob(urn_sample, X1=="B" & X2=="R" & X3=="V")
#+END_SRC

#+RESULTS:
: [1] 0.08333333

Formula delle probabilità totali:
- Esempio 2.10.1:
  - Una compagnia di assicurazioni ritiene che i guidatori appartengano a due categorie. Facili agli incidenti e non facili.
    Le statistiche dicono che un guidatore f.a.i. ha probabilità 0.4 di fare un incidente nell'anno mentre uno non
    f.a.i. ha probabilità 0.2. Supponiamo che il 30% dei guidatori sia f.a.i., qual'è la probabilità che un nuovo
    guidatore abbia un incidente nell'anno?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
prior <- c(0.3, 0.7)
inc <- c(0.4, 0.2)
sum(prior*inc)
#+END_SRC

#+RESULTS:
: [1] 0.26

Formula di Bates:
- Esempio 2.11:
  - Supponiamo di aver effettuato un'indagine sugli individui in età lavorativa abitanti in un quartiere di una data
    città italiana, e di aver riscontrato che il 40% di tali individui ha la licenza elementare o media, il 50% ha un
    titolo di scuola superiore mentre il restante 10% è laureato. Una seconda indagine ha permesso di rilevare i tassi
    di disoccupazione tra i tre gruppi di individui che risultano essere rispettivamente: 15%, 5% e 10%.
    - Supponiamo che l'individuo estratto è disoccupato, qual'è la probabilità che esso sia laureato?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
Occ <- c(0.4, 0.5, 0.1)
Dis <- c(0.15, 0.05, 0.1)
num <- Occ*Dis
Ptot <- sum(num)
num/Ptot
#+END_SRC

#+RESULTS:
: [1] 0.6315789 0.2631579 0.1052632

Esercizio 1:
- Supponiamo che 10 carte numerate da 1 a 10 vengano introdotte in un cappello e che una carta venga estratta a caso.
  Vogliamo determinare:
  1. Qual'è la probabilità che la carta estratta sia 10?
  2. Sapendo che la carta estratta è maggiore di 4, qual'è la probabilità che sia un 10?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
carte <- urnsamples(1:10, 1)
carte_space <- probspace(carte)

#1
Prob(carte_space, out==10)

#2
Prob(carte_space, out==10, given=out>4)
#+END_SRC

#+RESULTS:
: [1] 0.1
: [1] 0.1666667

Esercizio 2:
- In una famiglia vi sono 2 figli. Qual'è la probabilità che entrambi siano maschi dato che almeno uno di loro è maschio?
#+BEGIN_SRC R :results output :exports both :session calcoloProb

# Esercizio 2: In una famiglia vi sono 2 figli. 
# Qual è la probabilità che entrambi siano 'k' (ad esempio 'maschi') 
# dato che almeno uno di loro è 'k'? 


#---------------------------------------------------------


#importo la libreria prob
library(prob)



# PARAMETRI INPUT
n <- 2

# numero di figli
k <- 'm' # può essere 'm' o 'f'

#DATI


figli <- c('m', 'f')




#------------------------------------------

#PROCEDIMENTO:

#CREAZIONE DELLO SPAZIO DELLE PROBABILITà

# 1) creazione dello spazio di probabilità relativo alle possibili 
# combinazioni di figli all'interno della famiglia


space=iidspace(figli, n, probs = NULL)




#SOLUZIONE: Probabilità che entrambi i figli siano 'k' dato che almeno uno è 'k'

out <- Prob(space, X1==X2, (X1==k| X2==k))


# OSSERVAZIONE: se volessimo adattare il problema ad numero 'n' di figli
# dovremmo modificare:
# 1) il parametro n della parte iniziale 
# 2) l'istruzione per calcolare la soluzione. Esattamente dovremmo 
#   estendere le due condizioni a "n" Xi.

# Facciamo un esempio: se al posto di 2 volessi eseguire il problema con 3 figli
# la soluzione dovrebbe essere calcolata facendo:

# out <- Prob(space, (X1==X2 & X2==X3), (X1==k| X2==k | X3==k))
#+END_SRC
Esercizio 3:
- Si supponga che un'urna contenga 7 palline bianche e 5 nere. Supponiamo di estrarre 2 palline senza reimmissione.
  Assumendo che ogni pallina possa essere estratta con egual probabilità,
  - Qual'è la probabilità che entrambe le palline estratte siano bianche?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
palline <- NULL
for (i in 1:7) {
  palline <- append(palline, "B")
}
for (i in 1:5) {
  palline <- append(palline, "N")
}
urna <- urnsamples(palline, 2, replace=FALSE, ordered=FALSE)
urna_space <- probspace(urna)

Prob(urna_space, X1=="B" & X2=="B")
#+END_SRC

#+RESULTS:
: [1] 0.3181818

#+BEGIN_SRC R
# Esercizio 3: Si supponga che un'urna contenga 7  palline  bianche  e  5  nere.  
# Supponiamo  di  estrarre  due  palline  senza reimmissione. 

# Assumendo che ogni pallina possa essere estratta con egual probabilità, 


#	Qual è la probabilità che entrambe le palline estratte siano del tipo "k" (ex: siano bianche)?


#---------------------------------------------------------


#importo la libreria prob
library(prob)



# PARAMETRI 
#b = num palline bianche nell'urna
#n = num palline nere nell'urna
#k = colore di cui vogliamo calcolare la probabilità di estrarre palline

b <- 7
n <- 5

k <- "b"

#DATI

# inizializzazione dell'array relativo all'urna
y <- NULL


# Cicli che inseriscono elementi nell'urna

for (i in 1 : b){
  y <- append(y, "b");
}

for (i in 1 : n){
  y <- append(y, "n");
}


# calcolo numero elementi nell'urna
numel = nsamp(b+n, 2, replace=FALSE)


#------------------------------------------

#PROCEDIMENTO:

#CREAZIONE DELLO SPAZIO DELLE PROBABILITà

# 1) Creazione dello "Spazio degli eventi relativo all'urna"

urn = urnsamples(y, 2, replace = FALSE, ordered=FALSE)


# 2) Generiamo un array indicante, in ogni posizione "i", la probabilità di 
# verificarsi dell'i-esimo evento dello "Spazio degli eventi" sopra definito.
# numel = numero degli elementi nell'urna

p <- rep(1/numel, times=numel)



# creazione dello spazio di probabilità

space_urn = probspace(urn, p)



#SOLUZIONE: Probabilità che entrambe le palline estratte siano del tipo "k" 

out = Prob(space_urn, X1==X2 & X1==k)
#+END_SRC
Esercizio 6:
- Si considerino 2 urne. La prima contiene 2 palline bianche e 7 nere, mentre la seconda contiene 5 palline bianche
  e 6 nere. Lanciamo una moneta e se otteniamo testa estraiamo una palline dalla prima urna, altrimenti
  estraiamo una pallina della seconda urna.
  - Qual'è la probabilità che il lancio della moneta sia stato testa dato che la pallina estratta è bianca?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urna1 <- NULL
for (i in 1:2) {
  urna1 <- append(urna1, "b")
}
for (i in 1:7) {
  urna1 <- append(urna1, "n")
}
urna2 <- NULL
for (i in 1:5) {
  urna2 <- append(urna2, "b")
}
for (i in 1:6) {
  urna2 <- append(urna2, "n")
}

urna_space1 <- probspace(urnsamples(urna1, 1))
urna_space2 <- probspace(urnsamples(urna2, 1))
pBH <- Prob(urna_space1, out=="b")
pBT <- Prob(urna_space2, out=="b")

moneta <- c(0.5, 0.5)
pBHT <- c(pBH, pBT)
pB <- moneta*pBHT
pTot <- sum(moneta*pBHT)

pHeadGivenBianca <- pB/pTot
pHeadGivenBianca[1]
#+END_SRC

#+RESULTS:
: [1] 0.3283582

#+BEGIN_SRC R
# Esercizio 6: 
# Si considerino due urne. La prima contiene b1 (ex: b1=2) palline bianche e n1 (ex:n1=7) nere, 
# mentre la seconda contiene b2 (ex: b2=5) palline bianche e (ex: n2=6) nere. 

# Lanciamo una moneta e se otteniamo testa estraiamo una pallina dalla prima urna, 
# altrimenti estraiamo una pallina della seconda urna. 

# Qual è la probabilità che il lancio della moneta sia stato l (ex: l=testa) dato che 
# la pallina estratta è k (ex: k=bianca)?

#---------------------------------------------------------


#importo la libreria prob
library(prob)



# PARAMETRI 
#b1 = numero palline bianche nella prima urna
#n1 = numero palline nere nella prima urna

#b2 = numero palline bianche nella seconda urna
#n2 = numero palline nere nella seconda urna



b1 <- 2
n1 <- 7

b2 <- 5
n2 <- 6


#"probabilità che il lancio sia stato l ..."

l <- 'T' #l può assumere valore T (testa) o H (croce)

#"...sapendo che la palllina estratta è k"

k <- 'b' #può assumere valore 'b' (bianco) o 'n' (nero)


#calcolo del risultato inverso della moneta rispetto a quello scelto
#nell'esercizio. (verrà usato per lo svolgimento)

if (l =='T')
  notl <- 'H'
else
  notl <- 'T'



#DATI

# definizione della prima urna


# inizializzazione dell'array relativo alla prima urna
urn1 <- NULL


# Cicli che inseriscono elementi nell'urna

for (i in 1 : b1){
  urn1 <- append(urn1, "b");
}

for (i in 1 : n1){
  urn1 <- append(urn1, "n");
}




# definizione della seconda urna


# inizializzazione dell'array relativo alla seconda urna
urn2 <- NULL


# Cicli che inseriscono elementi nell'urna

for (i in 1 : b2){
  urn2 <- append(urn2, "b");
}

for (i in 1 : n2){
  urn2 <- append(urn2, "n");
}

#numel1 contiene il numero di elementi della prima urna
numel1 = length(urn1)


#numel2 contiene il numero di elementi della seconda urna
numel2 = length(urn2)



#------------------------------------------

#PROCEDIMENTO:

#CREAZIONE DELLO SPAZIO DELLE PROBABILITà

# 1) Generiamo un array indicante, in ogni posizione "i", la probabilità di 
# verificarsi dell'i-esimo evento dello "Spazio degli eventi" (urn1) sopra definito.
# numel1 = numero degli elementi nella prima urna

p1 <- rep(1/numel1, times=numel1)


# 2) creazione dello spazio di probabilità relativo alla prima urna

space_urn1 = probspace(urn1, p1)


# 3) Generiamo un array indicante, in ogni posizione "i", la probabilità di 
# verificarsi dell'i-esimo evento dello "Spazio degli eventi" (urn2) sopra definito.
# numel2 = numero degli elementi nella prima urna

p2 <- rep(1/numel2, times=numel2)


# 4) creazione dello spazio di probabilità relativo alla prima urna

space_urn2 = probspace(urn2, p2)


# 5) creazione dello spazio di probabilità relativo al lancio della moneta

coin = tosscoin(1, makespace=TRUE)





#SOLUZIONE: 


# (faremo uso della formula di Bayes)



# probabilità al numeratore 


#probabilità che la pallina sia k (ex: bianca) se esce l (ex: testa)

PBT <- Prob(space_urn1, x == k)

#probabilità che esca t

PT <- Prob(coin,toss1 == l)

#probabilità numeratore

num <- PBT * PT





# probabilità al denominatore


#probabilità che la pallina sia k (ex: bianca) se esce notl (ex: se l = T -> notl = H)

PBC <- Prob(space_urn2, x == k)

#probabilità che esca croce

PC <- Prob(coin,toss1==notl)


#probabilità denominatore

den <- PBT * PT + PBC * PC






# applicazione della formula di Bayes

out = num / den
#+END_SRC
Esercizio 7:
- Uno studente risponde ad un quiz multivalore conoscendo la risposta o selezionando a caso. Si indichi con "p" la probabilità
  che lo studente conosca la risposta (ha studiato). Supponiamo che uno studente che non conosca la risposta indovini con
  probabilità $1/m$, dove "m" è il numero di risposte possibili.
  - Qual'è la probabilità che lo studente conoscesse la risposta dato che ha risposto correttamente?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
# Numero di risposte possibili
m <- 4
# Probabilità che conosca la risposta
p <- 0.8

conoscenza <- c(p, 1-p)
corretto <- c(1, 1/m)

comb <- conoscenza*corretto
pTot <- sum(comb)

comb[1]/pTot
#+END_SRC

#+RESULTS:
: [1] 0.9411765

Esercizio 8:
- Un test di laboratorio è efficace al 95% nel rilevare una certa malattia quando questa è presente. Il test fornisce
  comunque "falsi positivi" (dice che una persona sana è malata) per l'1% delle persone sane esaminate. Nel caso in cui
  lo 0.5% della popolazione sia affetto dalla suddetta malattia,
  - Qual'è il problema che una persona sia effettivamente malata dato che il test dà risultato positivo?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
posti <- c(0.95, 0.01)
salute <- c(0.005, 0.995)

casi <- posti*salute
num <- casi[1]
denom <- sum(casi)

num/denom
#+END_SRC

#+RESULTS:
: [1] 0.3231293

Esercizio 9:
- Si sa che una data lettera ha eguale probabilità di essere contenuta in uno di tre folders. Si indichi con $\alpha_i$ la probabilità che la lettera
  venga trovata tramite una ricerca sommaria nel folder $i$. Pertanto con probabilità $(1 - \alpha_i)$ la lettera non viene trovata anche se si trova
  nel folder $i$. Supponiamo di cercare nel folder $1$ senza trovare la lettera.
  - Qual'è la probabilità che la lettera sia nel folder ~1~?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
n <- 3
probFolders <- rep(1/n, 3)
probRicercaSucc <- c(0.8, 0.9, 0.85)
probRicercaFall <- 1-probRicercaSucc
probRicercaPrimo <- c(probRicercaFall[1], 1, 1)
prob <- probFolders*probRicercaPrimo

num <- prob[1]
denom <- sum(prob)

num/denom
#+END_SRC

#+RESULTS:
: [1] 0.09090909

Esercizio 12:
- Date 2 urne indistinguibili esternamente. La prima contiene 3 palline rosse, 1 bianca e 2 verdi, mentre la seconda contiene 1 pallina rossa,
  3 bianche e 2 verdi. Presa un'urna a caso, si estraggono 2 palline senza reimbussolamento da tale urna. Calcolare la probabilità che:
  1. Vengano estratte 2 palline rosse;
  2. Venga estratta almeno una pallina rossa;
  3. L'urna dalla quale stiamo estraendo sia la prima quando la prima pallina estratta è rossa;
  4. La seconda pallina estratta dall'urna sia rossa quando la prima è rossa.
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urna1 <- c("r", "r", "r", "b", "v", "v")
urna2 <- c("r", "b", "b", "b", "v", "v")
numero_estrazioni <- 2
prob <- NULL
for(i in 1:numero_estrazioni) {
  prob <- append(prob, 1/numero_estrazioni)
}

urna1_space <- probspace(urnsamples(urna1, numero_estrazioni, ordered=T))
urna2_space <- probspace(urnsamples(urna2, numero_estrazioni, ordered=T))

#1
prob1 <- Prob(urna1_space, X1=="r" & X2=="r")
prob2 <- Prob(urna2_space, X1=="r" & X2=="r")
prob12 <- c(prob1, prob2)
prob_2_palline_rosse <- sum(prob12*prob)
prob_2_palline_rosse

#2
prob1_0r <- Prob(urna1_space, X1!="r" & X2!="r")
prob2_0r <- Prob(urna2_space, X1!="r" & X2!="r")
prob12_0r <- c(prob1_0r, prob2_0r)
1-sum(prob12_0r*prob)

#3
prob1_prima_r <- Prob(urna1_space, X1=="r")
prob2_prima_r <- Prob(urna2_space, X1=="r")
prob12_prima_r <- c(prob1_prima_r, prob2_prima_r)
prob_tot_prima_r <- prob12_prima_r*prob
num <- prob_tot_prima_r[1]
denom <- sum(prob_tot_prima_r)
num/denom

#4
num <- prob_2_palline_rosse
denom <- sum(prob_tot_prima_r)
num/denom
#+END_SRC

#+RESULTS:
: [1] 0.1
: [1] 0.5666667
: [1] 0.75
: [1] 0.3

Esercizio 13:
- 2 urne indistinguibili esternamente. La prima contiene 2 palline rosse, 2 bianche e 4 verdi, mentre la seconda contiene 2 palline rosse,
  4 bianche e 2 verdi. Selezioniamo un'urna a caso ed estraiamo da essa 2 palline senza reimbussolamento. Calcolare pa probabilità che:
  1. Vengano estratte 2 palline verdi;
  2. L'urna dalla quale stiamo estraendo sia la prima sapendo che la prima pallina estratta è verde;
  3. Vengano estratte 2 palline di colore diverso.
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urna1 <- c("r", "r", "b", "b", "v", "v", "v", "v")
urna2 <- c("r", "r", "b", "b", "b", "b", "v", "v")
prob <- c(0.5, 0.5)
urna1_space <- probspace(urnsamples(urna1, 2, ordered=T))
urna2_space <- probspace(urnsamples(urna2, 2, ordered=T))

#1
urna1_2_verdi <- Prob(urna1_space, X1==X2 & X2=="v")
urna2_2_verdi <- Prob(urna2_space, X1==X2 & X2=="v")
urne_2_verdi <- c(urna1_2_verdi, urna2_2_verdi)
urne_2_verdi_tot <- sum(urne_2_verdi*prob)
urne_2_verdi_tot

#2
urna1_prima_verde <- Prob(urna1_space, X1=="v")
urna2_prima_verde <- Prob(urna2_space, X1=="v")
urne_prima_verde <- c(urna1_prima_verde, urna2_prima_verde)
prob_urne_prima_verde <- urne_prima_verde*prob
num <- prob_urne_prima_verde[1]
denom <- sum(prob_urne_prima_verde)
num/denom

#3
urna1_colore_diverso <- Prob(urna1_space, X1!=X2)
urna2_colore_diverso <- Prob(urna2_space, X1!=X2)
urne_colore_diverso <- c(urna1_colore_diverso, urna2_colore_diverso)
sum(urne_colore_diverso*prob)
#+END_SRC

#+RESULTS:
: [1] 0.125
: [1] 0.6666667
: [1] 0.7142857

Esercizio 14:
- Si effettuano 3 tiri verso un medesimo bersaglio. La probabilità di colpirlo al primo, al secondo e al terzo colpo sono rispettivamente
  uguali a $p1 = 0.4, p2=0.5, p3=0.7$. Calcolare la probabilità:
  1. Di colpire il bersaglio una sola volta in 3 tiri;
  2. Di colpire il bersaglio al più una volta in 3 tiri;
  3. Di colpire il bersaglio almeno una volta in 3 tiri;
  4. Che il centro sia avvenuto al terzo colpo sapendo che il bersaglio viene colpito una sola volta.
#+BEGIN_SRC R :results output :exports both :session calcoloProb
prob_bersaglio <- c(0.4, 0.5, 0.7)
prob_mancato <- 1 - prob_bersaglio

#1
prob_matrix <- array(rep(prob_mancato, 3), c(3, 3))
diag(prob_matrix) <- prob_bersaglio
prob_1_bersaglio <- 0
for (i in 1:3) {
  prod <- 1
  for (j in 1:3) {
  prod <- prod*prod(prob_matrix[j, i])
  }
  prob_1_bersaglio <- prob_1_bersaglio + prod
}
prob_1_bersaglio

#2
prob_0_volte <- prob_mancato[1]*prob_mancato[2]*prob_mancato[3]
prob_al_massimo_1_volta <- prob_0_volte + prob_1_bersaglio
prob_al_massimo_1_volta

#3
prob_almeno_1_volta <- 1 - prob_0_volte
prob_almeno_1_volta

#4
prob_matrix_neg <- 1 - prob_matrix
num <- 1
for (i in 1:3) {
  num <- num*prob_matrix_neg[i,1]
}
num/prob_1_bersaglio
#+END_SRC

#+RESULTS:
: [1] 0.36
: [1] 0.45
: [1] 0.91
: [1] 0.5833333

Esercizio 15:
- Un dado equilibrato viene lanciato 3 volte. Calcolare la probabilità che:
  1. Il punteggio del primo lancio non sia divisibile per 3;
  2. Il punteggio somma dei primi 2 lanci non sia divisibile per 3;
  3. Né il punteggio del primo lancio, né il punteggio somma dei primi due lanci siano visibili per 3.
#+BEGIN_SRC R :results output :exports both :session calcoloProb
die <- probspace(rolldie(3))

#1
primo <- Prob(die, X1%%3!=0)
primo

#2
secondo <- Prob(die, (X1+X2)%%3!= 0)
secondo

#3
primo*secondo
#+END_SRC

#+RESULTS:
: [1] 0.6666667
: [1] 0.6666667
: [1] 0.4444444

Esercizio 16:
- Abbiamo dei componenti elettronici che provengono da 3 diversi centri di produzione, $C1$, $C2$ e $C3$, la cui probabilità
  di funzionamento è, rispettivamente $\frac{1}{10}, \frac{2}{10}$ e $\frac{3}{10}$. Sappiamo inoltre che il 30% dei
  componenti provengono da $C1$, altri 20% da $C2$ ed i restanti 50% provengono da $C3$. Calcolare:
  1. La probabilità che preso un componente a caso questo sia funzionante;
  2. La probabilità che un componente preso a caso provenga da $C1$ avendo osservato che esso è funzionante;
  3. La probabilità che almeno uno tra 3 componenti a caso sia funzionante;
  4. Il numero minimo di componenti da estrarre affinché la probabilità che almeno uno sia funzionante sia maggiore o
     uguale a 0.999.
#+BEGIN_SRC R :results output :exports both :session calcoloProb
funz_c1 <- 1/10
funz_c2 <- 2/10
funz_c3 <- 3/10
prob_funz <- c(funz_c1, funz_c2, funz_c3)

da_c1 <- 0.3
da_c2 <- 0.2
da_c3 <- 0.5
prob_orig <- c(da_c1, da_c2, da_c3)

#1
prob <- prob_funz*prob_orig
prob_componente_funz <- sum(prob)
prob_componente_funz

#2
num <- prob[1]
num/prob_componente_funz

#3
prob_0_funz <- (1-prob_componente_funz)^3
prob_almeno_1_funz <- 1 - prob_0_funz
prob_almeno_1_funz
 
#4
p <- 0.999
ceiling(log(1-p, base=(1-prob_componente_funz)))
#+END_SRC

#+RESULTS:
: [1] 0.22
: [1] 0.1363636
: [1] 0.525448
: [1] 28

Esercizio 18:
- Abbiamo un sistema elettronico costruito da 4 componenti posti in serie e parallelo come in figura:
  
  #+DOWNLOADED: /tmp/screenshot.png @ 2018-04-01 23:16:56
  [[file:Laboratorio/screenshot_2018-04-01_23-16-56.png]]

  Supponiamo che il funzionamento di ogni singolo componente sia indipendente dal funzionamento degli altri e
  supponiamo che i 2 in parallelo funzionino ognuno con probabilità $p_P$ mentre quelli in serie ognuno con
  probabilità $P_S$ ($P_P$ e $P_S \in (0, 1) \subseteq \mathbb{R}$)
#+BEGIN_SRC R :results output :exports both :session calcoloProb
ps <- 0.8
pp <- 0.9

#1
nessuno_funz <- (1-ps)^2 * (1-pp)^2
1- nessuno_funz

#2
nessun_paral_funz <- (1-pp)^2
almeno_uno_paral_funz <- 1 - nessun_paral_funz
due_serie_funz <- ps^2
sistema_funz <- almeno_uno_paral_funz*due_serie_funz
sistema_funz

#3
pp <- 0.8
ps <- NULL
p <- 0.9
nessun_paral_funz <- (1-pp)^2
sqrt(p/(1-nessun_paral_funz))

#4
ps <- 0.8
pp <- 0.9
sistema_funz <- almeno_uno_paral_funz*due_serie_funz
(ps^2*(1-pp)^2)/(1-sistema_funz)
#+END_SRC

#+RESULTS:
: [1] 0.9996
: [1] 0.6336
: [1] 0.9682458
: [1] 0.01746725

Esercizio 19:
- Da un mazzo regolare di 52 carte (13 cuori, 13 quadri, 13 fiori e 13 picche) vengono estratte contemporaneamente 3 carte.
  - Qual'è la probabilità che le 3 carte estratte siano tutte di picche?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
carte <- probspace(urnsamples(cards(), 3))
Prob(carte, all(suit=="Spade"))
#+END_SRC

#+RESULTS:
: [1] 0.01294118

Esercizio 20:
- Supponiamo che un'urna contenga 1 pallina rossa e 1 pallina bianca. Una pallina è estratta e se ne osserva il colore.
  Essa viene poi rimessa nell'urna insieme a 1 pallina dello stesso colore. Sia $R_i$ l'evento "All'\(i\)-esima estrazione
  viene estratta una pallina rossa", analogamente $B_i$ sia l'evento "All'\(i\)-esima estrazione viene estratta una pallina
  bianca". Si calcoli:
  1. $P(R_2)$ e $P(R_3)$;
  2. Sapendo che la seconda estratta è una pallina rossa, è più probabile che la prima estratta fosse rossa o bianca?
  3. Qual'è la probabilità che la prima estratta sia rossa e la seconda bianca?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
r <- 1
b <- 1
tot <- r+b
p <- 0.5

#1
prob_r1 <- r/tot
prob_b1 <- b/tot
tot <- tot+1
prob_r1_r2 <- (r+1)/tot
prob_b1_r2 <- r/tot
prob_r2 <- prob_r1_r2*p + prob_b1_r2*p
prob_r2
prob_r1_b2 <- b/tot
prob_b1_b2 <- (b+1)/tot
prob_b2 <- prob_r1_b2*p + prob_b1_b2*p
tot <- tot+1
p <- p/2
prob_r1_r2_r3 <- (r+2)/tot
prob_r1_b2_r3 <- (r+1)/tot
prob_b1_r2_r3 <- prob_r1_b2_r3
prob_b1_b2_r3 <- r/tot
prob_r3 <- prob_r1_r2_r3*p + prob_r1_b2_r3*p + prob_b1_r2_r3*p + prob_b1_b2_r3*p
prob_r3

#2
num_r <- prob_r1*prob_r1_r2
denom_r <- prob_r2
prob_r1_dato_r2 <- num_r/denom_r
num_b <- prob_b1*prob_b1_r2
denom_b <- prob_r2
prob_b1_dato_r2 <- num_b/denom_b
if (prob_r1_dato_r2 > prob_b1_dato_r2 | prob_r1_dato_r2 == prob_b1_dato_r2) {
  print("Più probabile la pallina rossa")
} else if (prob_b1_dato_r2 == prob_r1_dato_r2) {
  print("Stessa probabilità")
} else {
  print("Più probabile la pallina bianca")
}

#3
prob_r1_b2*0.5
#+END_SRC

#+RESULTS:
: [1] 0.5
: [1] 0.5
: [1] "Più probabile la pallina rossa"
: [1] 0.1666667

Esercizio 21:
- Un'urna contiene 6 palline di cui 3 bianche, 2 rosse ed 1 nera. Si estraggono senza reimmissione tre palline
  e si vince se una delle 3 è nera.
  1. Si calcoli la probabilità di vincere;
  2. Si calcoli la probabilità di vincere sapendo che la pallina nera non è uscita nelle prime 2 estrazioni;
  3. Sapendo di aver vinto, qual'è la probabilità che la pallina nera non sia uscita nelle prime 2 estrazioni?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urna <- c("b", "b", "b", "r", "r", "n")
urna_space <- probspace(urnsamples(urna, 3, ordered=T))

#1
prob_vincita <- Prob(urna_space, X1=="n" | X2=="n" | X3=="n")
prob_vincita

#2
prob_nera_al_terzo <- Prob(urna_space, X1!="n" & X2!="n" & X3=="n")
prob_prime_2_non_nera <- Prob(urna_space, X1!="n" & X2!="n")
prob_vincita_prime_2_non_nera <- prob_nera_al_terzo/prob_prime_2_non_nera
prob_vincita_prime_2_non_nera

#3
(prob_vincita_prime_2_non_nera*prob_prime_2_non_nera)/prob_vincita
#+END_SRC

#+RESULTS:
: [1] 0.5
: [1] 0.25
: [1] 0.3333333

Esercizio 22:
- Il modello "plus" di una chiave USB può presentare 2 tipi di difetto, difetto di tipo $A$ con probabilità pari a 0.03 e difetto di
  tipo $B$ con probabilità pari a 0.07. I due tipi di difetto sono indipendenti l'uno dall'altro. Qual'è la probabilità che
  una generica chiave USB
  1. Presenti entrambi i difetti?
  2. Sia difettosa?
  3. Presenti il difetto $A$, sapendo che è difettosa?
  4. Presenti uno solo dei difetti, sapendo che è difettosa?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
pa <- 0.03
pb <- 0.07

#1
entrambi_difetti <- pa*pb
entrambi_difetti

#2
difettosa <- pa + pb - pa*pb
difettosa

#3
pa/difettosa

#4
un_solo_difetto <- difettosa - pa*pb
un_solo_difetto/difettosa
#+END_SRC

#+RESULTS:
: [1] 0.0021
: [1] 0.0979
: [1] 0.3064351
: [1] 0.9785495

Esercizio 23:
- Una roulette semplificata è formata da 12 numeri che sono classificati rosso ($R$) e nero ($N$) in base allo schema seguente:
  
  #+DOWNLOADED: /tmp/screenshot.png @ 2018-04-02 11:03:33
  [[file:Laboratorio/screenshot_2018-04-02_11-03-33.png]]

  Siano $A =$ "esce un numero pari", $B =$ "esce un numero rosso", $C =$ "esce un numero" $\leq 6$ e $D =$ "esce un numero" $\leq 8$.
  1. Stabilire se gli eventi $A, B$ e $C$ sono a 2 a 2 indipendenti;
  2. Stabilire se $A, B$ e $C$ costituiscono una famiglia di eventi indipendenti;
  3. Stabilire se $A, B$ e $D$ costituiscono una famiglia di eventi indipendenti;
  4. Ponendo $E =$ "esce un numero dispari" $\leq 3$, $E$ è indipendente da $A$ e da $D$?
  5. Supponiamo di sapere che esca un numero rosso, gli eventi $A$ e $C$ sono indipendenti?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
numbers <- 1:12
colors <- c("r", "r", "n", "n", "r", "n", "n", "r", "n", "n", "r", "r")
roulette <- data.frame(numbers, colors)
    
roulette_space <- probspace(roulette)
    
prob_a <- Prob(roulette_space, numbers%%2==0)
prob_b <- Prob(roulette_space, colors=="r")
prob_c <- Prob(roulette_space, numbers<=6)
prob_d <- Prob(roulette_space, numbers<=8)

#1
prob_a_AND_b <- Prob(roulette_space, numbers%%2==0 & colors=="r")
prob_a_AND_c <- Prob(roulette_space, numbers%%2==0 & numbers <=6)
prob_b_AND_c <- Prob(roulette_space, colors=="r" & numbers <=6)
if (prob_a*prob_b==prob_a_AND_b & prob_a*prob_c==prob_a_AND_c & prob_b*prob_c==prob_b_AND_c) {
  print("Gli eventi A, B e C sono a 2 a 2 indipendenti")
} else {
  print("Gli eventi A, B e C non sono a 2 a 2 indipendenti")
}

#2
prob_a_AND_b_AND_c <- Prob(roulette_space, numbers%%2==0 & colors=="r" & numbers <=6)
if (prob_a*prob_b*prob_c==prob_a_AND_b_AND_c) {
  print("Gli eventi A, B e C costituiscono una famiglia di eventi indipendenti")
} else {
  print("Gli eventi A, B e C non costituiscono una famiglia di eventi indipendenti")
}

#3
prob_a_AND_d <- Prob(roulette_space, numbers%%2==0 & numbers <=8)
prob_b_AND_d <- Prob(roulette_space, colors=="r" & numbers <=8)
if (prob_a*prob_b==prob_a_AND_b & prob_a*prob_d==prob_a_AND_d & prob_b*prob_d==prob_b_AND_d) {
  print("Gli eventi A, B e D sono a 2 a 2 indipendenti")
} else {
  print("Gli eventi A, B e D non sono a 2 a 2 indipendenti")
}

#4
prob_e <- Prob(roulette_space, numbers%%2==1 & numbers<=3)
prob_a_AND_d <- Prob(roulette_space, numbers%%2==0 & numbers<=8)
prob_a_AND_e <- Prob(roulette_space, numbers%%2==0 & numbers%%2==1 & numbers<=3)
prob_b_AND_e <- Prob(roulette_space, colors=="r" & numbers%%2==1 & numbers<=3)
if (prob_a*prob_d==prob_a_AND_d & prob_a*prob_e==prob_a_AND_e & prob_b*prob_e==prob_b_AND_e) {
  print("Gli eventi A, D e E sono a 2 a 2 indipendenti")
} else {
  print("Gli eventi A, D e E non sono a 2 a 2 indipendenti")
}

#5
prob_a_dato_b <- prob_a_AND_b/prob_b
prob_c_dato_b <- prob_b_AND_c/prob_b
prob_a_AND_c_dato_b <- prob_a_AND_b_AND_c/prob_b
if (prob_a*prob_d==prob_a_AND_d & prob_a*prob_e==prob_a_AND_e & prob_b*prob_e==prob_b_AND_e) {
  cat("Se sappiamo che esce un numero rosso, gli eventi A e C sono indipendenti")
} else {
  cat("Se sappiamo che esce un numero rosso, gli eventi A e C non sono indipendenti")
}
#+END_SRC

#+RESULTS:
: [1] "Gli eventi A, B e C sono a 2 a 2 indipendenti"
: [1] "Gli eventi A, B e C non costituiscono una famiglia di eventi indipendenti"
: [1] "Gli eventi A, B e D sono a 2 a 2 indipendenti"
: [1] "Gli eventi A, D e E non sono a 2 a 2 indipendenti"

Esercizio 24:
- Da un'urna che contiene 3 palline bianche e 2 palline nere vengono trasferite 2 palline, scelte a caso, in un'altra
  urna che contiene 4 palline bianche e 4 palline nere. Infine, si estrae una pallina dalla seconda urna.
  1. Trovare la probabilità di estrarre una pallina bianca dalla seconda urna;
  2. Calcolare la probabilità di aver trasferito almeno una pallina bianca se estraiamo una pallina nera dalla seconda urna.
#+BEGIN_SRC R :results output :exports both :session calcoloProb
b1 <- 3
n1 <- 2
tot1<- b1+n1
b2 <- 4
n2 <- 4
tot2<- b2+n2+2

#1
bn <- b1*n1/choose(tot1, 2)
bb <- choose(b1, 2)/choose(tot1, 2)
nn <- choose(n1, 2)/choose(tot1, 2)
estr_b2 <- bn*(b2+1)/tot2 + bb*(b2+2)/tot2 + nn*b2/tot2
estr_b2

#2
almeno_1_b1 <- bn+bb
estr_n2_dato_almeno_1_b1 <- bn*(n2+1)/tot2 + bb*n2/tot2
estr_n2 <- 1 - estr_b2
estr_almeno_1_b1_dato_n2 <- estr_n2_dato_almeno_1_b1/estr_n2
estr_almeno_1_b1_dato_n2
#+END_SRC

#+RESULTS:
: [1] 0.52
: [1] 0.875

Esercizio 25:
- Ci sono 2 urne, la prima urna $U_1$ contiene 2 dadi a 6 facce (dadi onesti), la seconda urna $U_2$ contiene 2 dadi a 6 facce
  (dadi truccati nel modo seguente; ognuno dei dadi ha 3 facce che indicano il numero 6 e le rimanenti 3 il numero 5).
  Si lancia una moneta onesta e se l'esito del bilancio è testa si prendono i dadi dalla prima urna $U_1$ mentre l'esito è croce si
  prendono i dadi dalla seconda urna $U_2$, poi, in ogni caso si lanciano i dati:
  1. Calcolare la probabilità che la somma dei 2 dadi sia 11;
  2. Sapendo di aver ottenuto un 11 lanciando i due dadi, calcolare la probabilità di aver ottenuto croce lanciando la moneta;
  3. Sapendo di aver ottenuto un 11 lanciando i 2 dadi, calcolare la probabilità di aver ottenuto croce lanciando la moneta;
  4. Gli eventi $D_1 =$ "ottengo 6 sul primo lancio" e $D_2 =$ "ottengo 6 sul secondo dato" sono indipendenti?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
dado1 <- rolldie(2, makespace=T)
load_dado2 <- NULL
for (i in 1:3) {
  load_dado2 <- append(load_dado2, 6)
}
for (i in 1:3) {
  load_dado2 <- append(load_dado2, 5)
}
dado2 <- probspace(urnsamples(load_dado2, 2, ordered=T, replace=T))
moneta <- tosscoin(1, makespace=T)
t <- Prob(moneta, toss1=="H")
c <- Prob(moneta, toss1=="T")

#1
somma_11_dado1 <- Prob(dado1, (X1+X2)==11)
somma_11_dado2 <- Prob(dado2, (X1+X2)==11)
somma_11 <- somma_11_dado1*t + somma_11_dado2*c
somma_11

#2
somma_11_dado2*c/somma_11

#3
p_d1 <- Prob(dado1, X1==6)*t + Prob(dado1, X1==6)*c
p_d2 <- Prob(dado1, X2==6)*t + Prob(dado2, X2==6)*c
p_d1_AND_d2 <- Prob(dado1, X1==6 & X2==6)*t + Prob(dado2, X1==6 & X2==6)*c
if (p_d1*p_d2==p_d1_AND_d2) {
  print("D_1 e D_2 sono indipendenti")
} else {
  print("D_1 e D_2 non sono indipendenti")
}
#+END_SRC

#+RESULTS:
: [1] 0.2777778
: [1] 0.9
: [1] "D_1 e D_2 non sono indipendenti"

Esercizio 26:
- Nel gioco del lotto ad ogni estrazione che avviene settimanalmente vengono pescate contemporaneamente 5 palline da un'urna
  contenente 90 palline numerate da 1 a 90 e per il resto indistinguibili. Giovanni ogni settimana gioca l'ambo $\{89, 90\}$,
  cioè Giovanni vince se fra le 5 palline estratte vi sono quelle numerate con 89 e 90.
  1. Calcolare la probabilità di vincere in una singola estrazione;
  2. Se nelle prime 2 estrazioni Giovanni non ha vinto con quale probabilità vincerà almeno una volta nelle prime 10 estrazioni?
  3. Se nelle prime 10 giocate Giovanni ha vinto 3 volte, con quale probabilità ha vinto nelle prime 3 giocate?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
p1 <- 89
p2 <- 90

#1
vinc_1_estr <- choose(87, 3)/choose(90, 5)
vinc_1_estr

#2
1 - (1 - vinc_1_estr)^8

#3
1/choose(10, 3)
#+END_SRC

#+RESULTS:
: [1] 0.002411758
: [1] 0.01913198
: [1] 0.008333333

Esercizio 27:
- Un'urna contiene 2 palline rosse e 4 palline nere. Due giocatori $A$ e $B$ giocano nel modo seguente: le palline vengono
  estratte ad una ad una e messe da parte. $A$ vince se l'ultima palline estratta è rossa, altrimenti vince $B$.
  1. Qual'è la probabilità che $A$ vinca?
  2. Qual'è la probabilità che $A$ vinca sapendo che la prima pallina estratta è rossa?
  3. Qual'è la probabilità che $A$ vinca e che la prima pallina estratta sia rossa?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urna <- c("r", "r", "n", "n", "n", "n")

urna_space <- probspace(urnsamples(urna, 6, ordered=T))

#1
rossa_1 <- Prob(urna_space, X6=="r")
rossa_1

#2
rossa_1_6 <- Prob(urna_space, X1==X6 & X6=="r")
rossa_1_6/rossa_1

#3
rossa_1_6
#+END_SRC

#+RESULTS:
: [1] 0.3333333
: [1] 0.2
: [1] 0.06666667

Esercizio 28:
- Gli abitanti della località ~abc~ raggiungono ogni giorno la città ~xyz~ in treno o in macchina. Il 90% degli abitanti di
  ~abc~ arrivano nella città ~xyz~ in ritardo (sull'orario previsto). Il 40% di questi (cioè di quelli in ritardo) usano il
  treno, mentre, il 30% di quelli che NON arrivano in ritardo (sull'orario previsto) usano la macchina.
  1. Qual'è la probabilità che un abitante di ~abc~ usi il treno per raggiungere la città ~xyz~ (un dato giorno)?
  2. Se un abitante di ~abc~ usa il treno per raggiungere ~xyz~, è più probabile che arrivi in ritardo o non in ritardo?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
ritardo <- 0.9
rit_treno <- 0.4
or_macchina <- 0.2
orario <- 1-ritardo
rit_macchina <- 1-rit_treno
or_treno <- 1-or_macchina

#1
treno <- rit_treno*ritardo + or_treno*orario
treno

#2
ritardo_dato_treno <- rit_treno*ritardo/treno
orario_dato_treno <- or_treno*orario/treno
if (ritardo_dato_treno==orario_dato_treno) {
    print("La probabilità che arrivi in orario e che arrivi in ritardo è la medesima")
} else if (ritardo_dato_treno>orario_dato_treno) {
    print("È più probabile che arrivi in ritardo")
} else if (ritardo_dato_treno>orario_dato_treno) {
    print("È più probabile che arrivi in orario")
}
#+END_SRC

#+RESULTS:
: [1] 0.44
: [1] "È più probabile che arrivi in ritardo"

Esercizio 31:
- Ho 2 urne $A$ e $B$: $A$ contiene 6 palline di cui 1 numerata 1, 2 numerate 2 e 3 numerate 3 e $B$ contiene 9 palline di
  cui 2 numerate 2, 3 numerate 3 e 4 numerate 4. Lancio un dado regolare: se esce la faccio 6, estraggo una pallina dall'urna $A$.
  Invece, se non esce la faccia 6, estraggo una pallina dall'urna $B$.
  1. Qual'è la probabilità di estrarre una pallina numerata 2?
  2. Se la pallina estratta è numerata 2, qual'è la probabilità che il dado lanciato abbia esibito la faccia 6?
  3. Sia $X$ la variabile aleatoria discreta che rappresenta il numero della pallina estratta. Determinare la densità di $X$.
#+BEGIN_SRC R :results output :exports both :session calcoloProb
urna1 <- probspace(urnsamples(c(1, 2, 2, 3, 3, 3), 1))
urna2 <- probspace(urnsamples(c(2, 2, 3, 3, 3, 4, 4, 4, 4), 1))
dado <- rolldie(1, makespace=T)

#1
dado_6 <- Prob(dado, X1==6)
dado_non_6 <- 1-dado_6
urna1_2 <- Prob(urna1, out==2)
urna2_2 <- Prob(urna2, out==2)
estr_2 <- urna1_2*dado_6 + urna2_2*dado_non_6
estr_2

#2
urna1_2*dado_6/estr_2

#3
p <- NULL
for (i in 1:4) {
    p <- append(p, Prob(urna1, out==i)*dado_6 + Prob(urna2, out==i)*dado_non_6)
}
p
#+END_SRC

#+RESULTS:
: [1] 0.2407407
: [1] 0.2307692
: [1] 0.02777778 0.24074074 0.36111111 0.37037037

Esercizio 32:
- Giulio, Carlo e Federico giocano al tiro con l'arco. Si sa che la probabilità che Giulio colpisca il bersaglio è pari a
  3/4, quella che Carlo colpisca il bersaglio è pari ad 1/2 e quella di Federico è pari ad 1/4. Si sa inoltre che i risultati
  dei lanci dei 3 arcieri sono indipendenti.
  1. Calcolare la probabilità che nessuno di loro colpisca il bersaglio;
  2. Calcolare la probabilità che esattamente 2 di loro colpiscano il bersaglio;
  3. Calcolare la probabilità che Giulio colpisca il bersaglio sapendo che esattamente 2 di loro hanno colpito il bersaglio.
#+BEGIN_SRC R :results output :exports both :session calcoloProb
g <- 3/4
c <- 1/2
f <- 1/4

#1
nessuno <- (1-g)*(1-c)*(1-f)
nessuno

#2
due <- g*c*(1-f) + g*(1-c)*f + (1-g)*c*f
due

#3
due_dato_giulio <- due - (1-g)*c*f
due_dato_giulio*g/due
#+END_SRC

#+RESULTS:
: [1] 0.09375
: [1] 0.40625
: [1] 0.6923077

Esercizio 33:
- Un supermercato accetta pagamenti con carte di credito di 2 soli tipi, $A$ e $B$. Il 25% dei clienti possiede la carta $A$,
  il 50% la carta $B$ e il 15% possiede entrambe le carte.
  1. Calcolare la probabilità che un cliente scelto a caso possieda almeno una carta di credito accettata dal supermercato;
  2. Calcolare la probabilità che un cliente scelto a caso possieda esattamente una carta di credito accettata dal supermercato;
  3. Se un cliente possiede almeno una carta di credito accettata dal supermercato, qual'è la probabilità che sia una carta
     di credito di tipo $A$?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
a <- 0.25
b <- 0.50
ab <- 0.15

#1
almeno_una <- a+b-ab
almeno_una

#2
esattamente_una <- almeno_una - ab
esattamente_una

#3
a/almeno_una
#+END_SRC

#+RESULTS:
: [1] 0.6
: [1] 0.45
: [1] 0.4166667

Esercizio 35:
- Uno scommettitore ha nel suo portafoglio una moneta equa ed una moneta con entrambi i lati testa. Egli seleziona in modo
  casuale una delle due monete, la lancia e ne osserva l'esito testa.
  1. Qual'è la probabilità che si tratti della moneta equa?
  2. Supponiamo che lanci la stessa moneta una seconda volta e che l'esito sia ancora testa. Qual'è la probabilità che si
     tratti della moneta equa?
  3. Supponiamo che lanci una terza volta la stessa moneta ottenendo croce. Qual'è la probabilità che si tratti della
     moneta equa?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
p <- 0.5
equa <- c("t", "c")
truc <- c("t", "t")
equa_s <- probspace(urnsamples(equa, 1))
truc_s <- probspace(urnsamples(truc, 1))

#1
equa_testa <- Prob(equa_s, out=="t")
truc_testa <- Prob(truc_s, out=="t")
testa <- p*(equa_testa + truc_testa)
prob_equa <- equa_testa*p/testa
prob_equa

#2
equa_s <- probspace(urnsamples(equa, 2, ordered=T, replace=T))
truc_s <- probspace(urnsamples(truc, 2, ordered=T, replace=T))
equa_testa_2 <- Prob(equa_s, X1=="t" & X2=="t")
truc_testa_2 <- Prob(truc_s, X1=="t" & X2=="t")
testa_2 <- p*(equa_testa_2 + truc_testa_2)
prob_equa_2 <- equa_testa_2*p/testa_2
prob_equa_2 

#3
equa_s <- probspace(urnsamples(equa, 3, ordered=T, replace=T))
truc_s <- probspace(urnsamples(truc, 3, ordered=T, replace=T))
equa_testa_2_croce <- Prob(equa_s, X1=="t" & X2=="t" & X3=="c")
truc_testa_2_croce <- Prob(truc_s, X1=="t" & X2=="t" & X3=="c")
testa_2_croce <- p*(equa_testa_2_croce + truc_testa_2_croce)
prob_equa_3 <- equa_testa_2_croce*p/testa_2_croce
prob_equa_3 
#+END_SRC

#+RESULTS:
: [1] 0.3333333
: [1] 0.2
: [1] 1

Esercizio 36:
- In un certo collegio, il 25% degli studenti è stato bocciato in matematica, il 15% è stato bocciato in chimica, e il 10%
  è stato bocciato sia in matematica che in chimica. Viene scelto a caso uno studente.
  1. Se egli è stato bocciato in chimica, qual'è la probabilità che sia stato bocciato in matematica?
  2. Se egli è stato bocciato in matematica, qual'è la probabilità che sia stato bocciato in chimica?
  3. Qual'è la probabilità che sia stato bocciato in matematica o in chimica?
#+BEGIN_SRC R :results output :exports both :session calcoloProb
mat <- 0.25
chim <- 0.15
matchim <- 0.1

#1
matchim/chim

#2
matchim/mat

#3
mat+chim-matchim
#+END_SRC

#+RESULTS:
: [1] 0.6666667
: [1] 0.4
: [1] 0.3

*** Distribuzioni Notevoli
Nel package ~stats~ sono inclusi i metodi per le più importanti distribuzioni notevoli discrete:
|                | Distribuzione | Ripartizione | Quantile | Generazione |
|----------------+---------------+--------------+----------+-------------|
| /              | <             |              |          |             |
| Binomiale      | ~dbinom~      | ~pbinom~     | ~qbinom~ | ~rbinom~    |
| Ipergeometrica | ~dhyper~      | ~phyper~     | ~qhyper~ | ~rhyper~    |
| Geometrica     | ~dgeom~       | ~pgeom~      | ~qgeom~  | ~rgeom~     |
| Poisson        | ~dpois~       | ~ppois~      | ~qpois~  | ~rpois~     |
| Uniforme       | ~dunif~       | ~qunif~      | ~qunif~  | ~runif~     |
| Esponenziale   | ~dexp~        | ~pexp~       | ~qexp~   | ~rexp~      |
| Normale        | ~dnorm~       | ~pnorm~      | ~qnorm~  | ~rnorm~     |
| Chi-Quadro     | ~dchisq~      | ~pchisq~     | ~qchisq~ | ~rchisq~    |

Esercizio 1:
- 4 monete bilanciate vengono lanciate. Assumendo l'indipendenza dei risultati, qual'è la probabilità di ottenere 2 testa
  e 2 croce?
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
dbinom(2, 4, prob=1/2)
#+END_SRC

#+RESULTS:
: [1] 0.375

Distribuzioni notevoli discrete:
- Disegnare la distribuzione di probabilità della variabile binomiale: $X$ = "numero di volte in cui compare testa",
  lanciando 4 volte una moneta bilanciata.
#+BEGIN_SRC R :file graf_binom.png :results graphics :exports both :session distr_notevoli
plot(c(0:4), dbinom(0:4, 4, prob=1/2), type="h", xlab="X")
lines(c(0:4), dbinom(0:4, 4, prob=0.5), lty=5, col="red")
text(c(0:4), dbinom(0:4, 4, prob=0.5), dbinom(0:4, 4, prob=0.5))
#+END_SRC

#+RESULTS:
[[file:graf_binom.png]]

Disegnare la funzione di ripartizione:
#+BEGIN_SRC R :file graf_rip_binom.png :results graphics :exports both :session distr_notevoli
# Impostare il grafico
plot(0, xlim=c(-0.2, 4.2), ylim=c(-0.04, 1.04), type="n", xlab="X", ylab="Probabilità cumulata")

# Disegnare due linee orizzontali che limitano la y
abline(h=c(0, 1), lty=2, col="grey")

# Disegnare una funzione a gradini
lines(stepfun(0:4, pbinom(-1:4, size=4, prob=0.5)), verticals=FALSE, do.p=FALSE)

# Disegnare i punti estremi
points(0:4, pbinom(0:4, size=4, prob=0.5), pch=16, cex=1.2)
points(0:4, pbinom(-1:3, size=4, prob=0.5), pch=1, cex=1.2)
#+END_SRC

#+RESULTS:
[[file:graf_rip_binom.png]]

Esercizio 2:
- È noto che gli item prodotti da una macchina utensile saranno difettosi con probabilità 0.1. Qual'è la probabilità
  che in un campione di 3 items al più uno sia difettoso?
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
pbinom(1, 3, prob=0.1)
#+END_SRC

#+RESULTS:
: [1] 0.972

Esercizio 3:
- Supponiamo che il colore degli occhi di una persona sia determinato in base ad una coppia di geni e che ~r~ rappresenti
  il gene dominante mentre ~r~ il gene recessivo. Pertanto una persona con la coppia di geni ~dd~ ha dominanza pura,
  una con ~rr~ ha recessione pura e una con ~dr~ o ~rd~ è ibrida. Un bambino riceve un gene da ognuno dei genitori.
  Se rispetto al colore degli occhi i due genitori "ibridi" hanno 4 figli.
  - Qual'è la probabilità che esattamente 3 dei 4 figli abbiano 1 gene dominante?
    
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
dbinom(3, 4, prob=3/4)
#+END_SRC

#+RESULTS:
: [1] 0.421875

Esercizio 6:
- Un'azienda produce dischetti, la probabilità che un dischetto sia difettoso è pari a 0.01. L'azienda vende i dischetti
  in confezioni da 10 e rimborsa l'acquirente se più di 1 dischetto è difettoso.
  1. Quale proporzione delle confezioni sarà restituita?
  2. Se un acquirente acquista 3 scatole, qual'è la probabilità che ne restituisca esattamente 1?

#+BEGIN_SRC R :results output :exports both :session distr_notevoli
#1
1 - pbinom(1, 10, 0.01)
# pbinom(1, 10, 0.01, lower.t=FALSE)

#2
dbinom(1, 3, 0.0042662)
#+END_SRC

#+RESULTS:
: [1] 0.0042662
: [1] 0.01268963

Esercizio 4:
- Supponiamo che il numero di errori tipografici presenti in una singola pagina di un libro sia distribuito secondo
  una Poisson con parametri $\lambda=1$.
  - Si calcoli la probabilità che vi sia almeno un errore in una data pagina.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
ppois(0, lambda=1, lower.t=FALSE)
# 1 - ppois(0, 1)
#+END_SRC

#+RESULTS:
: [1] 0.6321206

Esercizio 7:
- Si supponga che la probabilità che un prodotto costruito da una macchina sia difettoso è pari a 0.1.
  1. Si trovi la probabilità che un campione di 10 prodotti contenga al più un prodotto difettoso;
  2. Si trovi la probabilità che un campione di 10 prodotti contenga tra 3 e i 5 prodotti difettosi. 
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
#1
ppois(1, lambda=10*0.1)

#2
diff(ppois(c(2, 5), lambda=10*0.1))
# Equivale a:
# dpois(3, lambda=10*0.1) + dpois(4, lambda=10*0.1) + dpois(5, lambda=10*0.1)
#+END_SRC

#+RESULTS:
: [1] 0.7357589
: [1] 0.07970721

Es:
- Tracciare il grafico della distribuzione della variabile casuale di Poisson per $\lambda=1$
#+BEGIN_SRC R :file graf_pois.png :results graphics :exports both :session distr_notevoli
plot(c(0:5), dpois(0:5, 1), type="h", xlab="X")
text(c(0:5), dpois(0:5, 1), round(dpois(0:5, 1), 4))
#+END_SRC

#+RESULTS:
[[file:graf_pois.png]]

Es:
- Un calciatore ha una probabilità di segnare un calcio di rigore di 0.812. Si supponga che tirare un calcio di rigore
  sia un evento Bernoulliano.
  1. Qual'è la probabilità che sbagli 5 calci di rigore prima di segnarne uno?
  2. Qual'è la probabilità che sbagli al massimo 5 calci di rigore prima di segnarne uno?
  3. Qual'è la probabilità che sbagli almeno 5 calci di rigore prima di segnarne uno?
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
#1
dgeom(5, prob=0.812)

#2
pgeom(5, prob=0.812)

#3
pgeom(4, prob=0.812, lower.tail=FALSE)
#+END_SRC

#+RESULTS:
: [1] 0.0001906976
: [1] 0.9999558
: [1] 0.0002348493

Esercizio 5:
- In media su un'autostrada si verificano 3 incidenti al giorno.
  - Qual'è la probabilità che non si verifichino incidenti oggi?
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
lambda <- 3
incidenti <- 0

dpois(incidenti, lambda)
#+END_SRC

#+RESULTS:
: [1] 0.04978707

Esercizio 13:
- Da un mazzo di 52 carte ne vengono estratte 5 con reinserimento. Si è interessati alla variabile casuale $X$ che descrive
  il numero di carte di cuori ottenute nelle estrazioni. Determinare:
  1. Il valore atteso e la varianza della variabile $X$;
  2. La probabilità di estrarre 3 carte di cuori;
  3. La probabilità di estrarre almeno 3 carte di cuori;
  4. La probabilità di estrarre al più 3 carte di cuori.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
pcuori <- 1/4
numero_estrazioni <- 5
numero_cuori <- 13

#1
mu <- pcuori*numero_estrazioni
mu

var <- mu*(1-pcuori)
var

#2
numero_cuori_estratti <- 3
dbinom(numero_cuori_estratti, numero_estrazioni, pcuori)

#3
pbinom(numero_cuori_estratti-1, numero_estrazioni, pcuori, lower.tail=FALSE)

#4
pbinom(numero_cuori_estratti, numero_estrazioni, pcuori)
#+END_SRC

#+RESULTS:
: [1] 1.25
: [1] 0.9375
: [1] 0.08789063
: [1] 0.1035156
: [1] 0.984375

Esercizio 15:
- Il numero $X$ di telefonate ricevute nell'intervallo di tempo $[0, t_o]$ è una variabile aleatoria distribuita secondo una
  distribuzione di Poisson con parametro $\lambda = t_0$.
  - Calcolare la probabilità $P(2 \leq X \leq 4)$ di ricevere da 2 a 4 telefonate (2 e 4 inclusi) entro l'istante $t_0 = 1$.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
lambda <- 1
lower <- 2
upper <- 4

ppois(upper, lambda) - ppois(lower-1, lambda)
#+END_SRC

#+RESULTS:
: [1] 0.2605813

Esercizio 16:
- In un dato canale di comunicazione sappiamo che la probabilità di ricevere in modo errato un singolo messaggio è pari a 0.01.
  Sapendo che viene inviata una sequenza di 150 messaggi, e che i messaggi trasmessi sono stocasticamente indipendenti tra loro,
  ci si chiede:
  - Quale sia la probabilità che 2 dei messaggi ricevuti siano errati.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
p <- 0.01
n <- 150
lambda <- p*n
n_messaggi_errati <- 2

binom <- dbinom(n_messaggi_errati, n, p)
binom
# Approssimativamente:
pois <- dpois(n_messaggi_errati, lambda)
pois
# Errore approssimazione:
binom-pois
#+END_SRC

#+RESULTS:
: [1] 0.2524971
: [1] 0.2510214
: [1] 0.001475634

Esercizio 17:
- In una certa provincia montuosa si può supporre che il numero $X$ di frane al mese sia una variabile aleatoria con la legge
  di Poisson di parametro $\lambda = 2.3$.
  1. Calcolare la probabilità che ci siano almeno 2 frane in un dato mese;
  2. Quanto dovrebbe valere il parametro $\lambda$ affinché la probabilità che in un mese non ci siano frane sia superiore a $1/2$?
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
lambda <- 2.3
frane_min <- 2

#1
ppois(frane_min-1, lambda, lower.tail=FALSE)

#2
lambdaB = log(1/p)
lambdaB
cat('lambda <', lambdaB)
#+END_SRC

#+RESULTS:
: [1] 0.6691458
: [1] 4.60517

Esercizio 18:
- Il "Crazy Boat" è un battello a due motori utilizzato per le crociere sul Tamigi. I due motori lavorano indipendentemente
  e il numero di piccoli guasti in una singola crociera è modellabile tramite una v.a. $X$ di Poisson di parametro 1 per il
  primo motore, e da una v.a. $Y$ di Poisson di parametro 2 per il secondo.
  1. Qual'è la distribuzione della v.a. $X+Y$?
  2. Calcolare la probabilità che non avvenga alcun guasto in una data crociera;
  3. Partono 10 modelli identici "Crazy Boat"; calcolare la probabilità che almeno 2 battelli concludano la crociera
     senza guasti.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
#1
lambda <- 3

#2
guasti <- 0
p_noguasti <- dpois(guasti, lambda)
p_noguasti

#3
battelli_min <- 2
modelli <- 10
pbinom(battelli_min-1, modelli, p_noguasti, lower.tail=FALSE)
#+END_SRC

#+RESULTS:
: [1] 0.04978707
: [1] 0.08550346

Il package ~distr~ contiene classi per gestire una grande varietà di distribuzioni:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-04-05 21:54:10
[[file:Laboratorio/screenshot_2018-04-05_21-54-10.png]]

Uniforme:

Esempio 1:
- Sia $X$ una v.c. che rappresenta la probabilità di ricevere una telefonata tra le 10 e le 10.30. Si calcoli la probabilità
  che la chiamata arrivi tra le 10:10 e le 10:20. $X$ è distribuita secondo una variabile uniforma con parametri $a=0$ e $b=30$.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
diff(punif(c(10, 20), 0, 30))
# punif(20, 0, 30) - punif(10, 0, 30)
#+END_SRC

#+RESULTS:
: [1] 0.3333333

#+BEGIN_SRC R :file grafunif.png :results graphics :exports both :session distr_notevoli
library(distr)
X <- Unif(Min=0, Max=30)
plot(X, to.draw.args=c("d", "p"))
#+END_SRC

#+RESULTS:
[[file:grafunif.png]]

Es:
- Generare 10 numeri da una distribuzione uniforme compresa tra 0 e 50.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
runif(n=10, min=0, max=50)
#+END_SRC

#+RESULTS:
:  [1] 39.19680 22.59643 38.92506 37.44995 27.95589  3.50865 20.74593 21.64919
:  [9] 24.82297 42.06274

Esercizio 8:
- Un bus arriva ad una data fermata ad intervalli di 15 minuti a partire dalle ore 7:00. Poiché il bus passa ogni quarto d'ora,
  se un passeggero arriva alla fermata in un istante di tempo uniformemente distribuito nell'intervallo 7:00-7:30, si determino:
  1. La probabilità che attenda meno di 5 minuti;
  2. La probabilità che attenda almeno 12 minuti;
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
#1
(punif(15, 0, 30) - punif(10, 0, 30)) + (punif(30, 0, 30) - punif(25, 0, 30))

#2
(punif(3, 0, 30) - punif(0, 0, 30)) + (punif(18, 0, 30) - punif(15, 0, 30))
#+END_SRC

#+RESULTS:
: [1] 0.3333333
: [1] 0.2

Esponenziale:

Esempio 2:
- Sia $X$ una v.c. che rappresenta la durata di una batteria in mesi. $X$ è distribuita secondo un'esponenziale
  con parametro $\lambda = 0.2$.
  1. Si calcoli la probabilità che una batteria duri tra i 4 e i 6 mesi;
  2. Si calcoli la probabilità che una batteria duri almeno 10 mesi;
  3. Si calcoli la probabilità che una batteria duri almeno 10 mesi;
  4. Si calcoli la probabilità che una batteria duri altri 10 mesi sapendo che la batteria sta durando da 4 mesi.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
#1
diff(pexp(c(4, 6), rate=0.2))

#2
pexp(10, rate=0.2, lower.tail=FALSE)

#3
pexp(14, rate=0.2, lower.tail=FALSE)/pexp(4, rate=0.2, lower.tail=FALSE)
#+END_SRC

#+RESULTS:
: [1] 0.1481348
: [1] 0.1353353
: [1] 0.1652989

Es:
- Generare 10 numeri da una distribuzione esponenziale con $\lambda = 0.4$.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
rexp(10, rate=0.4)
#+END_SRC

#+RESULTS:
:  [1] 5.7233408 1.6500531 0.3326744 2.8050922 2.6702100 0.6563906 3.9339364
:  [8] 2.4213817 1.2788360 3.5355768

Esercizio 12:
- Un certo tipo di componenti elettronici viene prodotto da una ditta che utilizza 2 linee di produzione. La prima di queste
  produce il 40% dei pezzi ed i pezzi prodotti hanno un tempo di vita che segue una legge esponenziale di parametro $\lambda_1 = 1.5$.
  La seconda invece produce il 60% dei pezzi ed i pezzi prodotti hanno tempo di vita che segue una legge esponenziale di
  parametro $\lambda_2 = 2$. Un componente viene scelto a caso tra quelli prodotti dalle 2 linee e indichiamone con $X$ il suo
  tempo di vita.
  1. Qual'è la probabilità che sia ancora funzionante al tempo $t = 1$?
  2. Sapendo che è ancora funzionante al tempo $t=1$, qual'è la probabilità che esso sia ancora funzionante la tempo $t=2$?
  3. Qual'è la probabilità che su 7 componenti scelti a caso e in modo indipendente l'uno dall'altro, almeno 3 siano ancora
     funzionanti al tempo $t=1$?
  4. Supponiamo di scegliere a caso ed in modo indipendente dei componenti. Qual'è la probabilità che il primo pezzo non
     funzionante al tempo $t=1$ sia il secondo esaminato?
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
l1 <- 0.4
l2 <- 0.6
lambda1 <- 1.5
lambda2 <- 2

#1
t1 <- 1
funz_1 <- pexp(t1, lambda1, lower.tail=FALSE)*l1 + pexp(t1, lambda2, lower.tail=FALSE)*l2
funz_1

#2
t2 <- 2
funz_2 <- pexp(t2, lambda1, lower.tail=FALSE)*l1 + pexp(t2, lambda2, lower.tail=FALSE)*l2
funz_2/funz_1

#3
funz_min <- 3
n_componenti <- 7
pbinom(funz_min-1, n_componenti, funz_1, lower.t=FALSE)

#4
funz_1*(1-funz_1)
#+END_SRC

#+RESULTS:
: [1] 0.1704532
: [1] 0.1813061
: [1] 0.1011738
: [1] 0.1413989

Esercizio 14:
- L'istante d'arrivo dell'autobus è uniformemente distribuito tra le 10:00 e le 10:30. Tu arrivi alla fermata dell'autobus alle 10:00.
  1. Qual'è la probabilità che tu debba aspettare più di 10 minuti?
  2. Quanto tempo aspetti in media? Con quale deviazione standard?
  3. Se l'autobus non è ancora passato alle 10:10, qual'è la probabilità che tu debba aspettare almeno altri 10 minuti?
  4. Supponi di arrivare alla fermata dell'autobus alle 10 per 100 giorni. Calcola, in modo approssimato, la probabilità che il tempo
     totale speso ad attendere l'autobus sia superiore a 1 giorno.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
min <- 0
max <- 30

#1
minuti_min1 <- 10
t_maggiore_10 <- punif(minuti_min1, min, max, lower.t=FALSE)
t_maggiore_10

#2
mean <- (min + max)/2
mean
stdev <- sqrt((max - min)^2/12)
stdev

#3
minuti_min2 <- 20
t_maggiore_20 <- punif(minuti_min2, min, max, lower.t=FALSE)
t_maggiore_20/t_maggiore_10

#4
minuti_min <- 1440
giorni <- 100
pnorm(minuti_min, mean=mean*100, sd=sqrt(giorni)*stdev, lower.t=FALSE)

#+END_SRC

#+RESULTS:
: [1] 0.6666667
: [1] 15
: [1] 8.660254
: [1] 0.5
: [1] 0.7557888

Normale:

Esempio 3.1:
- Sia $X$ una variabile aleatoria con distribuzione normale di parametri $\mu=10$ e $\sigma=1$ e si voglia determinare la probabilità
  dell'evento "$X \in [9.2, 11.35]$"
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
pnorm(11.35, mean=10) - pnorm(9.2, mean=10)
#+END_SRC

#+RESULTS:
: [1] 0.6996366

Esempio:
- Sia $X$ una variabile che rappresenta il risultato ad un test sul QI. $X$ è distribuita secondo una variabile normale con media 100
  e deviazione standard 15. Si calcoli la probabilità che una persona abbia un QI tra 85 e 115.di
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
diff(pnorm(c(85, 115), mean=100, sd=5))
#+END_SRC

#+RESULTS:
: [1] 0.9973002

Grafico distribuzione normale:
#+BEGIN_SRC R :file grafnorm.png :results graphics :exports both :session distr_notevoli
library(distr)
X <- Norm(mean=100, sd=15)
plot(X, to.draw.arg=c("d", "p"))
#+END_SRC

#+RESULTS:
[[file:grafnorm.png]]

Es:
1. $P(X \in [\mu - \sigma, \mu + \sigma]) = 0.683$;
2. $P(X \in [\mu - 2\sigma, \mu + 2\sigma]) = 0.954$;
3. $P(X \in [\mu - 3\sigma, \mu + 3\sigma]) = 0.997$
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
pnorm(1:3) - pnorm(-(1:3))
#+END_SRC

#+RESULTS:
: [1] 0.6826895 0.9544997 0.9973002

Es:
- Generare 1000 numeri da una normale standardizzata
#+BEGIN_SRC R :file grafrandnorm.png :results graphics :exports both :session distr_notevoli
rand <- rnorm(1000)
hist(rand)
#+END_SRC

#+RESULTS:
[[file:grafrandnorm.png]]

Esempio precedente sul QI:
- Qual'è il minimo QI che una persona deve avere per essere nell'1% delle persone con il QI più alto?
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
qnorm(0.99, mean=100, sd=15)
#+END_SRC

Es:
- Calcolare $z_{0.025}, z_0.01$ e $z_{0.05}$
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
qnorm(c(0.025, 0.01, 0.005), lower.t=FALSE)
# qnorm(c(0.975, 0.99, 0.995))
#+END_SRC

#+RESULTS:
: [1] 1.959964 2.326348 2.575829

Esercizio 9:
- Se $X$ è una variabile distribuita secondo una normale con $\mu=3$ e $\sigma^2=16$, si determini:
  1. $P(X < 11)$;
  2. $P(X > -1)$;
  3. $P(2 < X < 7)$
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
#1
pnorm(11, mean=3, sd=sqrt(16))

#2
pnorm(-1, mean=3, sd=4, lower.t=FALSE)

#3
pnorm(7, mean=3, sd=4) - pnorm(2, mean=3, sd=4)
#+END_SRC

#+RESULTS:
: [1] 0.9772499
: [1] 0.8413447
: [1] 0.4400511

Esercizio 10:
- Un'industria produce su commissione delle sbarre d'acciaio cilindriche, il cui diametro dovrebbe essere di 4 cm, ma che tuttavia
  sono accettabili se hanno diametro compreso tra 3.95 cm e 4.05 cm. Il cliente, nel controllare le sbarre fornitegli, constata che
  il 5% sono di diametro inferiore al minimo tollerato ed il 12% di diametro superiore al massimo tollerato.
  1. Supponendo che le misure dei diametri seguano una legge normale, determinare media e deviazione standard;
  2. Mantenendo la media precedentemente calcolata, determinare quale dovrebbe essere il valore massimo della deviazione
     standard affinché la probabilità che le sbarre abbiano un diametro superiore al massimo tollerato sia minore di 0.05.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
m1 <- 5
m2 <- 12
max <- 4.05
min <- 3.95

#1
q1 <- qnorm(m1/100)
q2 <- qnorm(m2/100, lower.t=FALSE)
sd <- (max-min)/(q2-q1)
sd
mean <- min-q1*sd
mean

#2
q3 <- qnorm(1-m1/100, mean=mean, sd=1)
sd2 <- (max)/q3
cat("sd < ", sd2)
#+END_SRC

#+RESULTS:
: [1] 0.035463
: [1] 4.008331

Esercizio 11:
- Un'azienda stipula un contratto per vendere barattoli di conserva da 500g. La quantità di conserva $X$ messa in ciascun barattolo
  è predeterminata meccanicamente ed è normalmente distribuita con media $\mu$ e deviazione standard 25g.
  1. A quale valore minimo $\mu$ deve essere tarata la macchina, perché non più del 2% dei barattoli contenga meno di 500g di conserva?
  2. Supponiamo che i barattoli siano di metallo e che il loro peso $Y$ da vuoti segua una distribuzione $N(mean=90, V=64)$. Se un
     ispettore pesa i barattoli pieni e scarta quelli il cui peso è inferiore a 590g, quale percentuale di barattoli non
     passerà l'ispezione?
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
sd1 <- 25

#1
p <- 2/100
g <- 500
q <- qnorm(p)
mean1 <- g - q*sd1
mean1

#2
mean2 <- 90
v2 <- 64
sd2 <- sqrt(v2)
pnorm(590, mean1+mean2, sqrt(sd1^2+sd2^2))
#+END_SRC

#+RESULTS:
: [1] 551.3437
: [1] 0.02523022

Chi-Quadro:
#+BEGIN_SRC R :file grafdchiquadro.png :results graphics :exports both :session distr_notevoli
curve(dchisq(x, df=3), from=0, to=20, ylab="y")
ind <- c(4, 5, 10, 15)
for (i in ind) curve(dchisq(x, df=i), 0, 20, add=TRUE, col=i)
legend("topright", legend=c("3 gl", "4 gl", "5 gl", "10 gl", "15 gl"),
       col=c("Black", "Blue", "light Blue", "Red", "Yellow"), lty=c(1, 1, 1, 1, 1), lwd=c(3, 3, 3, 3, 3))
#+END_SRC

#+RESULTS:
[[file:grafdchiquadro.png]]

#+BEGIN_SRC R :file grafpchiquadro.png :results graphics :exports both :session distr_notevoli
curve(pchisq(x, df=3), from=0, to=20, ylab="y")
ind <- c(4, 5, 10, 15)
for (i in ind) curve(pchisq(x, df=i), 0, 20, add=TRUE, col=i)
legend("topright", legend=c("3 gl", "4 gl", "5 gl", "10 gl", "15 gl"),
       col=c("Black", "Blue", "light Blue", "Red", "Yellow"), lty=c(1, 1, 1, 1, 1), lwd=c(3, 3, 3, 3, 3))
#+END_SRC

#+RESULTS:
[[file:grafpchiquadro.png]]

T di Student:

#+BEGIN_SRC R :file grafdt.png :results graphics :exports both :session distr_notevoli
curve(dt(x, df=1), from=-4, to=4, ylim=c(0, 0.4), ylab="y")
ind <- c(3, 5, 10, 15)
for (i in ind) curve(dt(x, df=i), -4, 4, add=TRUE, col=i)
legend("topright", legend=c("1 gl", "3 gl", "5 gl", "10 gl"),
       col=c("Black", "Green", "light Blue", "Red"), lty=c(1, 1, 1, 1), lwd=c(3, 3, 3, 3))
#+END_SRC

#+RESULTS:
[[file:grafdt.png]]

#+BEGIN_SRC R :file grafpt.png :results graphics :exports both :session distr_notevoli
curve(pt(x, df=1), from=-4, to=4, ylim=c(0, 1), ylab="y")
ind <- c(3, 5, 10, 15)
for (i in ind) curve(pt(x, df=i), -4, 4, add=TRUE, col=i)
legend("topleft", legend=c("1 gl", "3 gl", "5 gl", "10 gl"),
       col=c("Black", "Green", "light Blue", "Red"), lty=c(1, 1, 1, 1), lwd=c(3, 3, 3, 3))
#+END_SRC

#+RESULTS:
[[file:grafpt.png]]

Esempio:
- Il grafico della distribuzione t di Student con 9 gradi di libertà è il seguente:
#+BEGIN_SRC R :file grafdt9.png :results graphics :exports both :session distr_notevoli
curve(dt(x, df=9), from=-4, to=4, ylim=c(0, 0.4), ylab="y")
#+END_SRC

#+RESULTS:
[[file:grafdt9.png]]

Trovare:
- Il valore di $t_1$ tale per cui:
  1. L'area a destra di $t1$ è pari a 0.05;
  2. Il totale dell'area a sinistra di $-t_1$ e a destra di $t_1$ è pari a 0.05;
  3. Il totale dell'area compresa tra $-t_1$ e $t_1$ è pari a 0.99.
#+BEGIN_SRC R :results output :exports both :session distr_notevoli
#1
qt(0.95, df=9)

#2
qt(0.975, df=9)

#3
qt(0.995, df=9)
#+END_SRC

#+RESULTS:
: [1] 1.833113
: [1] 2.262157
: [1] 3.249836

F di Fisher:
#+BEGIN_SRC R :file grafdf.png :results graphics :exports both :session distr_notevoli
curve(df(x, df1=3, df2=3), 0, +3, ylim=c(0, 1.3), col="Black", lwd=3)
curve(df(x, df1=5, df2=10), 0, +3, col="Blue", lwd=3, add=TRUE)
curve(df(x, df1=30, df2=50), 0, +3, col="Red", lwd=3, add=TRUE)
legend("topright", legend=c("3,3 gl", "5,10 gl", "30,50 gl"),
       col=c("Black", "Blue", "Red"), lty=c(1, 1, 1), lwd=c(3, 3, 3))
#+END_SRC

#+RESULTS:
[[file:grafdf.png]]

#+BEGIN_SRC R :file grafpf.png :results graphics :exports both :session distr_notevoli
curve(pf(x, df1=3, df2=3), 0, 10, ylim=c(0, 1), col="Black", lwd=3)
curve(pf(x, df1=5, df2=10), 0, 10, col="Blue", lwd=3, add=TRUE)
curve(pf(x, df1=30, df2=50), 0, 10, col="Red", lwd=3, add=TRUE)
legend("topright", legend=c("3,3 gl", "5,10 gl", "30,50 gl"),
       col=c("Black", "Blue", "Red"), lty=c(1, 1, 1), lwd=c(3, 3, 3))
#+END_SRC

#+RESULTS:
[[file:grafpf.png]]
*** Stime di Parametri
Libreria utilizzata:
- Library:
  #+BEGIN_SRC R :results output :exports both :session stime_intervallari
  #install.packages("TeachingDemos")
  require("TeachingDemos")
  #+END_SRC

  #+RESULTS:
  : Carico il pacchetto richiesto: TeachingDemos

Intervalli di confidenza per la media:
- Popolazione non normalmente distribuita e varianza sconosciuta:
  #+BEGIN_SRC R
  z.test(x, mu=0, stdev, alternative=c("two.sided", "less", "greater"), sd=stdev, n=length(x), conf.level=0.95, ...)
  #+END_SRC
- Popolazione non normalmente distribuita e varianza nota:
  #+BEGIN_SRC R
  z.test()
  #+END_SRC
- Popolazione normalmente distribuita e varianza nota:
  #+BEGIN_SRC R
  z.test()  
  #+END_SRC
- Popolazione normalmente distribuita e varianza sconosciuta:
  #+BEGIN_SRC R
  t.test(x, y=NULL, alternative=c("two.sided", "less", "greater"), mu=0, paired=FALSE, var.equal=FALSE, conf.level=0.95, ...)
  #+END_SRC

Esercizio 08:
- Popolazione non normalmente distribuita e varianza nota;
- Da un lotto di gelati, se ne estraggono 100 e se ne calcola in 82 g il valore del peso medio. Sapendo che la varianza è pari
  a 25:
  1. Si determini l'intervallo di confidenza per il peso medio $\mu$ dei gelati al livello di confidenza del 97%;
  2. Si determini la probabilità che la differenza in valore assoluto tra la media campionaria e il peso medio $\mu$ dei gelati
     sia inferiore a 3 g.
     
2:
\begin{align*}
P(|\bar{X}_n - \mu| < 3) &= P(-3 < \bar{X}_n < 3)\\
&= P\left(-\frac{3}{\frac{\sigma}{\sqrt{n}}} < \frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} < \frac{3}{\frac{\sigma}{\sqrt{n}}}\right)\\
&= P(-6 < Z < 6)\\
&= P(Z < 6) - P(Z < -6)\\
&= P(Z < 6) - (1 - P(Z < 6))
\end{align*}
#+BEGIN_SRC R :results output :exports both :session stime_intervallari
#1
out <- z.test(82, stdev=sqrt(25), alternative="two.sided", n=100, conf.level=0.97)
out$conf.int

#2
pk <- pnorm((3*sqrt(100))/sqrt(25))
pk - (1-pk)
#+END_SRC

#+RESULTS:
: [1] 80.91495 83.08505
: attr(,"conf.level")
: [1] 0.97
: [1] 1

Esercizio 5.5:
- Popolazione non normalmente distribuita e varianza sconosciuta;
- Consideriamo la seguente tabella:
  
  #+DOWNLOADED: /tmp/screenshot.png @ 2018-06-10 09:39:58
  [[file:Laboratorio/screenshot_2018-06-10_09-39-58.png]]

  e supponiamo di voler determinare un intervallo di confidenza per la media della popolazione, richiedendo un livello di
  fiducia $\alpha = 0.05$.
#+BEGIN_SRC R :results output :exports both :session stime_intervallari
# Caricare il file "esempio1.2.csv"
ese12 <- scan("./Files/esempio1.2.csv", sep=";", dec=",")

out <- z.test(ese12, stdev=sd(ese12), alternative="two.sided", conf.level=0.95)
out
out$conf.int
#+END_SRC

#+RESULTS:
#+begin_example
Read 80 items

	One Sample z-test

data:  ese12
z = 19.756, n = 80.00000, Std. Dev. = 1.21320, Std. Dev. of the sample
mean = 0.13564, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 2.413901 2.945599
sample estimates:
mean of ese12 
      2.67975
[1] 2.413901 2.945599
attr(,"conf.level")
[1] 0.95
#+end_example

Soluzione alternativa:
#+BEGIN_SRC R :results output :exports both :session stime_intervallari
lower <- mean(ese12) - qnorm(0.975)*sd(ese12)/sqrt(length(ese12))

upper <- mean(ese12) + qnorm(0.975)*sd(ese12)/sqrt(length(ese12))

interval <- c(lower, upper)
interval
#+END_SRC

#+RESULTS:
: [1] 2.413901
: [1] 2.945599
: [1] 2.413901 2.945599

Esercizio 06:
- Popolazione normalmente distribuita e varianza nota;
- Un segnale radio viene emesso con frequenza distribuita normalmente e con valore atteso $\mu$ e deviazione standard 30 kHz.
  Supponendo di osservare la seguente serie di frequenze in kHz:
  $(610, 601, 578, 615, 640, 630, 618, 602, 613, 610, 625, 585, 622, 608, 597)$.
  1. Determinare una stima di $\mu$ e la probabilità che la frequenza stia nell'intervallo di estremi 590 kHz e 610 kHz;
  2. Determinare poi un intervallo di confidenza per $\mu$ al 95%.

#+BEGIN_SRC R :results output :exports both :session stime_intervallari
#1
fr <- c(610, 601, 578, 615, 640, 630, 618, 602, 613, 610, 625, 585, 622, 608, 597)
m <- mean(fr)
m
pnorm(610, m, 30) - pnorm(590, m, 30)

#2
out <- z.test(fr, stdev=30, alternative="two.sided", conf.level=0.95)
out
out$conf.int
#+END_SRC

#+RESULTS:
#+begin_example
[1] 610.2667
[1] 0.2467925

	One Sample z-test

data:  fr
z = 78.785, n = 15.000, Std. Dev. = 30.000, Std. Dev. of the sample
mean = 7.746, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 595.0849 625.4485
sample estimates:
mean of fr 
  610.2667
[1] 595.0849 625.4485
attr(,"conf.level")
[1] 0.95
#+end_example

Esercizio 02:
- Popolazione normalmente distribuita e varianza sconosciuta;
- Un segnale $\mu$ viene trasmesso da una sorgente $A$ ad una destinazione $B$. Il segnale ricevuto in $B$ è distribuito normalmente
  con media $\mu$ e varianza $\sigma^2$ incognita. Se vengono trasmessi 9 segnali successivi con valore $\mu$ eguale e vengono ricevuti in
  $B$ i seguenti valori: $(5, 8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5)$. Si calcoli un intervallo di confidenza con il 95% di confidenza
  per $\mu$.

#+BEGIN_SRC R :results output :exports both :session stime_intervallari
x <- c(5, 8.5, 12, 15, 7, 9, 7.5, 6.5, 10.5)
out <- t.test(x, conf.level=0.95)
out$conf.int
#+END_SRC

#+RESULTS:
: [1]  6.630806 11.369194
: attr(,"conf.level")
: [1] 0.95

Stime intervallari:
- Intervalli di confidenza per la varianza:
  - Popolazione normalmente distribuita:
    #+BEGIN_SRC R
    sigma.test(x, sigma=1, sigmasq=sigma^2, alternative=c("two.sided", "less", "greater"), conf.level=0.95, ...)
    #+END_SRC
    
Esempio 11:
- Popolazione normalmente distribuita;
- Si vuole verificare se la quantità $X$ di una sostanza inquinante emessa dalle marmitte prodotto da un'azienda è contenuta
  entro i limiti prestabiliti. A tal fine, si estrae un campione di 10 marmitte dalla produzione settimanale dell'azienda e,
  attraverso prove su strada, si rilevano le seguenti quantità (in mg/Km) della sostanza nociva rilasciante:
  $(895, 902, 894, 903, 901, 893, 897, 908, 906, 891)$. Sapendo che la quantità emessa della sostanza in esame ha distribuzione
  normale di parametri $\mu$ e $\sigma^2$ incogniti, determinare la stima intervallare di $\sigma^2$ al livello di confidenza del 99%.

#+BEGIN_SRC R :results output :exports both :session stime_intervallari
x <- c(895, 902, 894, 903, 901, 893, 897, 908, 906, 891)

out <- sigma.test(x, conf.level = 0.99)
out$conf.int
#+END_SRC

#+RESULTS:
: [1]  12.88717 175.22291
: attr(,"conf.level")
: [1] 0.99

Esercizio 1:
- Popolazione normalmente distribuita e varianza nota;
- Si desidera stimare il tempo medio di CPU utilizzato dagli utenti di un server, in modo da affermare con il 95% di confidenza
  che il valore stimato non disti più di mezzo secondo dal valore vero. Dall'esperienza passata, possiamo ipotizzare che il tempo
  di CPU utilizzato sia distribuito secondo una normale, con $\sigma^2 = 2.25$. Quale deve essere la dimensione $n$ del campione?
$$2 \cdot z_{1-\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} = \frac{1}{2} \implies n = 2 \cdot z_{1-\frac{\alpha}{2}} \frac{\sigma}{\frac{1}{2}}$$

#+BEGIN_SRC R :results output :exports both :session stime_intervallari
q1 <- qnorm(1-(1-0.95)/2) #quantile 0.975

n <- ceiling((2*q1*sqrt(2.25)/0.5)^2)
n
#+END_SRC

#+RESULTS:
: [1] 139

Esempio 5.5:
- Popolazione normalmente distribuita e varianza sconosciuta;
- Riprendiamo in considerazione il problema esposto nell'Esempio 5.5, nel quale si sono determinate 2 intervalli di confidenza
  per la media del costo al metro quadro degli appartamenti di un dato quartiere di una città italiana. Si voglia determinare
  quanto deve essere grande il campione affinché l'intervallo di confidenza simmetrico abbia ampiezza non superiore a 0.3,
  mantenendo sempre un livello di confidenza $\alpha = 0.05$.
#+BEGIN_SRC R :results output :exports both :session stime_intervallari
# Caricare il file "esempio1.2.csv"
ese12 <- scan("./Files/esempio1.2.csv", sep=";", dec=",")

alpha = 0.05
q <- qnorm(1-alpha/2)

n <- ceiling((2*q*sd(ese12)/0.3)^2)
n
#+END_SRC

#+RESULTS:
: Read 80 items
: [1] 252
*** Verifica di ipotesi: Test Parametrici
/Errori di I specie/

Esercizio 01:
- Per provare l'ipotesi che una moneta è buona viene adottata la seguente regola di decisione. Si accetta l'ipotesi se il numero di teste
  in un campione di 100 lanci è compreso tra 40 e 60 inclusi. In caso contrario si rifiuta l'ipotesi. Si determini la probabilità di rifiutare
  l'ipotesi quando essa è vera. Sia $X = \text{numero di teste}$. Sotto ipotesi nulla, $X$ è distribuita secondo una $\text{Bin}(n=100, p=0.5)$.
  Poiché $n > 30$, allora approssimiamo la distribuzione di $X$ con una $N(\mu = 50, \sigma^2 = 25)$. La regione critica è $(X<40) \cup (X>60)$.

  È necessario calcolare $P((X < 40) \cup (X > 60)) = P(X < 40) + P(X > 60)$
#+BEGIN_SRC R :results output :exports both
mu = 100*0.5
sd = sqrt(100*(1-0.5)*0.5)

Pmin <- pnorm(40, mean=mu, sd)
Pmax <- pnorm(60, mean=mu, sd, lower.tail=F)
Pmax + Pmin
#+END_SRC

#+RESULTS:
: [1] 0.04550026

Esercizio 03:
- Una fabbrica produce corde le cui resistenze alla rottura hanno una media di 300 N e uno scarto quadratico medio di 24N. Si crede che per mezzo di un
  recente processo produttivo, la resistenza media alla rottura possa essere aumentata.
  1. Si determini una regola di decisione per il rifiuto del vecchio processo produttivo al livello di significatività dello 0.01 se si vogliono provare
     64 corde.

  Considerazioni:
  - Ipotesi nulla: $H_0 : \mu = 300N$;
  - Ipotesi alternativa: $H_1 : \mu > 300N$.
  $$Z = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{\bar{X} - 300}{\frac{24}{\sqrt{64}}} = \frac{\bar{X} - 300}{3} \implies Z*3 + 300 = \bar{X}$$
#+BEGIN_SRC R :results output :exports both
z <- qnorm(1-0.01)
lower <- ceiling(z*(24/sqrt(64))+300) # limite inferiore della regione critica
cat("Si rifiuta l'ipotesi H_0 se X_n cade in: [", lower,", inf]")                 
#+END_SRC

#+RESULTS:
: Si rifiuta l'ipotesi H_0 se Xn cade in: [ 307 , inf]

/Errore di II specie/

Esercizio 04:
- Una fabbrica produce corde le cui resistenze alla rottura hanno una media di 300 N e uno scarto quadratico medio di 24 N. Si
  crede che per mezzo di un recente processo produttivo la resistenza media alla rottura possa essere aumentata.
  1. Qual'è la probabilità di accettare il vecchio processo produttivo con la regola di decisione stabilita quando in realtà il
     nuovo processo ha aumentato la resistenza media alla rottura a 310 N?
  Considerazioni:
  - Ipotesi nulla: $H_0 : \mu = 300 N$;
  - Ipotesi alternativa: $H_1 : \mu = 310 N$
  $$P(\bar{X} \in (-\infty, 307] \mid \mu = 310)$$
  Dobbiamo calcolare la probabilità che $\bar{X} \in (-\infty, 307]$ sapendo che $\bar{X}$ è distribuita come indicato da $H_1$, cioè
  $\bar{X} \sim N(310, 24)$.

#+BEGIN_SRC R :results output :exports both
out <- pnorm(307, mean=310, sd=24/sqrt(64))
out
#+END_SRC

#+RESULTS:
: [1] 0.1586553

Libreria utilizzata:
  #+BEGIN_SRC R :results output :exports both :session test_parametrici
  #install.packages("TeachingDemos")
  require("TeachingDemos")
  #+END_SRC

  #+RESULTS:
  : Loading required package: TeachingDemos

Test sulla media di una popolazione:
1. Popolazione normalmente distribuita e varianza nota:
   #+BEGIN_SRC R
   z.test(x, mu=0, stdev, alternative=c("two.sided", "less", "greater"), sd=stdev, n=length(x), conf.level=0.95, ...)
   #+END_SRC
2. Popolazione normalmente distribuita e varianza sconosciuta:
   #+BEGIN_SRC R
   t.test(x, y=NULL, alternative=c("two.sided", "less", "greater"), paired=FALSE, var.equal=FALSE, conf.level=0.95, ...)
   #+END_SRC
3. Popolazione non normalmente distribuita e varianza nota:
   #+BEGIN_SRC R
   z.test()
   #+END_SRC
4. Popolazione non normalmente distribuita e varianza sconosciuta:
   #+BEGIN_SRC R
   z.test()
   #+END_SRC
   
/Test sulla media di una popolazione/

\(p\)-value:
- Probabilità che una variabile avente la stessa distribuzione della statistica test, sotto ipotesi $H_0$, assuma un valore maggiore di
  quello da essa effettivamente ottenuto sulla base della realizzazione del campione.
Regola di decisione:
- $p-\text{value} < \alpha \implies \text{si rifiuta } H_0$;
- $p-\text{value} > \alpha \implies \text{non si rifiuta } H_0$.

Esercizio 15:
- Popolazione normalmente distribuita e varianza nota;
- Dato il campione $(0.5, 1.2, -0.5, 0.9, 2.0, -1.2, -0.3)$ estratto da una popolazione con distribuzione normale, testare l'ipotesi $\mu=0$
  contro $\mu \neq 0$.
  1. Con livello di significatività 0.05, supponendo varianza nota e uguale a 2.
  Considerazioni:
  - Ipotesi nulla: $H_0 : \mu=0$;
  - Ipotesi alternativa: $H_1 : \mu \neq 0$.
#+BEGIN_SRC R :results output :exports both :session test_parametrici
x <- c(0.5, 1.2, -0.5, 0.9, 2.0, -1.2, -0.3)
z.test(x, mu=0, stdev=sqrt(2))
#+END_SRC

#+RESULTS:
#+begin_example

	One Sample z-test

data:  x
z = 0.69488, n = 7.00000, Std. Dev. = 1.41420, Std. Dev. of the sample
mean = 0.53452, p-value = 0.4871
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 -0.6762162  1.4190734
sample estimates:
mean of x 
0.3714286
#+end_example
Non si rifiuta $H_0 : p-\text{value} > \alpha$

Soluzione alternativa:
$$Z_n = \frac{\bar{X}_n - \mu_0}{\frac{\sigma}{\sqrt{n}}}$$
#+BEGIN_SRC R :results output :exports both :session test_parametrici
z1 <- qnorm((1-(0.05/2))) # calcolo del quantile
Zn <- (mean(x) - 0)/(sqrt(2)/sqrt(7)) # calcolo della statistica
Zn

if (Zn <= -z1 | Zn >= z1) {
    out <- 'Rigettiamo H_0 in favore di H_1'
} else {
    out <- 'Non rigettiamo H_0 in favore di H_1'
}
out
#+END_SRC

#+RESULTS:
: [1] 0.6948792
: [1] "Non rigettiamo H_0 in favore di H_1"

Esercizio 15 (modificato):
- Popolazione normalmente distribuita e varianza nota;
- Dato il campione $(0.5, 1.2, -0.5, 0.9, 2.0, -1.2, -0.3)$ estratto da una popolazione con distribuzione normale, testare l'ipotesi
  $\mu=0$ contro $\mu > 0$.
  1. Con livello di significatività 0.05, supponendo varianza nota ed uguale a 2.
  Considerazioni:
  - Ipotesi nulla: $H_0 : \mu = 0$;
  - Ipotesi alternativa: $H_1 : \mu > 0$.
  Nota:
  - Test unidirezionale con coda a destra poiché $\mu > 0$;
  - Se l'esercizio avesse richiesto $\mu < 0$: test unidirezionale con coda a sinistra.
#+BEGIN_SRC R :results output :exports both :session test_parametrici
x <- c(0.5, 1.2, -0.5, 0.9, 2.0, -1.2, -0.3)
z.test(x, mu=0, stdev=sqrt(2), alternative="greater")
#+END_SRC

#+RESULTS:
#+begin_example

	One Sample z-test

data:  x
z = 0.69488, n = 7.00000, Std. Dev. = 1.41420, Std. Dev. of the sample
mean = 0.53452, p-value = 0.2436
alternative hypothesis: true mean is greater than 0
95 percent confidence interval:
 -0.5077827        Inf
sample estimates:
mean of x 
0.3714286
#+end_example
Non si rifiuta $H_0 : p-\text{value} > \alpha$

Esercizio 15:
- Popolazione normalmente distribuita e varianza sconosciuta:
- Dato il campione $(0.5, 1.2, -0.5, 0.9, 2.0, -1.2, -0.3)$ estratto da una popolazione con distribuzione normale, testare
  l'ipotesi $\mu=0$ contro $\mu \neq 0$.
  1. Con livello di significatività 0.1, supponendo varianza sconosciuta.
  Considerazioni:
  - Ipotesi nulla: $H_0 : \mu = 0$;
  - Ipotesi alternativa: $H_0 : \mu \neq 0$.
#+BEGIN_SRC R :results output :exports both :session test_parametrici
t.test(x, conf.level=0.9)
#+END_SRC

#+RESULTS:
#+begin_example

	One Sample t-test

data:  x
t = 0.89005, df = 6, p-value = 0.4077
alternative hypothesis: true mean is not equal to 0
90 percent confidence interval:
 -0.4394847  1.1823418
sample estimates:
mean of x 
0.3714286
#+end_example
Non si rifiuta $H_0 : p-\text{value} > \alpha$.

Soluzione alternativa (statistica test: media campionaria):
#+BEGIN_SRC R :results output :exports both :session test_parametrici
m <- mean(x) # statistica test

n <- length(x) # calcolo lunghezza campione
q1 <- qt(1-(1-0.9)/2, n) # calcolo quantile
sdcc <- sd(x) # calcolo deviazione standard campionaria corretta

T1 <- 0 - q1*sdcc/sqrt(n) # limiti della regione critica
T2 <- 0 + q1*sdcc/sqrt(n)

if (m <= T1 | m >= T2) {
    out2 <- 'Rigettiamo H_0 in favore di H_1'
} else {
    out2 <- 'Non rigettiamo H_0 in favore di H_1'
}
out2
#+END_SRC

#+RESULTS:
: [1] "Non rigettiamo H_0 in favore di H_1"

Esercizio 21:
- Popolazione non normalmente distribuita e varianza nota;
- Il signor Rossi desidera capire quale sia la media della spesa mensile per generi alimentari del proprio nucleo familiare. Per questo
  registra tali spese per 36 mesi, ottenendo una media campionaria uguale a 305 e una varianza campionaria corretta uguale a 225.
  1. Fornire una stima intervallare, con significatività $\alpha = 0.05$, per la media della spesa mensile in generi alimentari della famiglia Rossi.
  2. Controllare, con un livello $\alpha = 0.1$, l'ipotesi che la media della spesa mensile della Famiglia Rossi in generi alimentari sia in realtà
     di 280 euro.
  Considerazioni:
  - Ipotesi nulla: $H_0 : \mu = 280$;
  - Ipotesi alternativa: $H_1 : \mu \neq 280$.
#+BEGIN_SRC R :results output :exports both :session test_parametrici
z.test(305, mu=280, stdev=sqrt(225), conf.level=0.9, n=36)
#+END_SRC

#+RESULTS:
#+begin_example

	One Sample z-test

data:  305
z = 10, n = 36.0, Std. Dev. = 15.0, Std. Dev. of the sample mean = 2.5,
p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 280
90 percent confidence interval:
 300.8879 309.1121
sample estimates:
mean of 305 
        305
#+end_example
Si rifiuta $H_0 : p-\text{value} < \alpha$.

Test sulla varianza di una popolazione:
- Popolazione normalmente distribuita:
  #+BEGIN_SRC R
  sigma.test(x, sigma=1, sigmasq=sigma^2, alternative=c("two.sided", "less", "greater"), conf.level=0.95)
  #+END_SRC

Esercizio 28:
- Popolazione normalmente distribuita;
- Un campione di 10 individui estratto da una popolazione con distribuzione normale ha assunto i seguenti valori:
  $(1.4, 0.6, 2.2, 0.8, 2.3, -1.6, -0.1, -0.3, 1, 2)$
  1. Controllare con $\alpha = 0.01$, l'ipotesi che la deviazione standard della popolazione sia uguale a 1.2.
  Considerazioni:
  - Ipotesi nulla: $H_0 : \sigma = 1.2$;
  - Ipotesi alternativa: $H_1 : \sigma \neq 1.2$.
#+BEGIN_SRC R :results output :exports both :session test_parametrici
x <- c(1.4, 0.6, 2.2, 0.8, 2.3, -1.6, -0.1, -0.3, 1, 2)
sigma.test(x, sigma=1.2, conf.level=0.9)
#+END_SRC

#+RESULTS:
#+begin_example

	One sample Chi-squared test for variance

data:  x
X-squared = 9.6257, df = 9, p-value = 0.7633
alternative hypothesis: true variance is not equal to 1.44
90 percent confidence interval:
 0.8192575 4.1685803
sample estimates:
var of x 
1.540111
#+end_example
Non si rifiuta $H_0 : p-\text{value} > \alpha$.

Libreria:
#+BEGIN_SRC R
#install.packages('BSDA')
require('BSDA')
#+END_SRC

#+RESULTS:
: TRUE

Test sulla differenza delle medie di due popolazioni:
1. Popolazioni $X$ e $Y$ normalmente distribuite con varianze uguali e sconosciute:
   #+BEGIN_SRC R
   t.test(x, y=NULL, alternative=c("two.sided", "leww", "greater"), mu=0, paired=FALSE, var.equal=TRUE, conf.level=0.95, ...)
   #+END_SRC
2. Popolazioni $X$ e $Y$ normalmente distribuite con varianze diverse e note:
   #+BEGIN_SRC R
   z.test(x, y=NULL, alternative="two.sided", mu=0, sigma.x=NULL, sigma.y=NULL, conf.level=0.95)
   #+END_SRC
3. Popolazioni $X$ e $Y$ non normalmente distribuite con varianze diverse e note (campioni grandi con numerosità maggiore o uguale a 30):
   #+BEGIN_SRC R
   z.test()
   #+END_SRC
   
Esercizio 13:
- Popolazioni $X$ e $Y$ normalmente distribuite con varianze uguali e sconosciute;
- Un insieme di pazienti viene sottoposto all'uso di un nuovo farmaco per una data patologia. La patologia viene misurata quotidianamente
  da un esame, portando alla seguente tabella:
  - Campione $X: (110, 102, 104, 96, 92,  97, 85, 115)$;
  - Campione $Y: (125, 133, 128, 119, 134, 126, 132, 135)$.
  Si sa che la misura è normalmente distribuita e si vuole verificare che il farmaco influisce o meno sulla patologia con confidenza
  al 95%.
  Considerazioni:
  - Ipotesi nulla: $H_0 : \mu_X = \mu_Y$;
  - Ipotesi alternativa: $H_1 : \mu_X \neq \mu_Y$.
#+BEGIN_SRC R :results output :exports both :session test_parametrici
x <- c(110, 102, 104, 96, 92, 97, 85, 115)
y <- c(125, 133, 128, 119, 134, 126, 132, 135)

t.test(x, y, var.equal=T)
#+END_SRC

#+RESULTS:
#+begin_example

	Two Sample t-test

data:  x and y
t = -7.3221, df = 14, p-value = 3.778e-06
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -37.33301 -20.41699
sample estimates:
mean of x mean of y 
  100.125   129.000
#+end_example
Rifiutando $H_0$ in favore di $H_1$: il farmaco non influisce.

Esempio:
- Popolazione $X$ e $Y$ normalmente distribuite con varianze diverse e note;
- Si supponga di avere:
  - Campione $X: (154, 109, 137, 115, 140)$;
  - Campione $Y: (108, 115, 126, 92, 146)$.
  Estratti da popolazioni normalmente distribuite. Le deviazioni standard delle due popolazioni sono note e rispettivamente pari a
  15.5 e 13.5. Si vuole verificare che la differenza delle medie è pari a 0 con confidenza del 95%.
  Considerazioni:
  - Ipotesi nulla: $H_0: \mu_X = \mu_Y$;
  - Ipotesi alternativa: $H_1: \mu_X \neq \mu_Y$
#+BEGIN_SRC R :results output :exports both :session test_parametrici
x <- c(154, 109, 137, 115, 140)
y <- c(108, 115, 126, 92, 146)

z.test(x, y, mu=0, sigma.x=15.5, sigma.y=13.5)
#+END_SRC

#+RESULTS:
#+begin_example

	One Sample z-test

data:  x
z1 = 2.7123, z2 = 2.5472, z3 = 2.3248, z4 = 3.1840, z5 = 2.0063, n =
5.000, Std. Dev.1 = 108.000, Std. Dev.2 = 115.000, Std. Dev.3 =
126.000, Std. Dev.4 = 92.000, Std. Dev.5 = 146.000, Std. Dev. of the
sample mean1 = 48.299, Std. Dev. of the sample mean2 = 51.430, Std.
Dev. of the sample mean3 = 56.349, Std. Dev. of the sample mean4 =
41.144, Std. Dev. of the sample mean5 = 65.293, p-value = 0.006682,
p-value = 0.010860, p-value = 0.020083, p-value = 0.001453, p-value =
0.044821
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
  36.33557 231.80009
sample estimates:
mean of x 
      131 

Warning messages:
1: In c(-1, 1) * qnorm(1 - (1 - conf.level)/2) * sd :
  longer object length is not a multiple of shorter object length
2: In if (substr(fp, 1L, 1L) == "<") fp else paste("=", fp) :
  the condition has length > 1 and only the first element will be used
#+end_example
Non rifiutiamo $H_0$ in favore di $H_1$.

Test sull'uguaglianza della varianza di due popolazioni:
- Popolazioni $X$ e $Y$ normalmente distribuite:
  - $\tilde{V} = \frac{\sigma_Y^2 \cdot \hat{S}_{X, n}^2}{\sigma_X^2 \cdot \hat{S}_{Y, m}^2}$;
  - $H_0 : \sigma_X^2 = \sigma_Y^2$;
  - $V_{n, m} = \frac{\hat{S}_{X, n}^2}{\hat{S}_{Y, m}^2}$
  Risulta essere distribuita come una F con $(n-1)$ e $(m-1)$ gradi di libertà.

Esempio 6.6:
- Popolazioni $X$ e $Y$ normalmente distribuite con varianze uguali e sconosciute;
- Consideriamo due diversi quartieri della stessa città e supponiamo di essere interessati a confrontare il valor medio dei redditi annui
  familiari in migliaia di euro. Si supponga che sia lecito assumere che tali redditi siano normalmente distribuiti. Si supponga di estrarre
  un campione casuale per ogni quartiere di numerosità pari a 10 e 15. Supponiamo che le realizzazioni dei campioni relativi ad $X$ ed $Y$ siano
  $(22, 48, 51, 20, 28, 35, 38, 26, 50, 36)$ e $(40, 42, 50, 26, 30, 34, 37, 28, 25, 30, 32, 38, 22, 55, 40)$.

  Verificare l'uguaglianza delle due varianze.
  Considerazioni:
  - Ipotesi nulla: $H_0 : \sigma^2_X = \sigma^2_Y$;
  - Ipotesi alternativa: $H_1 : \sigma^2_X \neq \sigma^2_Y$.
#+BEGIN_SRC R :results output :exports both :session test_parametrici
x <- c(22, 48, 51, 20, 28, 35, 38, 26, 50, 36)
y <- c(40, 42, 50, 26, 30, 34, 37, 28, 25, 30, 32, 38, 22, 55, 40)
stat <- var(x)/var(y)
stat

F1 <- qf((0.1/2), 9, 14)
F2 <- qf(1-(0.1/2), 9, 14)
F1
F2

if (stat >= F1 & stat <= F2) {
    out <- 'Non rigettiamo H_0 in favore di H_1'
} else{
    out <- 'Rigettiamo H_0 in favore di H_1'
}
out
#+END_SRC

#+RESULTS:
: [1] 1.53924
: [1] 0.3305269
: [1] 2.645791
: [1] "Non rigettiamo H_0 in favore di H_1"

Soluzione alternativa:
#+BEGIN_SRC R
var.test(x, y, ratio=1, alternative=c("two.sided", "less", "greater"), conf.level=0.95)
#+END_SRC
#+BEGIN_SRC R :results output :exports both :session test_parametrici
var.test(x, y)
#+END_SRC

#+RESULTS:
#+begin_example

	F test to compare two variances

data:  x and y
F = 1.5392, num df = 9, denom df = 14, p-value = 0.4528
alternative hypothesis: true ratio of variances is not equal to 1
95 percent confidence interval:
 0.4796185 5.8459594
sample estimates:
ratio of variances 
           1.53924
#+end_example

Test di incorrelazione:
- $\rho_{XY} = \frac{\sigma_{XY}}{\sigma_X \cdot \sigma_Y}$;
- $R_n = \frac{\sum_{i=1}^n (X_i - \bar{X}) \cdot (Y_i - \bar{Y})}{n \cdot S_{X, n} \cdot S_{Y, n}}$;
- $H_0 : \rho_{XY} = 0$;
- $\hat{T}_n = R_n \cdot \sqrt{\frac{n-2}{1-R_n^2}}$.
Risulta essere distribuita come una t di Studente con $(n-2)$ gradi di libertà.
#+BEGIN_SRC R
cor.test(x, y, alternative=c("two.sided", "less", "greater"),
         method=c("pearson", "kendal", "spearman"), exact=NULL, conf.level=0.95, continuiti=FALSE, ...)
#+END_SRC

Esempio 6.7:
- Si consideri una popolazione $(X, Y)$ di travi di cemento, in cui $X$ indica il carico di prima lesione e $Y$ il carico a rottura finale.
  Si supponga di avere un'ampia popolazione, ovvero di avere un elevato numero di queste travi e di voler controllare, con un livello di
  significatività $\alpha = 0.05$, se esiste una correlazione tra il carico di prima lesione ed il carico a rottura finale. A tale scopo
  si supponga di aver effettuato delle prove su 15 travi, ottenendo i risultati riportati nel file Esempio 1.16.

  Considerazioni:
  - Ipotesi nulla: $H_0 : \rho_{XY} = 0$;
  - Ipotesi alternativa: $H_1 : \rho_{XY} \neq 0$.
#+BEGIN_SRC R :results output :exports both :session test_parametrici
ese116 <- read.csv("./Files/esempio1.16.csv", sep=";", header=TRUE)

cor.test(ese116$CaricoPrimaLesione.x., ese116$CaricoRottura.y., method="pearson")
#+END_SRC

#+RESULTS:
#+begin_example

	Pearson's product-moment correlation

data:  ese116$CaricoPrimaLesione.x. and ese116$CaricoRottura.y.
t = 8.4357, df = 13, p-value = 1.245e-06
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.7699373 0.9733190
sample estimates:
      cor 
0.9195286
#+end_example
*** Verifica di ipotesi: Test non Parametrici
Test per la bontà dell'adattamento:
- Ipotesi:
  - Nulla $H_0 : F_X (t) = F(t) \text{ per ogni } t \in \mathbb{R}$;
  - Alternativa: $H_1 : F_X(t) \neq F(t) text{ per almeno un} t \in \mathbb{R}$
- Test di Kolmogorov-Smirnov: $D_n = \text{sup}_{t} \in \mathbb{R}|F(t) - \hat{F}_{X, n}(t)|$.
  #+BEGIN_SRC R
  ks.test(x, y, ..., alternative = c("two.sided", "less", "greater"), exact=NULL)
  #+END_SRC
  
Esercizio 02:
- Un campione di numerosità 5 estratto da una popolazione $X$ ha fornito i seguenti valori: $(6.2, 8.8, 9.4, 13.6, 10)$.
  Controllare con Kolmogorov-Smirnov e livello di significatività 0.1 l'ipotesi che $X$ sia distribuita:
  1. Secondo una uniforme $[0, 20]$;
  2. Secondo un'esponenziale di parametro $\lambda = 0.1$.
  Considerazioni:
  1. Ipotesi:
     - Nulla: $H_0 : X \sim U[0, 20]$
     - Alternativa: $H_1 : X \text{ non } U[0, 20]$.
  2. Ipotesi:
     - Nulla: $H_0 : X \sim \text{Exp}[0.1]$;
     - Alternativa: $H_1 : X \text{ non } \sim \text{Exp}[0.1]$.
#+BEGIN_SRC R :results output :exports both :session test_non_parametrici
x <- c(6.2, 8.8, 9.4, 13.6, 10)

#1
ks.test(x, "punif", 0, 20)

#2
ks.test(x, "pexp", 0.1)
#+END_SRC

#+RESULTS:
#+begin_example

	One-sample Kolmogorov-Smirnov test

data:  x
D = 0.32, p-value = 0.585
alternative hypothesis: two-sided

	One-sample Kolmogorov-Smirnov test

data:  x
D = 0.46206, p-value = 0.1713
alternative hypothesis: two-sided
#+end_example

1. Non si rifiuta l'ipotesi $H_0$ che la popolazione segue una distribuzione Uniforme $p-\text{value} > \alpha = 0.1$
2. Non si rifiuta l'ipotesi $H_0$ che la popolazione segue una distribuzione Esponenziale $p-\text{value} > \alpha = 0.1$.

Test per la bontà dell'adattamento:
- Ipotesi:
  - Nulla $H_0 : F_X (t) = F(t) \text{ per ogni } t \in \mathbb{R}$;
  - Alternativa: $H_1 : F_X(t) \neq F(t) \text{ per almeno un} t \in \mathbb{R}$
- Test del Chi-quadro: $W = \sum_{k=1}^K \frac{(n_k - np_k)^2}{np_k}$, con $K$ numero di intervalli.
  - Quando $H_0$ è vera e quando $n_k$ sono sufficientemente grandi ($\geq 5$), la $W$ è distribuita come una Chi-Quadro con:
    - $K-1$ gradi di libertà se la funzione di ripartizione $F$ è stata decisa arbitrariamente senza fare uso preventivo
      dei dati campionari;
    - $K-r-1$ gradi di libertà se nella funzione di ripartizione $F$ compaiono $r$ parametri che sono stati stimati facendo
      uso dei dati campionari.
  #+BEGIN_SRC R
  chisq.test(x, y=NULL, correct=TRUE, p=rep(1/length(x), length(x)), rescale.p=FALSE, simulate.p.value=FALSE, B=2000)
  #+END_SRC
  
Esercizio 03:
- Un campione di dimensione 30, estratto da una popolazione $X$, ha fornito i seguenti valori:
  
  #+DOWNLOADED: /tmp/screenshot.png @ 2018-06-14 13:26:11
  
  [[file:Laboratorio/screenshot_2018-06-14_13-26-11.png]]

  Controllare con il metodo del Chi-Quadro, con significatività 0.01, l'ipotesi che $X$ sia distribuito secondo un'uniforme con
  parametri $[0, 20]$. Dividere l'intervallo in 4.

  Considerazioni:
  - Ipotesi:
    - Nulla: $H_0 : X \sim U[0, 20]$;
    - Alternativa: $H_1 : X \text{ non } \sim U[0, 20]$.
#+BEGIN_SRC R :results output :exports both :session test_non_parametrici
x <- c(2.03 , 9.82 , 2.50 , 6.14 , 4.44 , 4.62 , 10.10 , 7.34 , 8.31 , 8.32 , 16.46 , 3.61,16.55 , 
15.49 , 17.87 , 6.93 , 10.96 , 4.53, 17.72 , 7.52 , 4.69 , 6.70 , 16.24 , 6.19, 13.14 , 16.73 , 10.64 , 
11.53, 2.78, 15.36)

limiti <- c(0, 5, 10, 15, 20) # si indicano i limiti degli intervalli
interval <- cut(x, breaks=limiti) # si divide in intervalli
f <- table(interval) # si calcolano le frequenze assolute
chisq.test(f, p=diff(punif(limiti, 0, 20)))
#+END_SRC

#+RESULTS:
: 
: 	Chi-squared test for given probabilities
: 
: data:  f
: X-squared = 1.2, df = 3, p-value = 0.753

Non si rifiuta l'ipotesi $H_0$ che la popolazione segue una distribuzione Uniforme $p-\text{value} > \alpha = 0.01$

Test per il confronto delle distribuzioni di due popolazioni:
- Ipotesi:
  - Nulla: $H_0 : X \sim U[0, 20]$;
  - Alternativa: $H_1 : X \text{ non } \sim U[0, 20]$.
- Test:
  1. Dei segni;
  2. Dei ranghi di Wilcoxon;
  3. Di Kolmogorov-Smirnov al caso di due campioni.

Test dei segni:
- Se l'ipotesi nulla è vera e se $n - S^= \geq 10$, la quantità $S_n = S^+ - S^-$ risulta essere distribuita:
  $N\left(0, \frac{n-S^=}{2}\right)$, con:
  - $S^+ = \text{numero di volte che \(X_i > Y_i\)}$;
  - $S^- = \text{numero di volte che \(X_i < Y_i\)}$;
  - $S^= = \text{numero di volte che \(X_i = Y_i\)}$.
  Fissato il livello di significatività $\alpha$, la regione critica per $S_n$ è
  $C = \left(-\infty, -z_{1-\frac{\alpha}{2}} \frac{n-S^=}{2}\right) \cup \left(+z_{1-\frac{\alpha}{2}} \frac{n-S^=}{2}, +\infty\right)$
  
Esercizio 09:
- Un'azienda afferma che un integratore di sua ideazione è in grado di influire sul livello di colesterolo nel sangue, per tale
  ragione viene prodotto uno studio dell'azienda medesima dove a 10 individui è stato misurato il livello del colesterolo prima
  di assumere l'integratore in questione e dopo 2 settimane di assunzione del medesimo. Si è ottenuto quanto segue:
  
  #+DOWNLOADED: /tmp/screenshot.png @ 2018-06-14 14:24:33
  [[file:Laboratorio/screenshot_2018-06-14_14-24-33.png]]
  
  Cosa possiamo concludere circa l'affermazione dell'azienda?
- Ipotesi:
  - Nulla: $H_0 : F_X(t) = F_Y(t)$;
  - Alternativa: $H_1 : F_X(t) \neq F_Y(t)$;
  - $X$ indica il livello di colesterolo "prima" e $Y$ il livello "dopo" la cura con l'integratore.
#+BEGIN_SRC R :results output :exports both :session test_non_parametrici
prima <- c( 225, 310, 287, 249, 345, 288, 247, 268, 213, 332) 
dopo <- c(210, 301, 291, 212, 307, 290, 216, 245, 195, 301) 

s <- sign(prima-dopo)
p <- length(which(s>0))
n <- length(which(s<0))
u <- length(which(s==0))

stat <- p-n
stat

# Calcolo dei limiti della regione critica
- qnorm(0.975)*sqrt((length(x)-u)/2)
qnorm(0.975)*sqrt((length(x)-u)/2)
#+END_SRC

#+RESULTS:
: [1] 6
: [1] -7.590908
: [1] 7.590908

La statistica, che ha valore 6, cade nella Regione Critica. Rigettiamo l'ipotesi nulla che le due distribuzioni ($X$ ed $Y$)
siano identiche: il che porta a concludere che l'affermazione fatta dall'azienda non è falsa, ossia che l'integrazione
influisce sul livello di colesterolo nel sangue.

Test dei ranghi di Wilcoxon:
#+BEGIN_SRC R
wilcoxon.test(x, y=NULL, alternative=c("two.sided", "less", "greater"), mu=0, paired=FALSE, exact=NULL, correct=TRUE, conf.int=FALSE, conf.lecvel=0.95, ...)
#+END_SRC

Esercizio 10:
- Un'azienda afferma che un integratore di sua ideazione è in grado di influire sul livello di colesterolo nel sangue, per tale
  ragione viene prodotto uno studio dell'azienda medesima dove a 10 individui è stato misurato il livello del colesterolo prima
  di assumere l'integratore in questione e dopo 2 settimane di assunzione del medesimo. Si è ottenuto quanto segue:

  #+DOWNLOADED: /tmp/screenshot.png @ 2018-06-14 14:34:21
  [[file:Laboratorio/screenshot_2018-06-14_14-34-21.png]]

  Cosa possiamo concludere circa l'affermazione dell'azienda (considerare un livello di significatività pari a 0.05)?
- Ipotesi:
  - Nulla: $H_0 : F_X(t) = F_Y(t)$;
  - Alternativa: $H_1 : F_X(t) \neq F_Y(t)$;
  - $X$ indica il livello di colesterolo "prima" e $Y$ il livello "dopo" la cura con l'integratore.
#+BEGIN_SRC R :results output :exports both :session test_non_parametrici
X <- c( 225, 310, 287, 249, 345, 288, 247, 277, 267, 194)
Y <- c( 210, 301, 291, 212, 307, 290, 216, 312, 206, 185, 199, 204, 245, 195, 301)

wilcox.test(X, Y, exact=FALSE)
#+END_SRC

#+RESULTS:
: 
: 	Wilcoxon rank sum test with continuity correction
: 
: data:  X and Y
: W = 92, p-value = 0.36
: alternative hypothesis: true location shift is not equal to 0

Non si rifiuta l'ipotesi $H_0$ che le due distribuzioni siano uguali. $p-\text{value} > \alpha = 0.05$. Quindi l'affermazione
dell'azienda circa l'efficacia dell'integratore viene rigettata.

Test di Kolmogorov-Smirnov nel caso di due campioni:
#+BEGIN_SRC R
ks.test(x, y, ..., alternative=c("two.sided", "less", "greater"), exact=NULL)
#+END_SRC


Esercizio 12:
- Pazienti leucemici vengono sottoposti a due trattamenti $A$ e $B$. Il conteggio delle piastrine $(10^3)$ viene riportato
  nella tabella.

  #+DOWNLOADED: /tmp/screenshot.png @ 2018-06-14 14:39:46
  [[file:Laboratorio/screenshot_2018-06-14_14-39-46.png]]

  Possiamo affermare che tali conteggi provengano dalla stessa distribuzione?
- Ipotesi:
  - Nulla: $H_0 : F_X(t) = F_Y(t)$;
  - Alternativa: $H_1 : F_X(t) \neq F_Y(t)$;
  - $X$ indica il trattamento $A$ e $Y$ indica il trattamento $B$.
#+BEGIN_SRC R :results output :exports both :session test_non_parametrici
A <- c( 20.30, 22.53, 25.70, 13.23, 29.67, 24.46, 26.07, 19.35, 17.81,16.00, 13.50, 32.90)
B <- c( 10.56, 28.13, 19.94, 11.03, 8.09, 12.95, 21.14, 32.50, 10.90 )

ks.test(A, B)
#+END_SRC

#+RESULTS:
: 
: 	Two-sample Kolmogorov-Smirnov test
: 
: data:  A and B
: D = 0.55556, p-value = 0.06063
: alternative hypothesis: two-sided

Non si rifiuta l'ipotesi $H_0$ che le due distribuzioni $A$ e $B$ siano identiche. Ossia che il numero di piastrine sotto il
trattamento $A$ sia distribuito in modo identico dal numero di piastrine sotto il trattamento $B$ $p-\text{value} > \alpha = 0.05$.

Test di indipendenza:
- Test del Chi-quadro per l'indipendenza:
  - Consideriamo una popolazione bidimensionale $(X, Y)$ ed estraiamo un campione casuale $((X_1, Y_1), \dots, (X_n, Y_n))$;
  - Ipotesi:
    - Nulla: $H_0 : \text{ i caratteri \(X\) ed \(Y\) sono indipendenti}$;
    - Alternativa: $H_1 : \text{ i caratteri \(X\) ed \(Y\) non sono indipendenti}$;
    - Per ogni coppia $(k, j)$ consideriamo le seguenti quantità:
      - $n_k^X = \text{ numero di elementi \(X_i\) del campione che cadono in } I_k^X$;
      - $n_j^Y = \text{ numero di elementi \(Y_i\) del campione che cadono in }I_j^Y$;
      - $n_{k, j} = \text{ numero di elementi \((X_i, Y_i)\) del campione che cadono in } I_k^X \times I_j^Y$;
      - $f_k^X = \frac{n_k^X}{n} = \text{ frequenza relativa di }I_k^X$;
      - $f_j^Y = \frac{n_j^Y}{n} = \text{ frequenza relativa di } I_j^Y$;
      - $f_{k, j} = \frac{n_{k, j}}{n} = \text{ frequenza relativa di } I_k^X \times I_j^Y$.
    - $W = n \cdot \sum_{k=1}^{M_X}\sum_{j=1}^{M_Y} \frac{(f_{k, j} - f_k^X \cdot f_j^Y)^2}{f_k^X \cdot f_j^Y}$;
    - Si può dimostrare che quando l'ipotesi nulla è vera e quando le $n_{k, j}$ sono sufficientemente grandi $(\geq 5)$, allora $W$
      è approssimativamente distribuita come una Chi-Quadro con $(M_X - 1) \cdot (M_Y - 1)$ gradi di libertà.
#+BEGIN_SRC R
chisq.test(x, y=NULL, correct=TRUE, p=rep(1/length(1/length(x), length(x)), rescale.p=FALSE, simulate.p.value=FALSE, B=2000))
#+END_SRC

Esercizio 06:
- Si desidera controllare l'ipotesi di indipendenza tra due caratteri discreti $X$ ed $Y$ di una popolazione. Il campione, di
  numerosità 150, ha fornito i conteggi riportati in tabella.

  #+DOWNLOADED: /tmp/screenshot.png @ 2018-06-14 14:53:44
  [[file:Laboratorio/screenshot_2018-06-14_14-53-44.png]]

  Controllare l'ipotesi di indipendenza di $X$ ed $Y$ con un livello di significatività 0.05.
- Ipotesi:
  - Nulla: $H_0 : \text{i caratteri \(X\) ed \(Y\) sono indipendenti}$;
  - Alternativa: $H_1 : \text{i caratteri \(X\) ed \(Y\) non sono indipendenti}$;
#+BEGIN_SRC R :results output :exports both :session test_non_parametrici
arr <- c(25, 20, 5, 15, 30, 15, 10, 25, 5)
m <- matrix(arr, nrow=3, ncol=3)

chisq.test(m)
#+END_SRC

#+RESULTS:
: 
: 	Pearson's Chi-squared test
: 
: data:  m
: X-squared = 12.75, df = 4, p-value = 0.01256

Si rifiuta l'ipotesi $H_0$ che i due caratteri $X$ e $Y$ sono indipendenti. In conclusione i due caratteri $X$ ed $Y$ non sono
tra loro indipendenti $p-\text{value} < \alpha = 0.05$.

Test di incorrelazione:
- Coefficiente di correlazione dei ranghi di Spearman:
  Consideriamo una popolazione bidimensionale $(X, Y)$ ed estraiamo un campione casuale $((X_1, Y_1,), \dots, (X_n, Y_n))$
- Ipotesi:
  - Nulla: $H_0 : \text{i caratteri \(X\) ed \(Y\) sono indipendenti}$;
  - Alternativa: $H_1 : \text{i caratteri \(X\) ed \(Y\) non sono indipendenti}$;
- Ad ogni coppia $(x_i, y_i)$ associamo ora la corrispondente coppia di ranghi $(r_i^x, r_i^y)$ e denotiamo con $d_i$ le loro
  differenze $d_i = r_i^x - r_i^y$.

  Consideriamo la statistica $R_s = 1 - \frac{6 \cdot \sum_{i=1}^n d_i^2}{n^3 - n}$
  indipendentemente dalla forma della distribuzione
  congiunte $(X, Y)$. Quando i caratteri $X$ ed $Y$ sono incorrelati, nel senso di Spearman, e la numerosità del campione è maggiore
  di 10, allora la statistica $\tilde{T}_n = R_S \cdot \sqrt{\frac{n-2}{1 - R_S^2}}$
  risulta essere approssimativamente distribuita come una t di Student con $(n-2)$ gradi di libertà.
#+BEGIN_SRC R
cor(x, y=NULL, use="everything", method=c("pearson", "kendall", "spearman"))
#+END_SRC

Esercizio 11:
- La tabella sottostante riporta il rango dello HumanDevelopmentIndex (HDI) e dell'Income per 10 paesi:

  #+DOWNLOADED: /tmp/screenshot.png @ 2018-06-14 15:09:58
  [[file:Laboratorio/screenshot_2018-06-14_15-09-58.png]]

  Cosa possiamo concludere circa la correlazione tra Income e HDI?
- Ipotesi:
  - Nulla: $H_0 : \text{i caratteri \(X\) ed \(Y\) sono incorrelati secondo Spearman}$;
  - Alternativa: $H_1 : \text{i caratteri \(X\) ed \(Y\) non sono incorrelati secondo Spearman}$;
#+BEGIN_SRC R :results output :exports both :session test_non_parametrici
Income <- c(3, 2, 10, 8, 5, 7, 1, 6, 9, 4)
HDI <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)

rho <- cor(Income, HDI, method="spearman") # calcolo coeff. di correlazione di Spearman
Tn <- rho*sqrt((10-2)/(1-rho^2))
Tn

# Calcolo dei limiti della regione critica
qt(0.95, 8) # quantile di ordine (1 - alpha/2) con alpha=0.1
-qt(0.95, 8)
#+END_SRC

#+RESULTS:
: [1] 0.3278787
: [1] 1.859548
: [1] -1.859548

Non si rifiuta l'ipotesi $H_0$ che i due caratteri non sono correlati secondo Spearman poiché la statistica $T_n = 0.33$ non cade
nella regione critica $C = (-\infty, -1.86,) \cup (1.86, +\infty)$.
*** Regressione Lineare
Regressione lineare semplice - stima dei parametri del modello:
#+BEGIN_SRC R
lm(formula, data, subset, weights, na.action, method="qr", model=TRUE, x=FALSE, y=FALSE, qr=TRUE, singular.ok=TRUE, contrasts=NULL, offset, ...)
#+END_SRC

Regressione lineare semplice - stima puntuale della variabile risposta:
#+BEGIN_SRC R
predict(object, ...)
#+END_SRC

Esercizio 02:
- Si considerino i seguenti dati:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-14 18:16:20
[[file:Laboratorio/screenshot_2018-06-14_18-16-20.png]]

1. Stimare il coefficiente di correlazione lineare tra $X$ e $Y$ e si discuta la correlazione tra i due caratteri;
2. Si determini la retta di regressione lineare $Y = \alpha_0 + \alpha_1 X$;
3. Stimare il valore atteso della variabile di risposta $Y$ quando il regressore $X$ vale 5.

#+BEGIN_SRC R :results output :exports both :session regressione
#1
X <- c(-3, 4, 5, 8, 6)
Y <- c(2, 6, 7, 10, 5)
Rxy <- cor(X, Y, method="p")
Rxy

#+END_SRC

#+RESULTS:
: [1] 0.8814141

Forte correlazione (positiva) che lega i due caratteri $X$ e $Y$ poiché il coefficiente di correlazione lineare (=0.88) è prossimo
a 1.

#+BEGIN_SRC R :results output :exports both :session regressione
#2
rr <- lm(Y ~ X)
rr

# Coefficienti
coef <- rr$coefficients
#+END_SRC

#+RESULTS:
: 
: Call:
: lm(formula = Y ~ X)
: 
: Coefficients:
: (Intercept)            X  
:      3.5429       0.6143

#+BEGIN_SRC R :results graphics :file lm.png :exports both :session regressione
# Creare il grafico
plot(Y ~ X)
abline(coef(rr), col='red', lwd=3)
#+END_SRC

#+RESULTS:
[[file:lm.png]]

#+BEGIN_SRC R :results output :exports both :session regressione
#3
new <- data.frame(X=5)
predict(rr, new)

# Stima puntuale della variabile risposta
predict(rr)
#+END_SRC

#+RESULTS:
:        1 
: 6.614286
:        1        2        3        4        5 
: 1.700000 6.000000 6.614286 8.457143 7.228571

Regressione lineare semplice - residui di un modello:
#+BEGIN_SRC R
residuals(object, ...)
#+END_SRC

Regressione lineare semplice - intervalli di confidenza per i coefficienti di regressione:
#+BEGIN_SRC R
confint(object, parm, level=0.95)
#+END_SRC

Esempi:
#+BEGIN_SRC R :results output :exports both :session regressione
residuals(rr)
#+END_SRC

#+RESULTS:
:             1             2             3             4             5 
:  3.000000e-01  6.522560e-16  3.857143e-01  1.542857e+00 -2.228571e+00

#+BEGIN_SRC R :results output :exports both :session regressione
confint(rr, level=0.9) # livello di fiducia 0.1
#+END_SRC

#+RESULTS:
:                   5 %     95 %
: (Intercept) 1.0930143 5.992700
: X           0.1670077 1.061564

Regressione lineare semplice - intervalli di confidenza per valori stimati della variabile risposta e intervalli di predizione:
#+BEGIN_SRC R
predict(object, newdata, se.fit=FALSE, scale=NULL, df=Inf, interval=c("none", "confidence", "prediction"), level=0.95, type=c("response", "terms"), terms=NULL, na.action=na.pass, pred.var=res.var/weights, weights=1, ...)

# Intervalli di confidenza
predict(object, newdata, ..., interval="confidence", level=0.95)

# Intervalli di predizione
predict(object, newdata, ..., interval="prediction", level=0.95)
#+END_SRC

Esempi:
- $\left[a_0 - t_{1-\frac{\alpha}{2}} \cdot \sqrt{s_{\text{RES}}^2 \cdot \left[\frac{1}{n} + \frac{\bar{x}^2}{s_x^2}\right]},
  a_0 + t_{1-\frac{\alpha}{2}} \cdot \sqrt{s_{\text{RES}}^2 \cdot \left[\frac{1}{n} + \frac{\bar{x}^2}{s_x^2}\right]}\right]$;
- $\left[a_1 - t_{1-\frac{\alpha}{2}} \cdot \sqrt{s_{\text{RES}}^2 \cdot \frac{1}{s_x^2}},
  a_0 + t_{1-\frac{\alpha}{2}} \cdot \sqrt{s_{\text{RES}}^2 \cdot \frac{1}{s_x^2}}\right]$.
#+BEGIN_SRC R :results output :exports both :session regressione
# Intervalli di confidenza
predict(rr, level=0.9, interval="confidence")

# Intervalli di predizione
new <- data.frame(X=5)
predict(rr, newdata=new, level=0.9, interval="prediction")
#+END_SRC

#+RESULTS:
:        fit       lwr       upr
: 1 1.700000 -1.850160  5.250160
: 2 6.000000  4.326439  7.673561
: 3 6.614286  4.881985  8.346586
: 4 8.457143  6.007300 10.906986
: 5 7.228571  5.330931  9.126212
:        fit      lwr      upr
: 1 6.614286 2.490586 10.73799

#+BEGIN_SRC R :results graphics :file regressione.png :exports both :session regressione
# Rappresentazione grafica degli intervalli di confidenza e degli intervalli di predizione
library(HH)
ci.plot(rr, conf.level=0.9)
#+END_SRC

#+RESULTS:
[[file:regressione.png]]

Regressione lineare semplice - attendibilità del modello:
#+BEGIN_SRC R
summary(object, ...)
#+END_SRC

Esercizio 01:
- Si considerino i seguenti dati:

#+DOWNLOADED: /tmp/screenshot.png @ 2018-06-14 20:00:07
[[file:Laboratorio/screenshot_2018-06-14_20-00-07.png]]

1. Stimare i coefficienti di regressione di $Y$ su $X$;
2. Verificare se l'intercetta è significativamente diversa da 0 con confidenza 0.95;
3. Stimare il valore atteso della variabile di risposta $Y$ quando il regressore $X$ vale 13.
#+BEGIN_SRC R :results output :exports both :session regressione
#1
X <- c(12, 8, 11, 15, 18)
Y <- c(30, 23, 27, 36, 42)

fm <- lm(Y ~ X)
fm

#2
summary(fm)

#3
new <- data.frame(X=13)
predict(fm, new, level="prediction")
#+END_SRC

#+RESULTS:
#+begin_example

Call:
lm(formula = Y ~ X)

Coefficients:
(Intercept)            X  
      6.653        1.949

Call:
lm(formula = Y ~ X)

Residuals:
       1        2        3        4        5 
-0.04082  0.75510 -1.09184  0.11224  0.26531 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   6.6531     1.3559   4.907 0.016207 *  
X             1.9490     0.1023  19.047 0.000316 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.7846 on 3 degrees of freedom
Multiple R-squared:  0.9918,	Adjusted R-squared:  0.9891 
F-statistic: 362.8 on 1 and 3 DF,  p-value: 0.000316
      1 
31.9898
#+end_example

Regressione lineare semplice - analisi dei residui:
1. Test di normalità: Test di Shapiro Wilk;
2. Test di incorrelazione: Test di Durbin e Watson;
3. Test di omoschedasticità: Metodi grafici;
4. Outliers: Distanza di Cook.
   
Test di normalità - Test di Shapiro Wilk:

Ipotesi:
- Nulla: $H_0: \text{i residui seguono una distribuzione normale}$;
- Alternativa: $H_1: \text{i residui non seguono una distribuzione normale}$.
#+BEGIN_SRC R :results output :exports both :session regressione
X <- c(-3, 4, 5, 8, 6)
Y <- c(2, 6, 7, 10 ,5)
rr <- lm(Y ~ X)
residui <- residuals(rr)
shapiro.test(residui)
#+END_SRC

#+RESULTS:
: 
: 	Shapiro-Wilk normality test
: 
: data:  residui
: W = 0.89097, p-value = 0.362

Non si rifiuta l'ipotesi $H_0$ ossia che i residui seguono una distribuzione normale poiché il $p-\text{value} > 0.05$.

Test di incorrelazione - Test di Durbin e Watson:
#+BEGIN_SRC R :results output :exports both :session regressione
library(lmtest)
residui <- residuals(rr)
dwtest(rr, alternative="two.sided")
#+END_SRC

#+RESULTS:
: 
: 	Durbin-Watson test
: 
: data:  rr
: DW = 2.0831, p-value = 0.6294
: alternative hypothesis: true autocorrelation is not 0

Non si rifiuta l'ipotesi $H_0$, quindi i residui sono incorrelati poiché il $p-\text{value} > 0.05$.

Test di omoschedasticità - Metodi grafici:
#+BEGIN_SRC R :results graphics :file omoschedasticità.png :exports both :session regressione
plot(X, rr$residuals, xlab="X", ylab="RESIDUALS", main="Verifica Omoschedasticità")
abline(h=0)
#+END_SRC

#+RESULTS:
[[file:omoschedasticità.png]]

#+BEGIN_SRC R :results graphics :file omoschedasticità2.png :exports both :session regressione
plot(fitted(rr), abs(residui), ylab="RESIDUAL", xlab="Fitted", main="Residui in valore assoluto vs Fitted Value")
#+END_SRC
#+RESULTS:
[[file:omoschedasticità2.png]]

Outliers - Distanza di Cook:
#+BEGIN_SRC R :results output :exports both :session regressione
cook <- cooks.distance(rr)
cook
#+END_SRC

#+RESULTS:
:            1            2            3            4            5 
: 1.601695e+00 2.628942e-32 1.021151e-02 6.177966e-01 4.576271e-01

Un sospetto outliers è influente sui parametri della regressione se la Distanza di Cook, $D > 1$:
$$D_i = \frac{\sum_j \left(y_{(i)_j}^* - \hat{y_j} \right)^2}{\text{SSE}}$$

Regressione lineare multipla:
#+BEGIN_SRC R :results output :exports both :session regressione
lm(formula, data, subset, weights, na.action, method="qr", model=TRUE, x=FALSE, y=FALSE, qr=TRUE, singular.ok=TRUE, contrasts=NULL, offset, ...)
#+END_SRC

Esempio:
- Il dataset "trees" (fornito da R) contiene delle misurazioni sulla circonferenza, altezza e volume di 31 alberi di ciliegio abbattuti.
Dati:
1. Girth: diametro in pollici ($X_1$);
2. Height: altezza in piedi ($X_2$);
3. Volume: volume in piedi alla terza ($Y$).

Vogliamo determinare quanto segue:
1. Coefficienti della retta di regressione: $Y = \alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 + \epsilon$;
2. Intervalli di confidenza per i coefficienti di regressione;
3. Stima puntuale, intervalli di confidenza e intervalli di predizione della variabile risposta su nuovi dati con Girth
   $(9.1, 11.6, 12.5)$ e Height $(46, 74, 87)$.
#+BEGIN_SRC R :results output :exports both :session regressione
#1
trees.lm <- lm(Volume ~ Girth + Height, data=trees)
trees.lm

#2
confint(trees.lm)

#3
# Stima puntuale
new <- data.frame(Girth=c(9.1, 11.6, 12.5), Height=c(49, 74, 87))
predict(trees.lm, newdata=new)

# Intervalli di confidenza del valore mu_0 = E[Y_0]
predict(trees.lm, newdata=new, interval="confidence")

# Intervalli di predizione del valore y_0 = Y_0
predict(trees.lm, newdata=new, interval="prediction")

# Attendibilità del modello
summary(trees.lm)
#+END_SRC

#+RESULTS:
#+begin_example

Call:
lm(formula = Volume ~ Girth + Height, data = trees)

Coefficients:
(Intercept)        Girth       Height  
   -57.9877       4.7082       0.3393
                   2.5 %      97.5 %
(Intercept) -75.68226247 -40.2930554
Girth         4.16683899   5.2494820
Height        0.07264863   0.6058538
        1         2         3 
 1.479912 21.731594 30.379205
        fit       lwr       upr
1  1.479912 -5.009282  7.969106
2 21.731594 20.111104 23.352084
3 30.379205 26.909637 33.848773
        fit       lwr      upr
1  1.479912 -8.783476 11.74330
2 21.731594 13.616578 29.84661
3 30.379205 21.703641 39.05477

Call:
lm(formula = Volume ~ Girth + Height, data = trees)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.4065 -2.6493 -0.2876  2.2003  8.4847 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -57.9877     8.6382  -6.713 2.75e-07 ***
Girth         4.7082     0.2643  17.816  < 2e-16 ***
Height        0.3393     0.1302   2.607   0.0145 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 3.882 on 28 degrees of freedom
Multiple R-squared:  0.948,	Adjusted R-squared:  0.9442 
F-statistic:   255 on 2 and 28 DF,  p-value: < 2.2e-16
#+end_example
